[
    {
        "abstract": "We present ionization structures of IC 2003, a planetary nebula with [WR] central star, using a 1-D dusty photo-ionization code: \\textit{\"CLOUDY 17.03\"}. The photo-ionization model is constrained by archival UV emission line fluxes, medium-resolution optical spectroscopy, $IRAS$ $25 \\mu m$ flux, absolute $H\\beta$ flux, and the mean angular size of the nearly spherical optical nebula. To constrain the carbon abundance and the effect of photo-electric heating in the ionized gas, we used UV emission lines. We considered an amorphous carbon dust grain with MRN and KMH size distributions to address the importance of photo-electric heating in the ionized nebula. We show that KMH grain size distribution with quantum dust heating reproduces the observations quite well. We construct the ionization structures of different elements at their different ionization stages in the nebula. We derive the physical properties of the planetary nebula and its chemical composition, as well as the parameters of its central star. The estimated nebular dust-to-gas mass ratio is $2.37\\times 10^{-3}$, and the enhanced photo-electric heating yielded by small dust grains is $9.4\\%$ of the total heating. We considered the H-poor model atmosphere for the central star; the effective temperature of the central star is 177kK, the specific gravity log(g) is 6, and its luminosity ($L_*$) is 6425 $L_\\odot$. The derived central star parameters plotted on stellar evolutionary tracks correspond to a central star mass of 0.636$M_\\odot $ and to a progenitor mass of 3.26$M_\\odot$.",
        "citation_title": "Photo-ionization structures of Planetary Nebula IC 2003 with [WR] central star",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We introduce the DREAMS project, an innovative approach to understanding the astrophysical implications of alternative dark matter models and their effects on galaxy formation and evolution. The DREAMS project will ultimately comprise thousands of cosmological hydrodynamic simulations that simultaneously vary over dark matter physics, astrophysics, and cosmology in modeling a range of systems -- from galaxy clusters to ultra-faint satellites. Such extensive simulation suites can provide adequate training sets for machine-learning-based analyses. This paper introduces two new cosmological hydrodynamical suites of Warm Dark Matter, each comprised of 1024 simulations generated using the Arepo code. One suite consists of uniform-box simulations covering a $(25~h^{-1}~{\\rm M}_\\odot)^3$ volume, while the other consists of Milky Way zoom-ins with sufficient resolution to capture the properties of classical satellites. For each simulation, the Warm Dark Matter particle mass is varied along with the initial density field and several parameters controlling the strength of baryonic feedback within the IllustrisTNG model. We provide two examples, separately utilizing emulators and Convolutional Neural Networks, to demonstrate how such simulation suites can be used to disentangle the effects of dark matter and baryonic physics on galactic properties. The DREAMS project can be extended further to include different dark matter models, galaxy formation physics, and astrophysical targets. In this way, it will provide an unparalleled opportunity to characterize uncertainties on predictions for small-scale observables, leading to robust predictions for testing the particle physics nature of dark matter on these scales.",
        "citation_title": "Introducing the DREAMS Project: DaRk mattEr and Astrophysics with Machine learning and Simulations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Simulations and observations find long tails in jellyfish galaxies, which are commonly thought to originate from ram-pressure stripped gas of the interstellar medium (ISM) in the immediate galactic wake. While at larger distances from the galaxy, they have been claimed to form in situ owing to thermal instability and fast radiative cooling of mixed ISM and intracluster medium (ICM). In this paper, we use magneto-hydrodynamical windtunnel simulations of a galaxy with the arepo code to study the origin of gas in the tails of jellyfish galaxies. To this end, we model the galaxy orbit in a cluster by accounting for a time-varying galaxy velocity, ICM density and turbulent magnetic field. Tracking gas flows between the ISM, the circumgalactic medium (CGM) and the ICM, we find contrary to popular opinion that the majority of gas in the tail originated in the CGM. Prior to the central passage of the jellyfish galaxy in the cluster, the CGM is directly transported to the clumpy jellyfish tail that has been shattered into small cloudlets. After the central cluster passage, gas in the tail originates both from the initial ISM and the CGM, but that from the latter was accreted to the galactic ISM before being ram-pressure stripped to form filamentary tentacles in the tail. Our simulation shows a declining gas metallicity in the tail as a function of downstream distance from the galaxy. We conclude that the CGM plays an important role in shaping the tails of jellyfish galaxies.",
        "citation_title": "Comparing the interstellar and circumgalactic origin of gas in the tails of jellyfish galaxies",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We propose a simple fit function, $L_{\\nu_i}(t) = C\\, t^{-\\alpha}\\, e^{-(t/\\tau)^{n}}$, to parametrize the luminosities of neutrinos and antineutrinos of all flavors during the protoneutron star (PNS) cooling phase at post-bounce times $t \\gtrsim 1$ s. This fit is based on results from a set of neutrino-hydrodynamics simulations of core-collapse supernovae in spherical symmetry. The simulations were performed with an energy-dependent transport for six neutrino species and took into account the effects of convection and muons in the dense and hot PNS interior. We provide values of the fit parameters $C$, $\\alpha$, $\\tau$, and $n$ for different neutron star masses and equations of state as well as correlations between these fit parameters. Our functional description is useful for analytic supernova modeling, for characterizing the neutrino light curves in large underground neutrino detectors, and as a tool to extract information from measured signals on the mass and equation of state of the PNS and on secondary signal components on top of the PNS's neutrino emission.",
        "citation_title": "Simple fits for the neutrino luminosities from protoneutron star cooling",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Recent low-frequency radio observations at 140 MHz discovered a 3 Mpc-long bridge of diffuse emission connecting the galaxy clusters Abell 0399 and Abell 0401. We present follow-up observations at 60 MHz to constrain the spectral index of the bridge, which so far has only been detected at 140 and 144 MHz. We analysed deep (~18 hours) LOw Frequency ARray (LOFAR) Low Band Antenna (LBA) data at 60 MHz to detect the bridge at very low frequencies. We then conducted a multi-frequency study with LOFAR HBA data at 144 MHz and uGMRT data at 400 MHz. Assuming second-order Fermi mechanisms for the re-acceleration of relativistic electrons driven by turbulence in the radio bridge regions, we compare the observed radio spectrum with theoretical synchrotron models. The bridge is detected in the 75'' resolution LOFAR image at 60 MHz and its emission fully connects the region between the two galaxy clusters. Between 60 MHz and 144 MHz we found an integrated spectral index value of -1.44 +\\- 0.16 for the bridge emission. For the first time, we produced spectral index and related uncertainties maps for a radio bridge. We produce a radio spectrum, which show significant steepening between 144 and 400 MHz. This detection at low frequencies provides important information on the models of particle acceleration and magnetic field structure on very extended scales. The spectral index gives important clues to the origin of inter-cluster diffuse emission. The steepening of the spectrum above 144 MHz can be explained in a turbulent re-acceleration framework, assuming that the acceleration timescales are longer than ~200 Myr.",
        "citation_title": "Abell 0399-Abell 0401 radio bridge spectral index: the first multifrequency detection",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present determinations of the gas-phase and stellar metallicities of a sample of 65 star-forming galaxies at $z \\simeq 3.5$ using rest-frame far-ultraviolet (FUV) spectroscopy from the VANDELS survey in combination with follow-up rest-frame optical spectroscopy from VLT/KMOS and Keck/MOSFIRE. We infer gas-phase oxygen abundances ($Z_{\\mathrm{g}}$; tracing O/H) via strong optical nebular lines and stellar iron abundances ($Z_{\\star}$; tracing Fe/H) from full spectral fitting to the FUV continuum. Our sample spans the stellar mass range $8.5 < \\mathrm{log}(M_{\\star}/\\mathrm{M}_{\\odot}) < 10.5$ and shows clear evidence for both a stellar and gas-phase mass-metallicity relation (MZR). We find that our O and Fe abundance estimates both exhibit a similar mass-dependence, such that $\\mathrm{Fe/H}\\propto M_{\\star}^{0.30\\pm0.11}$ and $\\mathrm{O/H}\\propto M_{\\star}^{0.32\\pm0.09}$. At fixed $M_{\\star}$ we find that, relative to their solar values, O abundances are systematically larger than Fe abundances (i.e., $\\alpha$-enhancement).We estimate an average enhancement of $\\mathrm{(O/Fe)} = 2.65 \\pm 0.16 \\times \\mathrm{(O/Fe)_\\odot}$ which appears to be independent of $M_{\\star}$. We employ analytic chemical evolution models to place a novel constraint on the strength of galactic-level outflows via the mass-outflow factor ($\\eta$). We show that outflow efficiencies that scale as $\\eta \\propto M_{\\star}^{-0.32}$ can simultaneously explain the functional form of of the stellar and gas-phase MZR, as well as the degree of $\\alpha$-enhancement at fixed Fe/H. Our results add further evidence to support a picture in which $\\alpha$-enhanced abundance ratios are ubiquitous in high-redshift star-forming galaxies, as expected for young systems whose interstellar medium is primarily enriched by core-collapse supernovae.",
        "citation_title": "The NIRVANDELS Survey: the stellar and gas-phase mass-metallicity relations of star-forming galaxies at z = 3.5",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "A dark star cluster (DSC) is a system in which the cluster potential is dominated by stellar remnants, such as black holes and neutron stars having larger masses than the long-lived low-mass stars. Due to mass segregation, these remnants are located in the central region of the cluster and form a dark core. We expect that at a few kpc from the Galactic centre, the efficient evaporation of the lower-mass stars caused by the strong tidal force exposes the dark core, because the dynamical properties of the DSC are dominated by the remnants. Due to the invisibility of the remnants, finding a DSC by observation is challenging. In this project, we use $N$-body simulations to obtain models of DSCs and try to discern observables that signify a DSC. We consider four observables: the mass spectrum, the observational mass density profile, the observational velocity dispersion profile and the mass segregation. The models show that a DSC typically exhibits two distinct characteristics: for a given mass in stars and a given half-light radius the expected velocity dispersion is underestimated when only visible stars are considered, and there is a lack of measurable mass segregation among the stars. These properties can be helpful for finding DSCs in observational data, such as the Gaia catalogue.",
        "citation_title": "Mass segregation and velocity dispersion as evidence for a dark star cluster",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In recent years, there has been a push to understand how chemical composition affects the magnetic activity levels of main sequence low-mass stars. Results indicate that more metal-rich stars are more magnetically active for a given stellar mass and rotation period. This metallicity dependence has implications for how the rotation periods and activity levels of low-mass stars evolve over their lifetimes. Numerical modelling suggests that at late ages more metal-rich stars should be rotating more slowly and be more magnetically active. In this work, we study the rotation and activity evolution of low-mass stars using a sample of Kepler field stars. We use the gyro-kinematic age dating technique to estimate ages for our sample and use the photometric activity index as our proxy for magnetic activity. We find clear evidence that, at late ages, more metal-rich stars have spun down to slower rotation in agreement with the theoretical modeling. However, further investigation is required to definitively determine whether the magnetic activity evolution occurs in a metallicity dependent way.",
        "citation_title": "The impact of stellar metallicity on rotation and activity evolution in the Kepler field using gyro-kinematic ages",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Five self-lensing binaries (SLBs) have been discovered with data from the \\textit{Kepler} mission. One of these systems is KIC 8145411, which was reported to host an extremely low mass (ELM; $0.2\\,M_{\\odot}$) white dwarf (WD) in a 456-day orbit with a solar-type companion. The system has been dubbed ``impossible'', because evolutionary models predict that $\\sim 0.2\\,M_{\\odot}$ WDs should only be found in tight orbits ($P_{\\rm orb} \\lesssim$ days). In this work, we show that KIC 8145411 is in fact a hierarchical triple system: it contains a WD orbiting a solar-type star, with another solar-type star $\\sim 700\\,$AU away. The wide companion was unresolved in the Kepler light curves, was just barely resolved in Gaia DR3, and is resolved beyond any doubt by high-resolution imaging. We show that the presence of this tertiary confounded previous mass measurements of the WD for two reason: it dilutes the amplitude of the self-lensing pulses, and it reduces the apparent radial velocity (RV) variability amplitude of the WD's companion due to line blending. By jointly fitting the system's light curves, RVs, and multi-band photometry using a model with two luminous stars, we obtain a revised WD mass of $(0.53 \\pm 0.01)\\,M_{\\odot}$. Both luminous stars are near the end of their main-sequence evolution. The WD is thus not an ELM WD, and the system does not suffer the previously proposed challenges to its formation history. Similar to the other SLBs and the population of astrometric WD binaries recently identified from Gaia data, KIC 8145411 has parameters in tension with standard expectations for formation through both stable and unstable mass transfer. The system's properties are likely best understood as a result of unstable mass transfer from an AGB star donor.",
        "citation_title": "No longer impossible: the self-lensing binary KIC 8145411 is a triple",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present 1 - 12 GHz Karl G. Jansky Very Large Array observations of 9 off-nuclear persistent radio sources (PRSs) in nearby (z < 0.055) dwarf galaxies, along with high-resolution European very-long baseline interferometry (VLBI) Network (EVN) observations for one of them at 1.7GHz. We explore the plausibility that these PRSs are associated with fast radio burst (FRB) sources by examining their properties, physical sizes, host-normalized offsets, spectral energy distributions (SEDs), radio luminosities, and light curves, and compare them to those of the PRSs associated with FRBs 20121102A and 20190520B, two known active galactic nuclei (AGN), and one likely AGN in our sample with comparable data, as well as other radio transients exhibiting characteristics analogous to FRB-PRSs. We identify a single source in our sample, J1136+2643, as the most promising FRB- PRS, based on its compact physical size and host-normalized offset. We further identify two sources, J0019+1507 and J0909+5955, with physical sizes comparable to FRB-PRSs, but which exhibit large offsets and flat spectral indices potentially indicative of a background AGN origin. We test the viability of neutron star wind nebulae and hypernebulae models for J1136+2643, and find that the physical size, luminosity, and SED of J1136+2643 are broadly consistent with these models. Finally, we discuss the alternative interpretation that the radio sources are instead powered by accreting massive black holes and outline future prospects and follow-up observations for differentiating between these scenarios.",
        "citation_title": "A Radio Study of Persistent Radio Sources in Nearby Dwarf Galaxies: Implications for Fast Radio Bursts",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The GAMA J0913$-$0107 system is a rare conjunction of a submillimeter galaxy (SMG) at $z \\approx 2.7$ and two background QSOs with projected separations $<$200 kpc. Previous high-resolution QSO absorption-line spectroscopy has revealed high H $\\tiny{\\rm I}$ column density, extremely metal-poor ($\\sim 1\\%$ solar) gas streams in the circumgalactic medium of the SMG. Here we present deep optical integral-field spectroscopy of the system with the Keck Cosmic Web Imager (KCWI). Reaching a $2\\sigma$ surface brightness (SB) limit $\\approx 10^{-19}$ erg s$^{-1}$ cm$^{-2}$ arcsec$^{-2}$ with $\\sim$2 hours of integration time, we detect a filamentary Ly$\\alpha$ nebula stretching $\\sim$180 kpc from the SMG intercepting both QSO sightlines. This Ly$\\alpha$ filament may correspond to the same cool gas stream penetrating through the hot halo seen in the absorption. In contrast to Ly$\\alpha$ nebulae around QSOs, there is no obvious local source for photoionization due to the massive dust content. While uncertain, we consider the possibility that the nebula is ionized by shocks induced by the infall, obscured star formation, and/or a boosted UV background. The SMG-QSOs conjunction multiplied the efficiency of the KCWI observations, allowing a direct comparison of Ly$\\alpha$ nebulae in two distinct environments. We find that the nebula around the QSOs are much brighter and show steeper surface brightness profiles than the SMG nebula. This is consistent with the additional photoionization and Ly$\\alpha$ scattering provided by the QSOs. While illustrating the challenges of detecting Ly$\\alpha$ nebulae around SMGs, our work also demonstrates that important insights can be gained from comparative studies of high-$z$ Ly$\\alpha$ nebulae.",
        "citation_title": "H1 Ly$\u03b1$ Emission from a Metal-Poor Cool Stream Fueling an Early Dusty Starburst",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The kinetic Sunyaev Zel'dovich (kSZ) effect is a blackbody cosmic microwave background (CMB) temperature anisotropy induced by Thomson scattering off free electrons in bulk motion with respect to the CMB rest frame. The statistically anisotropic cross-correlation between the CMB and galaxy surveys encodes the radial bulk velocity (more generally, the remote dipole field), which can be efficiently reconstructed using a quadratic estimator. Here, we develop and implement a quadratic estimator for the remote dipole field to data from the Planck satellite and the unWISE galaxy redshift catalog. With this data combination, we forecast a $\\sim 1$-$\\sigma$ detection within $\\Lambda$CDM assuming a simple model for the distribution of free electrons. Using reconstructions based on individual frequency temperature maps, we characterize the impact of foregrounds, concluding that they can be effectively mitigated by masking and removing the estimator monopole. We demonstrate that reconstructions based on component-separated CMB maps have no detectable biases from foregrounds or systematics at the level of the expected statistical error. We use these reconstructions to constrain the multiplicative optical depth bias to $b_v < 1.40$ at $68 \\%$ confidence. Our fiducial signal model with $b_v =1$ is consistent with this measurement. Our results support an optimistic future for kSZ velocity reconstruction with near-term datasets.",
        "citation_title": "Kinetic Sunyaev Zel'dovich velocity reconstruction from Planck and unWISE",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The observed rest-UV luminosity function at cosmic dawn ($z \\sim 8-14$) measured by JWST revealed an excess of UV-luminous galaxies relative to many pre-launch theoretical predictions. A high star-formation efficiency (SFE) and a top-heavy initial mass function (IMF) are among the mechanisms proposed for explaining this excess. Although a top-heavy IMF has been proposed for its ability to increase the light-to-mass ratio (\\(\\Psi_{\\mathrm{UV}}\\)), the resulting enhanced radiative pressure from young stars could decrease the star formation efficiency (SFE), potentially driving galaxy luminosities back down. In this Letter, we use idealized radiation hydrodynamic simulations of star cluster formation to explore the effects of a top-heavy IMF on the SFE of clouds typical of the high pressure conditions found at these redshifts. We find that the SFE in star clusters with solar neighbourhood-like dust abundance decreases with increasingly top-heavy IMF's -- by $\\sim 20 \\%$ for an increase of factor 4 in $\\Psi_{\\mathrm{UV}}$, and by $50 \\%$ for a factor $ \\sim 10$ in $\\Psi_{\\mathrm{UV}}$. However, we find that an expected decrease in the dust-to-gas ratio ($\\sim 0.01 \\times \\mathrm{Solar}$) at these redshifts can completely compensate for the enhanced light output. This leads to a (cloud-scale; $\\sim 10 \\, \\mathrm{pc}$) SFE that is $\\gtrsim 70\\%$ even for a factor 10 increase in $\\Psi_{\\mathrm{UV}}$, implying that highly efficient star formation is unavoidable for high surface density and low metallicity conditions. Our results suggest that a top-heavy IMF, if present, likely coexists with efficient star formation in these galaxies.",
        "citation_title": "The Interplay between the IMF and Star Formation Efficiency through Radiative Feedback at High Stellar Surface Densities",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The baryon acoustic oscillation (BAO) analysis from the first year of data from the Dark Energy Spectroscopic Instrument (DESI), when combined with data from the cosmic microwave background (CMB), has placed an upper-limit on the sum of neutrino masses, $\\sum m_\\nu < 70$ meV (95%). In addition to excluding the minimum sum associated with the inverted hierarchy, the posterior is peaked at $\\sum m_\\nu = 0$ and is close to excluding even the minumum sum, 58 meV at 2$\\sigma$. In this paper, we explore the implications of this data for cosmology and particle physics. The sum of neutrino mass is determined in cosmology from the suppression of clustering in the late universe. Allowing the clustering to be enhanced, we extended the DESI analysis to $\\sum m_\\nu < 0$ and find $\\sum m_\\nu = - 160 \\pm 90$ meV (68%), and that the suppression of power from the minimum sum of neutrino masses is excluded at 99% confidence. We show this preference for negative masses makes it challenging to explain the result by a shift of cosmic parameters, such as the optical depth or matter density. We then show how a result of $\\sum m_\\nu =0$ could arise from new physics in the neutrino sector, including decay, cooling, and/or time-dependent masses. These models are consistent with current observations but imply new physics that is accessible in a wide range of experiments. In addition, we discuss how an apparent signal with $\\sum m_\\nu < 0$ can arise from new long range forces in the dark sector or from a primordial trispectrum that resembles the signal of CMB lensing.",
        "citation_title": "No $\u03bd$s is Good News",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Stellar flares are short-duration ($<$ hours) bursts of radiation associated with surface magnetic reconnection events. Stellar magnetic activity generally decreases as a function of both age and Rossby number, $R_0$, a measure of the relative importance of the convective and rotational dynamos. Young stars ($<300$ Myr) have typically been overlooked in population-level flare studies due to challenges with flare-detection methods. Here, we select a sample of stars that are members of 26 nearby moving groups, clusters, or associations with ages $<$300 Myr that have been observed by the Transiting Exoplanet Survey Satellite at 2-minute cadence. We identified 26,355 flares originating from 3,157 stars and robustly measure the rotation periods of 1,847 stars. We measure and find the flare frequency distribution (FFD) slope, $\\alpha$, saturates for all spectral types at $\\alpha \\sim -0.5$ and is constant over 300 Myr. Additionally, we find that flare rates for stars $t_\\textrm{age} = 50 - 250$ Myr are saturated below $R_0 < 0.14$, which is consistent with other indicators of magnetic activity. We find evidence of annual flare rate variability in eleven stars, potentially correlated with long term stellar activity cycles. Additionally, we cross match our entire sample with GALEX and find no correlation between flare rate and Far- and Near-Ultraviolet flux. Finally, we find the flare rates of planet hosting stars are relatively lower than comparable, larger samples of stars, which may have ramifications for the atmospheric evolution of short-period exoplanets.",
        "citation_title": "Evolution of Flare Activity in GKM Stars Younger than 300 Myr over Five Years of TESS Observations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The tidal disruption event (TDE) AT2018fyk has unusual X-ray, UV, and optical light curves that decay over the first $\\sim$600d, rebrighten, and decay again around 1200d. We explain this behavior as a one-off TDE associated with a massive black hole (BH) \\emph{binary}. The sharp drop-offs from $t^{-5/3}$ power laws at around 600d naturally arise when one BH interrupts the debris fallback onto the other BH. The BH mass $M_\\bullet$ derived from fitting X-ray spectra with a slim disk accretion model and, independently, from fitting the early UV/optical light curves, is smaller by two orders of magnitude than predicted from the $M_\\bullet$--$\\sigma_*$ host galaxy relation, suggesting that the debris is accreted onto the secondary, with fallback cut off by the primary. Furthermore, if the rebrightening were associated with the primary, it should occur around 5000d, not the observed 1200d. The secondary's mass and dimensionless spin is $M_{\\bullet,{\\rm s}}=2.7^{+0.5}_{-1.5} \\times 10^5 M_\\odot$ and $a_{\\bullet,{\\rm s}}>0.3$ (X-ray spectral fitting), while the primary's mass is $M_{\\bullet,{\\rm p}}=10^{7.7\\pm0.4}M_\\odot$ ($M_\\bullet$-$\\sigma_*$ relation). An intermediate mass BH secondary is consistent with the observed UV/optical light curve decay, i.e., the secondary's outer accretion disk is too faint to produce a detectable emission floor. The time of the first accretion cutoff constrains the binary separation to be $(6.7\\pm 1.2) \\times 10^{-3}~{\\rm pc}$. X-ray spectral fitting and timing analysis indicate that the hard X-rays arise from a corona above the secondary's disk. The early UV/optical emission, suggesting a super-Eddington phase for the secondary, possibly originates from shocks arising from debris circularization.",
        "citation_title": "AT2018fyk: Candidate Tidal Disruption Event by a (Super)massive Black Hole Binary",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Continuous wavelet analysis is gaining popularity in science and engineering for its ability to analyze data across spatial and scale domain simultaneously. In this study, we introduce a wavelet-based method to identify halos and assess its feasibility in two-dimensional (2D) scenarios. We begin with the generation of four pseudo-2D datasets from the SIMBA dark matter simulation by compressing thin slices of three-dimensional (3D) data into 2D. We then calculate the continuous wavelet transform (CWT) directly from the particle distributions, identify local maxima that represent actual halos, and segment the CWT to delineate halo boundaries. A comparison with the traditional Friends-of-Friends (FOF) method shows that our CWT-identified halos, while containing slightly fewer particles, have smoother boundaries and are more compact in dense regions. In contrast, the CWT method can link particles over greater distances to form halos in sparse regions due to its spatial segmentation scheme. The spatial distribution and halo power spectrum of both CWT and FOF halos demonstrate substantial consistency, validating the 2D applicability of CWT for halo detection. Our identification scheme operates with a linear time complexity of $\\mathcal{O}(N)$, suggesting its suitability for analyzing significantly larger datasets in the future.",
        "citation_title": "Identifying Halos in Cosmological Simulations with Continuous Wavelet Analysis: The 2D Case",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Astronomical (or Milankovi\u0107) forcing of the Earth system is key to understanding rhythmic climate change on time scales >~ 10 kyr. Paleoceanographic and paleoclimatological applications concerned with past astronomical forcing rely on astronomical calculations (solutions), which represent the backbone of cyclostratigraphy and astrochronology. Here we present state-of-the-art astronomical solutions over the past 3.5 Gyr. Our goal is to provide tuning targets and templates for interpreting deep-time cyclostratigraphic records and designing external forcing functions in climate models. Our approach yields internally consistent orbital and precession-tilt solutions, including fundamental solar system frequencies, orbital eccentricity and inclination, lunar distance, luni-solar precession rate, Earth's obliquity, and climatic precession. Contrary to expectations, we find that the long eccentricity cycle (previously assumed stable and labeled ''metronome'', recent period ~405 kyr), can become unstable on long time scales. Our results reveal episodes during which the long eccentricity cycle is very weak or absent and Earth's orbital eccentricity and climate-forcing spectrum are unrecognizable compared to the recent past. For the ratio of eccentricity-to-inclination amplitude modulation (frequently observable in paleorecords) we find a wide distribution around the recent 2:1 ratio, i.e., the system is not restricted to a 2:1 or 1:1 resonance state. Our computations show that Earth's obliquity was lower and its amplitude (variation around the mean) significantly reduced in the past. We therefore predict weaker climate forcing at obliquity frequencies in deep time and a trend toward reduced obliquity power with age in stratigraphic records. For deep-time stratigraphic and modeling applications, the orbital parameters of our 3.5-Gyr integrations are made available at 400-year resolution.",
        "citation_title": "Milankovi\u0107 Forcing in Deep Time",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Lenticular galaxies are notoriously misclassified as elliptical galaxies and, as such, a (disc inclination)-dependent correction for dust is often not applied to the magnitudes of dusty lenticular galaxies. This results in overly red galaxy colours, impacting their distribution in the colour-magnitude diagram. It is revealed how this has led to an underpopulation of the `green valley' by hiding a `green mountain' of massive dust-rich lenticular galaxies - known to be built from gas-rich major mergers - within the `red sequence' of colour-(stellar mass) diagrams. Correcting for dust, a `green mountain' appears at $M_{\\rm *,gal}\\sim10^{11}$ M$_\\odot$, along with signs of an extension to lower masses producing a `green range' or `green ridge' on the green side of the `red sequence' and `blue cloud.' The `red sequence' is shown to be comprised of two components: a red plateau defined by elliptical galaxies with a near-constant colour and by lower-mass dust-poor lenticular galaxies, which are mostly a primordial population but may include faded/transformed spiral galaxies. The quasi-triangular-shaped galaxy evolution sequence, previously called the `Triangal', is revealed in the galaxy colour-(stellar mass) diagram. It tracks the speciation of galaxies and their associated migration through the diagram. The connection of the `Triangal' to previous galaxy morphology sequences (Fork, Trident, Comb) is also shown herein. Finally, the colour-(black hole mass) diagram is revisited, revealing how the dust correction generates a blue-green sequence for the spiral $and$ dust-rich lenticular galaxies that is offset from a green-red sequence defined by the dust-poor lenticular and elliptical galaxies.",
        "citation_title": "Repainting the colour-mass diagrams by unearthing the green mountain: dust-rich S0 galaxies in the colour-(galaxy stellar mass) diagram, and the colour-(black hole mass) relations for dust-poor versus dust-rich galaxies",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Space Weather is the study of the conditions in the solar wind that can affect life on the surface of the Earth, particularly the increasingly technologically sophisticated devices that are part of modern life. Solar radio observations are relevant to such phenomena because they generally originate as events in the solar atmosphere, including flares, coronal mass ejections and shocks, that produce electromagnetic and particle radiations that impact the Earth. Low frequency solar radio emission arises in the solar atmosphere at the levels where these events occur: we can use frequency as a direct measure of density, and an indirect measure of height, in the atmosphere. The main radio burst types are described and illustrated using data from the Green Bank Solar Radio Burst Spectrometer, and their potential use as diagnostics of Space Weather is discussed.",
        "citation_title": "Solar Radio Bursts and Space Weather",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this contribution, we present a short account of gravitational lenses and how to calculate different properties of its images in the case of having a transparent distribution of matter such as the uniform transparent sphere, isothermal gas sphere, non-singular isothermal gas sphere and a transparent King profile. With the help of XFGLenses software, and numerical methods, different images arising from all of these profiles, and the different caustics and critical curves are shown. The images were consistent with several previous results that are expected for transparent profiles, like having an odd number of images, and reducing the number of images by two when the source passes through the caustic. The curves shown in the caustics where the diamond, the ellipse and the lemniscate-like. For the critical curves, the most common curve was the ellipse, and the lemniscate appeared in the transparent NSIS case, which is consistent with the fact that these curves are common in gravitational lenses.",
        "citation_title": "Transparent Spheres as Gravitational Lens",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Euclid is a recently launched medium-class mission by the European Space Agency (ESA) designed to measure cosmological parameters, test the cosmological standard model, and explore the nature of dark matter and dark energy. To this end, Euclid conducts a survey of up to 14000 square degrees of the extra-galactic sky and obtains optical and near-infrared photometric measurements for more than a billion galaxies as well as near-infrared slitless spectroscopy for more than 35 million galaxies. These observations will be used to estimate galaxy clustering and cosmic shear. It is expected that Euclid will achieve percent-level constraints on the Dark Energy equation of state parameter. The survey will also be exploited with a range of other cosmological probes and prove revolutionary for non-cosmological science.",
        "citation_title": "Euclid -- The Dark Universe detective",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Atomic carbon in its ground electronic state, C(3P), is expected to be present at high abundances during the evolution of dense molecular clouds. Consequently, its reactions with other interstellar species could have a strong influence on the chemical composition of these regions. Here, we report the results of an investigation of the reaction between C(3P) and dimethylether, CH3OCH3, which was recently detected in dark cloud TMC-1. Experiments were performed to study the kinetics of this reaction using a continuous supersonic flow reactor employing pulsed laser photolysis and pulsed laser induced fluorescence for atomic radical generation and detection respectively. Rate constants for this process were measured between 50 K and 296 K, while additional measurements of the product atomic hydrogen yields were also performed over the 75-296 K range. To better understand the experimental results, statistical rate theory was used to calculate rate constants over the same temperature range and to provide insight on the major product channels. These simulations, based on quantum chemical calculations of the ground triplet state of the C3H6O molecule, allowed us to obtain the most important features of the underlying potential energy surface. The measured rate constant increases as the temperature falls, reaching a value of k_(C+CH_3 OCH_3 )= 7.5 x 10-11 cm3 s-1 at 50 K, while the low measured H-atom yields support the theoretical prediction that the major reaction products are CH3 + CH3 + CO. The effects of this reaction on the abundances of interstellar CH3OCH3 and related species were tested using a gas-grain dense cloud model, employing an expression for the rate constant, k(T) = alpha(T/300)^beta, with alpha = 1.27 x 10-11 and beta = -1.01. These simulations predict that the C(3P) + CH3OCH3 reaction decreases gas-phase CH3OCH3 abundances by more than an order of magnitude at early times.",
        "citation_title": "A Low Temperature Kinetic Study of the C(3P) + CH3OCH3 Reaction. Rate constants, H-atom Product Yields and Astrochemical Implications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Patchy reionization generates kinematic Sunyaev-Zeldovich (kSZ) anisotropies in the cosmic microwave background (CMB). Large-scale velocity perturbations along the line of sight modulate the small-scale kSZ power spectrum, leading to a trispectrum (or four-point function) in the CMB that depends on the physics of reionization. We investigate the challenges in detecting this trispectrum and use tools developed for CMB lensing, such as realization-dependent bias subtraction and cross-correlation based estimators, to counter uncertainties in the instrumental noise and assumed CMB power spectrum. We also find that both lensing and extragalactic foregrounds can impart larger trispectrum contributions than the reionization kSZ signal. We present a range of mitigation methods for both of these sources of contamination, validated on microwave-sky simulations. We use ACT DR6 and Planck data to calculate an upper limit on the reionization kSZ trispectrum from a measurement dominated by foregrounds. The upper limit is about 50 times the signal predicted from recent simulations.",
        "citation_title": "The Atacama Cosmology Telescope: Reionization kSZ trispectrum methodology and limits",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "V2487 Ophiuchi (V2487 Oph) is a recurrent nova with classical nova eruptions in 1900 and 1998, and it is also the most extreme known superflare star. These superflares are roughly-hour-long flares with amplitudes and optical energies reaching up to 1.10 mag and $10^{39.21}$ ergs, with the superflares recurring once-a-day. The V2487 Oph superflares are certainly operating with the same mechanism as all the other types of superflare stars, where magnetic loops are twisted and stretched until reconnection occurs, whereupon ambient electrons are accelerated to relativistic energies and then emitted bremsstrahlung radiation from X-ray to radio. V2487 Oph is unique among known superflare stars in that one of the loop footprints is in an accretion disk. This exact mechanism was theoretically predicted by M. R. Hayashi and colleagues in 1996. Now, I have found two superflares recorded on Harvard archival photographs from the years 1941 and 1942. These two superflares have $B$ magnitude amplitudes of $>$1.83 and $>$2.00 mag and total radiated energies of $10^{42.4}$ and $10^{42.5}$ ergs with bolometric corrections. Each has emitted energies of $\\sim$30-billion Carringtons, in units of the most energetic solar flare. Further, I find superflares in the Zwicky Transient Factory light curves, so V2487 Oph has been superflaring from 1941 to 2023. For the observed number distribution of $dN/dE$=$4E^{-2}$ superflares per year, for $E$ in units of $10^{41}$ ergs, the emitted energy in superflare light is $10^{42.1}$ erg in each year, or $10^{44.1}$ ergs from 1941 to 2023.",
        "citation_title": "Recurrent Nova V2487 Oph Had Superflares in 1941 and 1942 With Radiant Energies 10$^{42.5\\pm1.6}$ Ergs",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Aims. We aim to resolve the spatial and kinematic sub-structures in five detached-shell sources to provide detailed constraints for hydrodynamic models that describe the formation and evolution of the shells. Methods. We use observations of the 12 CO (1-0) emission towards five carbon-AGB stars with ALMA. The data have angular resolutions of 0.3 arcsec to 1arcsec and a velocity resolution of 0.3 km/s . This enables us to quantify spatial and kinematic structures in the shells. Results. The observed emission is separated into two distinct components: a more coherent, bright outer shell and a more filamentary, fainter inner shell. The kinematic information shows that the inner sub-shells move at a higher velocity relative to the outer sub-shells. The observed sub-structures confirm the predictions from hydrodynamical models. However, the models do not predict a double-shell structure, and the CO emission likely only traces the inner and outer edges of the shell, implying a lack of CO in the middle layers of the detached shell. Previous estimates of the masses and temperatures are consistent with originating mainly from the brighter subshell, but the total shell masses are likely lower limits. Conclusions. The observed spatial and kinematical splittings of the shells appear consistent with results from hydrodynamical models, provided the CO emission does not trace the H2 density distribution in the shell but rather traces the edges of the shells. It is therefore not possible to constrain the total shell mass based on the CO observations alone. Complementary observations of, e.g., CI as a dissociation product of CO would be necessary to understand the distribution of CO compared to H2.",
        "citation_title": "Probing the dynamical and kinematical structures of detached shells around AGB stars",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Wolf-Rayet stars in close binary systems can be tidally spun up by their companions, potentially leaving behind fast-spinning highly-magnetized neutron stars, known as ``magnetars\", after core collapse. These newborn magnetars can transfer rotational energy into heating and accelerating the ejecta, producing hydrogen-poor superluminous supernovae (SLSNe). In this {\\em{Letter}}, we propose that the magnetar wind of the newborn magnetar could significantly evaporate its companion star, typically a main-sequence or helium star, if the binary system is not disrupted by the SN kick. The subsequent heating and acceleration of the evaporated star material along with the SN ejecta by the magnetar wind can produce a post-peak bump in the SLSN lightcurve. Our model can reproduce the primary peaks and post-peak bumps of four example observed multiband SLSN lightcurves, revealing that the mass of the evaporated material could be $\\sim0.4-0.6\\,M_\\odot$ if the material is hydrogen-rich. We suggest that the magnetar could induce strongly enhanced evaporation from its companion star near the pericenter if the orbit of the post-SN binary is highly eccentric, ultimately generating multiple post-peak bumps in the SLSN lightcurves. This ``magnetar-star binary engine\" model offers a possible explanation for the evolution of polarization, along with the origin and velocity broadening of late-time hydrogen or helium broad spectral features observed in some bumpy SLSNe. The diversity in the lightcurves and spectra of SLSNe may be attributed to the wide variety of companion stars and post-SN binary systems.",
        "citation_title": "Bumpy Superluminous Supernovae Powered by Magnetar-star Binary Engine",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Nova shocks behave like scaled-down supernova remnant shocks with a lifetime of only a few weeks or months, thereby providing a unique opportunity to study the dynamics of non-relativistic shocks as well as the shock acceleration physics.Recently, GeV and TeV gamma-ray emissions from an outburst of the recurrent nova RS Ophiuchi have been observed. The light curves of the gamma-ray emissions suggest that they arise from an external shock, which is formed as the nova ejecta interacts with the ambient medium. The shock is thought to transit from an adiabatic shock to a radiative one at later times, but no such later observations are available for RS Ophiuchi. In addition, the spectral evolution of the gamma-ray outburst of RS Ophiuchi was not well measured, and hence the related particle acceleration mechanisms are not well understood. T Coronae Borealis (T CrB) is another recurrent nova in Milky Way and its last outburst was nearly ten times brighter than RS Ophiuichi. Recently the optical light curve of T CrB displayed a state transition behavior before the eruption, and it was predicted that T CrB will undergo an outburst in the near future. By performing a theoretical investigation, we find that Fermi-LAT could capture the transition of the shock from the adiabatic phase to the radiative phase at the GeV band, thanks to a longer detectable time than that of RS Ophiuchi.Due to its higher brightness, we also find that imaging atmospheric Cherenkov telescopes (IACTs) such as MAGIC and VERITAS, and extensive air shower experiments such as LHAASO could detect the nova outburst and measure the gamma-ray spectrum in the very-high-energy (VHE, energy above 0.1 TeV) band more precisely. This can be used to constrain the high-energy cutoff index in the accelerated proton spectrum and the acceleration efficiency, which will shed light on the particle acceleration physics in nova shocks.",
        "citation_title": "Probing the nova shock physics with future gamma-ray observations of the upcoming outburst from T Coronae Borealis",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Massive stars rotate faster, on average, than lower mass stars. Stellar rotation triggers hydrodynamical instabilities which transport angular momentum and chemical species from the core to the surface. Models of high-mass stars that include these processes predict that chemical mixing is stronger at lower metallicity. We aim to test this prediction by comparing the surface abundances of massive stars at different metallicities. We performed a spectroscopic analysis of single O stars in the Magellanic Clouds (MCs) based on the ULLYSES and XshootU surveys. We determined the fundamental parameters and helium, carbon, nitrogen, and oxygen surface abundances of 17 LMC and 17 SMC non-supergiant O6-9.5 stars. We complemented these determinations by literature results for additional MCs and also Galactic stars to increase the sample size and metallicity coverage. We investigated the differences in the surface chemical enrichment at different metallicities and compared them with predictions of three sets of evolutionary models. Surface abundances are consistent with CNO-cycle nucleosynthesis. The maximum surface nitrogen enrichment is stronger in MC stars than in Galactic stars. Nitrogen enrichment is also observed in stars with higher surface gravities in the SMC than in the Galaxy. This trend is predicted by models that incorporate chemical transport caused by stellar rotation. The distributions of projected rotational velocities in our samples are likely biased towards slow rotators. A metallicity dependence of surface abundances is demonstrated. The analysis of larger samples with an unbiased distribution of projected rotational velocities is required to better constrain the treatment of chemical mixing and angular momentum transport in massive single and binary stars.",
        "citation_title": "X-Shooting ULLYSES: Massive stars at low metallicity -- V. Effect of metallicity on surface abundances of O stars",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "White dwarfs are the dense, burnt-out remnants of the vast majority of stars, condemned to cool over billions of years as they steadily radiate away their residual thermal energy. To first order, their atmosphere is expected to be made purely of hydrogen due to the efficient gravitational settling of heavier elements. However, observations reveal a much more complex situation, as the surface of a white dwarf (1) can be dominated by helium rather than hydrogen, (2) can be polluted by trace chemical species, and (3) can undergo significant composition changes with time. This indicates that various mechanisms of element transport effectively compete against gravitational settling in the stellar envelope. This phenomenon is known as the spectral evolution of white dwarfs and has important implications for Galactic, stellar, and planetary astrophysics. This invited review provides a comprehensive picture of our current understanding of white dwarf spectral evolution. We first describe the latest observational constraints on the variations in atmospheric composition along the cooling sequence, covering both the dominant and trace constituents. We then summarise the predictions of state-of-the-art models of element transport in white dwarfs and assess their ability to explain the observed spectral evolution. Finally, we highlight remaining open questions and suggest avenues for future work.",
        "citation_title": "The spectral evolution of white dwarfs: where do we stand?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present new calculations of atomic data needed to model autoionizing states of Fe XVI. We compare the state energies, radiative and excitation data with a sample of results from previous literature. We find a large scatter of results, the most significant ones in the autoionization rates, which are very sensitive to the configuration interaction and state mixing. We find relatively good agreement between the autoionization rates and the collisional excitation rates calculated with the R-matrix suite of programs and autostructure. The largest model, which includes J-resolved states up to n=10, produces ab-initio wavelengths and intensities of the satellite lines which agree well with solar high-resolution spectra of active regions, with few minor wavelength adjustements. We review previous literature, finding many incorrect identifications, most notably those in the NIST database. We provide several new tentative identifications in the 15-15.7 A range, and several new ones at shorter wavelengths, where previous lines were unidentified. Compared to the previous CHIANTI model, the present one has an increased flux in the 15--15.7 A range at 2 MK of a factor of 1.9, resolving the discrepancies found in the analysis of the Marshall Grazing Incidence X-Ray Spectrometer (MaGIXS) observation. It appears that the satellite lines also resolve the long-standing discrepancy in the intensity of the important Fe XVII 3D line at 15.26 A.",
        "citation_title": "Satellite lines from autoionizing states of Fe XVI and the problems with the X-ray Fe XVII lines",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Solar white-light flares are characterized by an enhancement in the optical continuum, which are usually large flares (say X- and M-class flares). Here we report a small C2.3 white-light flare (SOL2022-12-20T04:10) observed by the \\emph{Advanced Space-based Solar Observatory} and the \\emph{Chinese H$\\alpha$ Solar Explorer}. This flare exhibits an increase of $\\approx$6.4\\% in the photospheric Fe \\textsc{i} line at 6569.2\\,\u00c5 and {$\\approx$3.2\\%} in the nearby continuum. The continuum at 3600\\,\u00c5 also shows an enhancement of $\\approx$4.7\\%. The white-light brightening kernels are mainly located at the flare ribbons and co-spatial with nonthermal hard X-ray sources, which implies that the enhanced white-light emissions are related to nonthermal electron-beam heating. At the brightening kernels, the Fe \\textsc{i} line displays an absorption profile that has a good Gaussian shape, with a redshift up to $\\approx$1.7 km s$^{-1}$, while the H$\\alpha$ line shows an emission profile though having a central reversal. The H$\\alpha$ line profile also shows a red or blue asymmetry caused by plasma flows with a velocity of several to tens of km s$^{-1}$. It is interesting to find that the H$\\alpha$ asymmetry is opposite at the conjugate footpoints. It is also found that the CHASE continuum increase seems to be related to the change of photospheric magnetic field. Our study provides comprehensive characteristics of a small white-light flare that help understand the energy release process of white-light flares.",
        "citation_title": "Spectral and Imaging Observations of a C2.3 White-Light Flare from the Advanced Space-Based Solar Observatory (ASO-S) and the Chinese H$\u03b1$ Solar Explorer (CHASE)",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "3-Hydroxypropenal (HOCHCHCHO) is the lower energy tautomer of malonaldehyde which displays a complex rotation-tunneling spectrum. It was detected tentatively toward the solar-type protostar IRAS 16293$-$2422 B with ALMA in the framework of the Protostellar Interferometric Line Survey (PILS). Several transitions, however, had large residuals, preventing not only their detection, but also the excitation temperature of the species from being determined unambiguously. We want to extend the existing rotational line list of 3-hydroxypropenal to shed more light on the recent observational results and to facilitate additional radio astronomical searches for this molecule. We analyzed the rotation-tunneling spectrum of 3-hydroxypropenal in the frequency regions between 150 and 330 GHz and between 400 and 660 GHz. Transitions were searched for in the PILS observations of IRAS 16293$-$2422. Local thermodynamic equilibrium (LTE) models were carried out and compared to the observations to constrain the excitation temperature. Additional transitions were searched for in other ALMA archival data of the same source to confirm the presence of 3-hydroxypropenal. More than 11500 transitions were assigned in the course of our investigation with quantum numbers $2 \\le J \\le 100$, $K_a \\le 59$, and $K_c \\le 97$, resulting in a greatly improved set of spectroscopic parameters. The comparison between the LTE models and the observations yields an excitation temperature of 125 K with a column density $N = 1.0 \\times 10^{15}$ cm$^{-2}$ for this species. We identified seven additional lines of 3-hydroxypropenal that show a good agreement with the model in the ALMA archive data. The calculated rotation-tunneling spectrum of 3-hydroxypropenal has sufficient accuracy for radio astronomical searches. The detection of 3-hydroxypropenal toward IRAS 16293$-$2422 B is now secure.",
        "citation_title": "The rotation-tunneling spectrum of 3-hydroxypropenal and confirmation of its detection toward IRAS 16293$-$2422 B",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Discrete Source Classifier (DSC) provides probabilistic classification of sources in Gaia Data Release 3 using a Bayesian framework and a global prior. The DSC Combmod classifier in GDR3 achieved for the extragalactic classes (quasars and galaxies) a high completeness of 92% but a low purity of 22% due to contamination from the far larger star class. However, these single metrics mask significant variation in performance with magnitude and sky position. Furthermore, a better combination of the individual classifiers is possible. Here we compute two-dimensional representations of the completeness and the purity as function of Galactic latitude and source brightness, and exclude also the Magellanic Clouds where stellar contamination significantly depresses the purity. Reevaluated on a cleaner validation set and without introducing changes to the published GDR3 DSC probabilities themselves, we achieve for Combmod average 2-d completenesses of 92% and 95% and average 2-d purities of 55% and 89% for the quasar and galaxy classes respectively. Since the relative proportions of extragalactic objects to stars in Gaia is expected to vary significantly with brightness and latitude, we introduce a new prior as a continuous function of brightness and latitude, and compute new class probabilities. This variable prior only improves the performance by a few percentage points, mostly at the faint end. Significant improvement, however, is obtained by a new additive combination of Specmod and Allosmod. This classifier, Combmod-$\\alpha$, achieves average 2-d completenesses of 82% and 93% and average 2-d purities of 79% and 93% for the quasar and galaxy classes respectively when using the global prior. Thus we achieve a significant improvement in purity for a small loss of completeness. The improvement is most significant for faint quasars where the purity rises from 20% to 62%.",
        "citation_title": "Improved source classification and performance analysis using Gaia DR3",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Constraining the processes that drive coronal heating from observations is a difficult task due to the complexity of the solar atmosphere. As upcoming missions such as MUSE will provide coronal observations with unprecedented spatial and temporal resolution, numerical simulations are becoming increasingly realistic. Despite the availability of synthetic observations from numerical models, line-of-sight effects and the complexity of the magnetic topology in a realistic setup still complicate the prediction of signatures for specific heating processes. 3D MHD simulations have shown that a significant part of the Poynting flux injected into the solar atmosphere is carried by small-scale motions, such as vortices driven by rotational flows inside intergranular lanes. MHD waves excited by these vortices have been suggested to play an important role in the energy transfer between different atmospheric layers. Using synthetic spectroscopic data generated from a coronal loop model incorporating realistic driving by magnetoconvection, we study whether signatures of energy transport by vortices and eventual dissipation can be identified with future missions such as MUSE.",
        "citation_title": "MUSE observations of small-scale heating events",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Cosmological emulators of observables such as the Cosmic Microwave Background (CMB) spectra and matter power spectra commonly use training data sampled from a Latin hypercube. This method often incurs high computational costs by covering less relevant parts of the parameter space, especially in high dimensions where only a small fraction of the parameter space yields a significant likelihood.\nIn this paper, we introduce hypersphere sampling, which instead concentrates sample points in regions with higher likelihoods, significantly enhancing the efficiency and accuracy of emulators. A novel algorithm for sampling within a high-dimensional hyperellipsoid aligned with axes of correlation in the cosmological parameters is presented. This method focuses the distribution of training data points on areas of the parameter space that are most relevant to the models being tested, thereby avoiding the computational redundancies common in Latin hypercube approaches.\nComparative analysis using the \\textsc{connect} emulation tool demonstrates that hypersphere sampling can achieve similar or improved emulation precision with more than an order of magnitude fewer data points and thus less computational effort than traditional methods. This was tested for both the $\\Lambda$CDM model and a 5-parameter extension including Early Dark Energy, massive neutrinos, and additional ultra-relativistic degrees of freedom. Our results suggest that hypersphere sampling holds potential as a more efficient approach for cosmological emulation, particularly suitable for complex, high-dimensional models.",
        "citation_title": "Cutting corners: Hypersphere sampling as a new standard for cosmological emulators",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The granulation of red supergiants (RSGs) in the Magellanic Clouds are systematically investigated by combining the latest RSGs samples and light curves from the Optical Gravitational Lensing Experiment and the All-Sky Automated Survey for Supernovae. The present RSGs samples are firstly examined for foreground stars and possible misidentified sources, and the light curves are sequentially checked to remove the outliers by white noise and photometric quality. The Gaussian Process regression is used to model the granulation, and the Markov Chain Monte Carlo is applied to derive the granulation amplitude $\\sigma$ and the period of the undamped oscillator $\\rho$, as well as the damping timescale $\\tau$. The dimensionless quality factor $Q$ is then calculated through $Q=\\pi \\tau/\\rho$. RSGs around $Q = 1/\\sqrt{2}$ are considered to have significant granulation signals and are used for further analysis. Combining granulation parameters with stellar parameters, robust scaling relations for the timescale $\\rho$ are established, while the scaling relations for amplitude $\\sigma$ are represented by a piecewise function, possibly related to the tendency of amplitudes in faint RSGs to converge towards a certain value. Comparing results between the SMC and LMC confirms that amplitudes and timescales become larger with metallicity. In examining the scaling relations between the two galaxies, it is found that $\\rho$ is nearly independent of metallicity, whereas $\\sigma$ is more significantly affected by metallicity. The Gaussian Process method is compared with the periodogram fitting of the granulations, and the advantages of either are discussed.",
        "citation_title": "Modeling of Granulation in Red Supergiants in the Magellanic Clouds with the Gaussian Process Regressions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present a detailed analysis of the dynamics of proto-compact star (PCS) convection and the core ${}^2\\!g_1$-mode in core-collapse supernovae based on general relativistic 2D and 3D neutrino hydrodynamics simulations. Based on 2D simulations, we derive a mode relation for the core $g$-mode frequency in terms of PCS and equation of state parameters, and discuss its limits of accuracy. This relation may prove useful for parameter inference from future supernova gravitational wave (GW) signals if the core $g$-mode or an emission gap at the avoided crossing with the fundamental mode can be detected. The current 3D simulation does not show GW emission from the core $g$-mode due to less power in high-frequency convective motions to excite the mode, however. Analysing the dynamics of PCS convection in 3D, we find that simple scaling laws for convective velocity from mixing-length theory (MLT) do not apply. Energy and lepton number transport is instead governed by a more complex balance between neutrino fluxes and turbulent fluxes that results in roughly uniform rates of change of entropy and lepton number in the PCS convection zone. Electron fraction and enthalpy contrasts in PCS convection are not well captured by the MLT gradient approximation. We find distinctly different spectra for the turbulent kinetic energy and turbulent fluctuations in the electron fraction, which scale approximately as $l^{-1}$ without a downturn at low $l$. We suggest that the different turbulence spectrum of the electron fraction is naturally expected for a passive scalar quantity.",
        "citation_title": "Convection and the Core $g$-mode in Proto-Compact Stars -- A detailed analysis",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Hubble constant, $H_0$, tension is the tension among the local probes, Supernovae Ia, and the Cosmic Microwave Background Radiation. It has been almost a decade, and this tension still puzzles the community. Here, we add intermediate redshift probes, such as Gamma-Ray Bursts (GRB) and Quasars (QS0s), to check if and to what extent these higher redshift probes can reduce this tension. We use the three-dimensional fundamental plane relation among the prompt peak luminosity, the luminosity at the end of the plateau emission, and its rest frame duration. We find similar trend in GRB intrinsic parameters as previously seen in Pantheon-Plus intrinsic parameters. We find an apparent $3.14\\sigma$ tension for the GRB intrinsic parameter $b$. Indeed, this tension disappears and the parameters are actually compatible within $2.26\\sigma$. Another interesting point is that the 3D relation plays an important role in conjunction with Supernovae data with Pantheon Plus and that this apparent discrepancy show how it is important the correction for selection biases and redshift evolution. The incorporation of redshift evolution correction results in a reduction of the GRB tension to $2.26\\sigma$ when adjusting correction parameters. We envision that with more data this indication of tension will possibly disappear when the evolutionary parameters of GRBs are computed with increased precision.",
        "citation_title": "Revisiting the Concordance $\u039b$CDM model using Gamma-Ray Bursts together with Supernovae Ia and Planck data",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Commercial flatbed scanners have the potential to deliver a quick and efficient means of capturing the scientific content of spectra recorded on photographic plates. We discuss the digitization of selected spectra in the Dominion Astrophysical Observatory (DAO) photographic plate collection with commercial scanners. In this pilot study, emphasis is placed on assessing if the information on the plates can be recovered using Epson V800 and 12000XL scanners; the more complicated issues associated with the shortcomings of photographic materials, such as correcting for nonlinearity, are deferred to a future study. Spectra of Vega that were recorded over ~4 decades with the DAO 1.8 meter telescope are examined. These spectra sample a range of photographic emulsions, plate preparation techniques, calibration information, observing techniques, and spectrograph configuration. A scanning density of 2400 elements per inch recovers information in the spectra. Differences in the modulation transfer function (MTF) of the two scanners are found, with the Epson 12000XL having a superior MTF. Comparisons with a CCD spectrum of Vega confirm that moderately weak features are faithfully recovered in photographic spectra that have been digitized with the 12000XL scanner. The importance of scanning the full plate to cover the light profile of the target and calibration information is emphasized. Lessons learned from these experiments are also presented.",
        "citation_title": "The Digitization of Photographic Spectra in the Dominion Astrophysical Observatory Plate Collection with Commercial Scanners: A Pilot Study",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The identification of red, apparently massive galaxies at $z>7$ in early JWST photometry suggests a strongly accelerated timeline compared to standard models of galaxy growth. A major uncertainty in the interpretation is whether the red colors are caused by evolved stellar populations, dust, or other effects such as emission lines or AGN. Here we show that three of the massive galaxy candidates at $z=6.7-8.4$ have prominent Balmer breaks in JWST/NIRSpec spectroscopy from the RUBIES program. The Balmer breaks demonstrate unambiguously that stellar emission dominates at $\\lambda_{\\rm rest} = 0.4\\,\\mu$m, and require formation histories extending hundreds of Myr into the past in galaxies only 600--800 Myr after the Big Bang. Two of the three galaxies also show broad Balmer lines, with H$\\beta$ FWHM $>2500~{\\rm km\\,s^{-1}}$, suggesting that dust-reddened AGN contribute to, or even dominate, the SEDs of these galaxies at $\\lambda_{\\rm rest}\\gtrsim 0.6\\,\\mu$m. All three galaxies have relatively narrow [O III] lines, seemingly ruling out a high-mass interpretation if the lines arise in dynamically-relaxed, inclined disks. Yet, the inferred masses also remain highly uncertain. We model the high-quality spectra using Prospector to decompose the continuum into stellar and AGN components, and explore limiting cases in stellar/AGN contribution. This produces a wide range of possible stellar masses, spanning $M_\\star \\sim 10^9 - 10^{11}\\,{\\rm M_{\\odot}}$. Nevertheless, all fits suggest a very early and rapid formation, most of which follow with a truncation in star formation. Potential origins and evolutionary tracks for these objects are discussed, from the cores of massive galaxies to low-mass galaxies with over-massive black holes. Intriguingly, we find all of these explanations to be incomplete; deeper and redder data are needed to understand the physics of these systems.",
        "citation_title": "RUBIES: Evolved Stellar Populations with Extended Formation Histories at $z \\sim 7-8$ in Candidate Massive Galaxies Identified with JWST/NIRSpec",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present results of the analysis of a set of images obtained in the field of the Milky Way bulge globular cluster NGC 6355 using the Dark Energy Camera, which is attached to the 4m Blanco telescope of the Cerro-Tololo Interamerican Observatory. We dealt with a heavy differential absorption across the observed field, a crowded field star population, and the superposition of field stars on to the cluster color-magnitude diagram main features to produce an intrinsic cluster stars density map. The resulting stellar density map reveals the presence of an extended envelope, a tidal tail, and scattered debris; the tidal tails pointing toward the Milky Way center. Such extra-tidal overdensities, detected above the mean star field density, resulted to be between four and six times larger that the local star field density fluctuation. They have also been recently generated by two independent studies which performed numerical simulations of synthetic tidal tails of Milky Way globular clusters. These results contrast with previous theoretical speculations about the possibility to detect tidal tails of globular clusters with chaotic orbits because they would be washed out after they were generated.",
        "citation_title": "Surviving tidal tails around the Milky Way bulge globular cluster NGC 6355",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Neutron stars represent unique laboratories, offering insights into the physics of supranuclear-density matter and serving as potential hosts for dark matter. This study explores the impact of dark matter cores on rapidly rotating neutron stars through the two-fluid approximation, assuming minimal interaction between baryonic matter and dark matter. The investigation employs phenomenological models for fermionic and bosonic dark matter, revealing that universal relations governing mass and radius changes due to rotation remain largely unaffected in the presence of a dark matter core. Specifically, for a 5 % dark matter mass fraction, the percent deviations in total mass ($M_{tot}$), the baryonic equatorial radius ($R_{Be}$), and polar-to-equatorial baryonic radius ratio ($R_{ratioB}$) are within 3.9 %, 1.8 %, and 1.4 %, respectively. These findings suggest that the universal relations governing neutron star shape can be utilized to infer constraints on the properties of dark matter cores even in cases where the dark matter significantly softens the neutron star's equation of state.",
        "citation_title": "The Effect of a Dark Matter Core on the Structure of a Rotating Neutron Star",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The transient Galactic black hole candidate Swift J1727.8-1613 went through an outburst for the very first time that started in August 2023 and lasted for almost 6 months. We study the timing and spectral properties of this source using publicly available archival Insight-HXMT data for the first 10 observation IDs that last from MJD 60181 to 60198 with a total of 92 exposures for all three energy bands. We extracted the quasi-periodic oscillation properties by model fitting the power density spectrum and from those properties we designate that the QPOs are type-C in nature. We also conclude that the origin of the QPOs could be the shock instabilities, formed in the transonic accretion flow around black holes. The spectral analysis was performed using simultaneous data from the three on-board instruments LE, ME, and HE of HXMT in the broad energy band of $2-150 $ keV. To achieve the best fit, we needed a combination of interstellar absorption, power-law, multi-color disk-blackbody continuum, gaussian emission/absorption, and power-law reflection by neutral material. From the spectral properties, we found that the source was in an intermediate state at the start of the analysis period and was making a transition toward the softer states. The shock (the boundary layer of the corona) moved inward in progressive days in accordance with the spectral nature. We found that the source is present in a high-inclination binary system. The hydrogen column density was found with an average value of $0.27_{-0.17}^{+0.08}\\times10^{22}$ cm$^{-2}$.",
        "citation_title": "Insight-HXMT View of the BHC Swift J1727.8-1613 during its outburst in 2023",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "With active time-domain surveys like the Zwicky Transient Facility, the anticipated Rubin Observatory's Legacy Survey of Space and Time, and multi-messenger experiments such as LIGO/VIRGO/KANGRA for gravitational wave detection and IceCube for high-energy neutrino events, there is a new era in both time-domain and multi-messenger astronomy. The Astro2020 decadal survey highlights effectively responding to these astronomical alerts in a timely manner as a priority, and thus, there is an urgent need for the development of a seamless follow-up infrastructure at existing facilities that are capable of following up on detections at the survey depths. At the W. M. Keck Observatory (WMKO), we are actively constructing critical infrastructure, aimed at facilitating the Target-of-Opportunity (ToO) trigger, optimizing observational planning, streamlining data acquisition, and enhancing data product accessibility. In this document, we provide an overview of these developing services and place them in context of existing observatory infrastructure like the Keck Observatory Archive (KOA) and Data Services Initiative (DSI).",
        "citation_title": "Advancements in Streamlining Time-Domain and Multi-Messenger Astronomy Follow-Up Infrastructure at Keck Observatory",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "White dwarf symbiotic binaries are detected in X-rays with luminosities in the range of 10$^{30}$ to 10$^{34}$ lumcgs. Their X-ray emission arises either from the accretion disk boundary layer, from a region where the winds from both components collide or from nuclear burning on the white dwarf surface. In our continuous effort to identify X-ray emitting symbiotic stars, we studied four systems using observations from the Neil Gehrels Swift Observatory and XMM-Newton satellites in X-rays and from TESS in the optical. The X-ray spectra were fit with absorbed optically thin thermal plasma models, either single- or multitemperature with kT $<$ 8 keV for all targets. Based on the characteristics of their X-ray spectra, we classified BD Cam as possible $\\beta$-type, V1261 Ori and CD -27 8661 as $\\delta$-type, and confirmed NQ Gem as $\\beta$/$\\delta$-type. The $\\delta$-type X-ray emission most likely arise in the boundary layer of the accretion disk, while in the case of BD Cam, its mostly-soft emission originates from shocks, possibly between the red giant and WD/disk winds. In general, we have found that the observed X-ray emission is powered by accretion at a low accretion rate of about 10$^{-11}$ M$_{\\odot}$ yr$^{-1}$. The low ratio of X-ray to optical luminosities, however indicates that the accretion-disk boundary layer is mostly optically thick and tends to emit in the far or extreme UV. The detection of flickering in optical data provides evidence of the existence of an accretion disk.",
        "citation_title": "Symbiotic stars in X-rays IV: XMM-Newton, Swift and TESS observations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We developed an algorithm to use Galaxy Zoo 3D spiral arm masks produced by citizen scientist volunteers to semi-automatically classify spiral galaxies as either multi-armed or grand design spirals. Our final sample consists of 299 multi-armed and 245 grand design galaxies. On average, the grand design galaxies have smaller stellar masses than the multi-armed galaxies. For a given stellar mass, the grand design galaxies have larger concentrations, earlier Hubble types, smaller half-light radii, and larger central surface mass densities than the multi-armed galaxies. Lower mass galaxies of both arm classes have later Hubble types and lower concentrations than higher mass galaxies. In our sample, a higher fraction of grand design galaxies have classical bulges rather than pseudo-bulges, compared to multi-armed galaxies. These results are consistent with theoretical models and simulations which suggest that dense classical bulges support the development and/or longevity of 2-armed spiral patterns. Similar specific star formation rates are found in multi-armed and grand design galaxies with similar stellar masses and concentrations. This implies that the specific star formation rates in spiral galaxies is a function of concentration and stellar mass, but independent of the number of spiral arms. Our classifications are consistent with arm counts from the Galaxy Zoo 2 project and published m=3 Fourier amplitudes.",
        "citation_title": "Grand Design vs. Multi-Armed Spiral Galaxies: Dependence on Galaxy Structure",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Gaia mission has delivered hundreds of thousands of variable star light curves in multiple wavelengths. Recent work demonstrates that these light curves can be used to identify (non-)radial pulsations in the OBAF-type stars, despite the irregular cadence and low light curve precision of order a few mmag. With the considerably more precise TESS photometry, we revisit these candidate pulsators to conclusively ascertain the nature of their variability. We seek to re-classify the Gaia light curves with the first two years of TESS photometry for a sample of 58,970 p- and g- mode pulsators, encompassing gamma Dor, delta Scuti, SPB, and beta Cep variables. We also supply four new catalogues containing the confirmed pulsators, along with their dominant and secondary pulsation frequencies, the number of independent mode frequencies, and a ranking according to their usefulness for future asteroseismic ensemble analysis. We find that the Gaia photometry is exceptionally accurate for detecting the dominant and secondary frequencies, reaching approximately 80% accuracy in frequency for p- and g-mode pulsators. The majority of Gaia classifications are consistent with the classifications from the TESS data, illustrating the power of the low-cadence Gaia photometry for pulsation studies. We find that the sample of g-mode pulsators forms a continuous group of variable stars along the main sequence across B, A, and F spectral types, implying that the mode excitation mechanisms for all these pulsators need to be updated with improved physics. Finally, we provide a rank-ordered table of pulsators according to their asteroseismic potential for follow-up studies. Our catalogue offers a major increase in the number of confirmed gravity-mode pulsators with an identified dominant mode suitable for follow-up TESS ensemble asteroseismology of such stars.",
        "citation_title": "Confronting sparse Gaia DR3 photometry with TESS for a sample of about 60,000 hot massive non-radial pulsators",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "During the past few decades, abundant evidence for physics beyond the two standard models of particle physics and cosmology was found. Yet, we are tapping into the dark regarding our understanding of the dark sector. For more than a century, open problems related to the nature of the vacuum remain unresolved. Besides the traditional high-energy frontier and cosmology, technological advancement provides complementary access to new physics via high-precision experiments. Among the latter, the Casimir And Non-Newtonian force EXperiment (\\cannex{}) has successfully completed its proof-of-principle phase and will soon commence operation. Benefiting from its plane parallel plate geometry, both interfacial and gravity-like forces are maximized, leading to increased sensitivity. A wide range of dark sector forces, Casimir forces in and out of thermal equilibrium, and gravity will be tested. This article describes the final experimental design, its sensitivity, and expected results.",
        "citation_title": "Force metrology with plane parallel plates: Final design review and outlook",
        "date_delivered": "[Submitted on 16 Mar 2024]"
    },
    {
        "abstract": "The formation of protein precursors, due to the condensation of atomic carbon under the low-temperature conditions of the molecular phases of the interstellar medium, opens alternative pathways for the origin of life. We perform peptide synthesis under conditions prevailing in space and provide a comprehensive analytic characterization of its products. The application of 13C allowed us to confirm the suggested pathway of peptide formation that proceeds due to the polymerization of aminoketene molecules that are formed in the C + CO + NH3 reaction. Here, we address the question of how the efficiency of peptide production is modified by the presence of water molecules. We demonstrate that although water slightly reduces the efficiency of polymerization of aminoketene, it does not prevent the formation of peptides.",
        "citation_title": "Formation of extraterrestrial peptides and their derivatives",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "If the cosmological dark matter (DM) couples to Standard Model (SM) fields, it can decay promptly to SM states in a highly energetic hard process, which subsequently showers and hadronizes to give stable particles including $e^\\pm$, $\\gamma$, $p^{\\pm}$ and $\\nu\\bar{\\nu}$ at lower energy. If the DM particle is very heavy, the high-energy $e^\\pm$, due to the Klein-Nishina cross section suppression, preferentially lose energy via synchrotron emission which, in turn, can be of unusually high energies. Here, we present novel bounds on heavy decaying DM up to the Planck scale, by studying the synchrotron emission from the $e^\\pm$ produced in the ambient Galactic magnetic field. In particular, we explore the sensitivity of the resulting constraints on the DM decay width to (i) different SM decay channels, to (ii) the Galactic magnetic field configurations, and (iii) to various different DM density profiles proposed in the literature. We find that constraints from the synchrotron component complement and improve on constraints from very high-energy cosmic-ray and gamma-ray observatories targeting the prompt emission when the DM is sufficiently massive, most significantly for masses in excess of $10^{12}\\text{ GeV}$.",
        "citation_title": "Astrophysical constraints from synchrotron emission on very massive decaying dark matter",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In this paper, we investigate the gravitational collapse to form the black hole in the acceleratingly expanding universe in the frame of Einstein--Gauss-Bonnet theory having two scalar fields and we study the propagation of the gravitational wave (GW). The collapsing spacetime can be obtained by using the formulation of the ``reconstruction'', that is, we find a model that realises the desired or given geometry. In the reconstructed models, ghosts often appear, which could be eliminated by imposing constraints. We show that the standard cosmological solutions or self-gravitating objects such as a planet, the Sun, various types of stars, etc., in Einstein's gravity, are also solutions in this model. Using the dynamical value of Gauss-Bonnet coupling, the propagation of the high-frequency GW is investigated. The propagating speed changes due to the coupling during the period of the black hole formation. The speed at which the GW propagates The speed at which the GW propagates going into the black hole is different from that of the wave going out.",
        "citation_title": "Gravitational collapse and gravitational wave in Einstein--Gauss-Bonnet theory with two scalar fields",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work we compute numerical bounds on the mass $\\mu$ of superradiantly unstable scalar fields in a Kerr black hole background using the continued fraction method. We show that the normalized upper bound on the mass $\\mu$ increases with the angular momentum number $\\ell$ and the azimuthal number $m$, approaching the most stringent analytical bound known to date when $\\ell=m \\gg 1$. We also provide an analytical fit to the numerically determined mass bound as a function of the dimensionless spin parameter $a/M$ of the black hole with an accuracy of the order $0.1\\%$ for the fundamental mode with $\\ell=m=1$, and of the order $1\\%$ for higher-order modes (up to $\\ell=m=20$). We argue that this analytical fit is particularly useful in astrophysical scenarios, since the lowest $\\ell=m$ modes are capable of producing the strongest observable imprints of superradiance.",
        "citation_title": "Bounds on the mass of superradiantly unstable scalar fields around Kerr black holes",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We report a methodology to determine the quality factor ($Q$) in implementations of the so-called dielectric haloscope, a new concept of wavy dark matter detector equipped with a multilayered resonator. An anechoic chamber enables the observation of the resonance frequency and its amplitude for an unlimited series of layers for the first time, which is conveniently filtered. The frequency-normalized power enhancement measured in a Dark-photons \\& Axion-Like particles Interferometer (DALI) prototype is a few hundred per layer over a sweep bandwidth of half a hundred MHz. In light of this result, this scaled-down prototype is sensitive to axions saturating the local dark matter density with a coupling to photons between $g_{a\\gamma\\gamma}\\gtrsim10^{-12}$ GeV$^{-1}$ and $g_{a\\gamma\\gamma}\\gtrsim$ few $\\times 10^{-14}$ GeV$^{-1}$ at frequencies of several dozens of GHz once cooled down to the different working temperatures of the experiment and immersed in magnetic fields ranging from 1 T to 10 T; while the sensitivity of the full-scale DALI is projected at $g_{a\\gamma\\gamma}\\gtrsim\\mathrm{few}\\times10^{-15}$ GeV$^{-1}$ over the entire 25--250 {\\mu}eV range since $Q\\gtrsim10^4$ is expected.",
        "citation_title": "Echo-free quality factor of a multilayer axion haloscope",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Understanding the properties and physical phase of the dense strongly interacting matter present in the cores of neutron stars or created in their binary mergers remains one of the most prominent open problems in nuclear astrophysics. While most microscopic analyses have historically relied on solvable phenomenological models of nuclear and quark matter, in recent years a model-independent approach utilizing only controlled ab-initio calculations and astrophysical observations has emerged as a viable alternative.\nIn these lecture notes, I review recent progress in first-principles weak-coupling calculations within high-density quark matter, shedding light on its thermodynamic and transport properties. I cover the most important technical tools used in such calculations, introduce selected highlight results, and explain how this information can be used in phenomenological studies of neutron-star physics. The notes do not offer a self-consistent treatment of the topics covered, but rather aim at filling gaps in existing textbooks on thermal field theory and at connecting the dots in a story developed in several recent research articles, to which the interested reader is directed for further technical details.",
        "citation_title": "Particle-theory input for neutron-star physics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Liquid argon detectors are ubiquitous in particle, astroparticle, and applied physics. They reached an unprecedented level of maturity thanks to more than 20 years of R&D and the operation of large-scale facilities at CERN, Fermilab, and the Gran Sasso laboratories. This article reviews such an impressive advance - from the grounding of the experimental technique up to cutting-edge applications. We commence the review by describing the physical and chemical properties of liquid argon as an active and target medium for particle detection, together with advantages and limitations compared with other liquefied noble gases. We examine the opportunities and challenges of liquid argon detectors operated as calorimeters, scintillators, and time projection chambers. We then delve into the core applications of liquid argon detectors at colliders (ATLAS), accelerator neutrino beams (SBN, DUNE), and underground laboratories (DarkSide, DEAP, ICARUS) for the observation of rare events. We complete the review by looking at unconventional developments (pixelization, combined light-charge readout, Xe-doped devices, all-optical readout) and applications in medical and applied physics to extend this technology's scope toward novel research fields.",
        "citation_title": "The science and technology of liquid argon detectors",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Nontopological fermionic solitons exist across a diverse range of particle physics models and have rich cosmological implications. This study establishes a general framework for calculating fermionic soliton profiles under arbitrary scalar potentials, utilizing relativistic mean field theory to accurately depict the interaction between the fermion condensate and the background scalar field. Within this framework, the conventional fermion bound states are revealed as a subset of fermionic solitons. In addition, we demonstrate how the analytical formulae in previous studies are derived as special cases of our algorithm, discussing the validity of such approximations. Furthermore, we explore the phenomenology of fermionic solitons, highlighting new formation mechanisms and evolution paths, and reconsidering the possibility of collapse into primordial black holes.",
        "citation_title": "Revisiting the fermion-field nontopological solitons",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We numerically analyse solutions of the spherically symmetric gravitational Vlasov-Poisson system close to compactly supported stable steady states. We observe either partially undamped oscillations or macroscopically damped solutions. We investigate for many steady states to which of these behaviours they correspond. A linear relation between the exponents of polytropic steady states and the qualitative behaviour close to them is identified. Undamped oscillations are also observed around not too concentrated King models and around all shells with a sufficiently large vacuum region. We analyse all solutions both at the non-linear and linearised level and find that the qualitative behaviours are identical at both. To relate the observed phenomena to theoretical results, we further include a comprehensive numerical study of the radial particle periods in the equilibria.",
        "citation_title": "Numerical experiments on stationary, oscillating, and damped spherical galaxy models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "While numerical simulations offer unparalleled precision and robustness in studying complex physical systems, their execution is often hindered by complexity, costliness, and time consumption due to the intricate equations involved. This challenge is already encountered in General Relativity, where non-flat spacetimes exacerbate the computational burden. This complexity is further intensified when dealing with additional degrees of freedom. To address this challenge head-on, we introduce GRBoondi, a groundbreaking fixed-background numerical relativity code designed to provide a unified interface for numerically solving Generalized Proca theories. GRBoondi grants users the ability to make arbitrary modifications to the Proca equations of motion on any background, providing a robust and versatile tool for exploring diverse classes of Generalized Proca theories. This letter serves as part of the submission of GRBoondi to the Journal of Open Source Software. For access to the code, please visit this https URL.",
        "citation_title": "GRBoondi: A code for evolving Generalized Proca theories on arbitrary backgrounds",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we systematically investigate the inflationary complexity of the two-mode squeezed state with thermal effect for the single field inflation, modified dispersion relation, and non-trivial sound speed with the method of closed system and open system, respectively, which our analysis is valid for most inflationary models. First, the numeric of Krylov complexity in the method of the closed system indicates that the evolution of Krylov complexity highly depends on the squeezed angle parameter once taking the thermal effect into account, which will decay into some very tiny values, but the Krylov complexity will always enhance without thermal effect. For comparison, the numeric of circuit complexity shows that the evolution is always increasing no matter whether there are thermal effects or not which is independent of the evolution of squeezed angle parameter. By utilizing the method of open system, we first construct the wave function. As for the Krylov complexity with the method of open system, our investigations show the evolution of Krylov complexity will enhance upon some peaks factoring in the thermal effects. For completeness, we also calculate the Krylov entropy in the method of closed system and open system, which indicates that the hotter universe, the more chaotic the universe. Furthermore, our derivation for the Krylov complexity and Krylov entropy could nicely recover into the case of closed system under weak dissipative approximation, which confirms the validity of construction for the wave function. Finally, our numeric of Lanczos coefficient shows that the non-trivial sound speed has minimal chaos compared to the other two cases.",
        "citation_title": "Inflationary complexity of thermal state",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Ground-based laser interferometric gravitational wave detectors consist of complex multiple optical cavity systems. An arm-length stabilization (ALS) system has played an important role in bringing such complex detector into operational state and enhance the duty cycle. The sensitivity of these detectors can be improved if the thermal noise of their test mass mirror coatings is reduced. Crystalline AlGaAs coatings are a promising candidate for this. However, traditional ALS system with frequency-doubled 532 nm light is no longer an option with AlGaAs coatings due to the narrow bandgap of GaAs, thus alternative locking schemes must be developed. In this letter, we describe an experimental demonstration of a novel ALS scheme which is compatible with AlGaAs coatings. This ALS scheme will enable the use of AlGaAs coatings and contribute to improved sensitivity of future detectors.",
        "citation_title": "Experimental demonstration of frequency downconverted arm length stabilization for a future upgraded gravitational wave detector",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The existence of 'peculiar' velocities due to the formation of cosmic structure marks a point of discord between the real Universe and the usually assumed Friedmann-Lema\u00edtre-Robertson-Walker metric which accomodates only the smooth Hubble expansion on large scales. In the standard $\\Lambda$CDM model framework, Type Ia supernovae data are routinely \"corrected\" for the peculiar velocities of both the observer and the supernova host galaxies relative to the cosmic rest frame, in order to infer evidence for acceleration of the expansion rate from their Hubble diagram. However observations indicate a strong, coherent local bulk flow that continues outward without decaying out to a redshift $z \\gtrsim 0.1$, contrary to the $\\Lambda$CDM expectation. By querying the halo catalogue of the Dark Sky Hubble-volume N-body simulation, we find that an observer placed in an unusual environment like our local Universe should see correlations between supernovae in the JLA catalogue that are 2-8 times stronger than seen by a typical or Copernican observer. This accounts for our finding that peculiar velocity corrections have a large impact on the value of the Cosmological Constant inferred from supernova data. We also demonstrate that local Universe-like observers will infer a downward biased value of the clustering parameter $S_8$ from comparing the density and velocity fields. More realistic modelling of the peculiar local Universe is thus essential for correctly interpreting cosmological data.",
        "citation_title": "The impact of peculiar velocities on supernova cosmology",
        "date_delivered": "[Submitted on 23 Mar 2020 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "Weak gravitational lensing directly probes the matter distribution surrounding satellite galaxies in galaxy clusters. We measure the weak lensing signal induced on the shapes of background galaxies around SDSS redMaPPer cluster satellite galaxies, which have their central galaxies assigned with a probability $P_{\\rm cen}>0.95$ in the redshift range, $0.1\\leq z\\leq 0.33$. We use the galaxy shapes from the Subaru Hyper Suprime-Cam (HSC) survey for this purpose. We bin satellite galaxies by their distance from the cluster centre and compare it to the signal around a control sample of galaxies which do not reside in clusters but have similar colours and magnitudes. We explore the effect of environmental processes on the dark matter mass around satellites. We see hints of a difference in the mass of the subhalo of the satellite compared to the halo masses of galaxies in our control sample, especially in the innermost cluster-centric radial bin ($0.1<r<0.3$ [$h^{-1}\\rm Mpc$]). For the first time, we put an upper limit on the prevalence of orphan galaxies which have entirely lost their dark matter halos with cluster-centric distances with the help of our measurements. However, these upper limits could be relaxed if there is substantial contamination in the satellite galaxy sample.",
        "citation_title": "Subaru HSC weak lensing of SDSS redMaPPer cluster satellite galaxies: Empirical upper limit on orphan fractions",
        "date_delivered": "[Submitted on 29 Apr 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "A novel analysis is performed, incorporating time-of-flight (TOF) information to study the interactions of dark matter (DM) with standard model particles. After supernova (SN) explosions, DM with mass $m_\\chi\\lesssim\\mathcal{O}({\\rm MeV})$ in the halo can be boosted by SN neutrinos (SN$\\nu$) to relativistic speed. The SN$\\nu$ boosted DM (BDM) arrives on Earth with TOF which depends only on $m_\\chi$ and is independent of the cross section. These BDMs can interact with detector targets in low-background experiments and manifest as afterglow events after the arrival of SN$\\nu$. The characteristic TOF spectra of the BDM events can lead to large background suppression and unique determination of $m_\\chi$. New cross section constraints on $\\sqrt{\\sigma_{\\chi e} \\sigma_{\\chi\\nu}}$ are derived from SN1987a in the Large Magellanic Cloud with data from the Kamiokande and Super-Kamiokande experiments. Potential sensitivities for the next galactic SN with Hyper-Kamiokande are projected. This analysis extends the existing bounds on $\\sqrt{\\sigma_{\\chi e}\\sigma_{\\chi \\nu}}$ over a broad range of $r_\\chi=\\sigma_{\\chi \\nu}/\\sigma_{\\chi e}$. In particular, the improvement is by 1-3 orders of magnitude for $m_\\chi<\\mathcal{O}(100\\,{\\rm keV})$ for $\\sigma_{\\chi e}\\sim\\sigma_{\\chi \\nu}$. Prospects of exploiting TOF information in other astrophysical systems to probe exotic physics with other DM candidates are discussed.",
        "citation_title": "Searching for Afterglow: Light Dark Matter Boosted by Supernova Neutrinos",
        "date_delivered": "[Submitted on 14 Jun 2022 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "Recent observations have granted to us two unique insights into the early universe: the presence of a low-frequency stochastic gravitational wave background detected by the NANOGrav and Pulsar Timing Array (PTA) experiments and the emergence of unusually massive galaxy candidates at high redshifts reported by the James Webb Space Telescope (JWST). In this letter, we consider the possibility that both observations have a common origin, namely primordial black holes (PBHs) in the mass range between $10^{6}~M_{\\odot}$ and $10^{13}~M_{\\odot}$. While superheavy PBHs act as seeds for accelerated galaxy formation capable of explaining the JWST extreme galaxies, they can also form binary mergers that source gravitational waves which can be potentially identified as the PTA signal. The analysis is performed taking into account the constraints on the relevant region of the PBH parameter space including the novel bound imposed by the Ultraviolet Luminosity Function of galaxies observed by the Hubble Space Telescope. We conclude that PTA's and JWST's interpretations in terms of PBH binary mergers and Poissonian gas of PBHs, respectively, are strongly excluded.",
        "citation_title": "Scrutinizing the Primordial Black Holes Interpretation of PTA Gravitational Waves and JWST Early Galaxies",
        "date_delivered": "[Submitted on 4 Jul 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Supernova neutrino boosted dark matter (SN$\\nu$ BDM) and its afterglow effect have been shown to be a promising signature for beyond Standard Model (bSM) physics. The time-evolution feature of SN$\\nu$ BDM allows for %the possibly direct inference of DM mass $m_\\chi$, and results in significant background suppression with improving sensitivity. This paper extends the earlier study and provides a general framework for computing the SN$\\nu$ BDM fluxes for a supernova that occurs at any location in our galaxy. A bSM $U(1)_{L_\\mu-L_\\tau}$ model with its gauge boson coupling to both DM and the second and third generation of leptons is considered, which allows for both DM-$\\nu$ and DM-$e$ interactions. Detailed analysis of the temporal profile, angular distribution, and energy spectrum of the SN$\\nu$ BDM are performed. Unique signatures in SN$\\nu$ BDM allowing extraction of $m_\\chi$ and detail features that contain information of the underlying interaction type are discussed. Expected sensitivities on the above new physics model from Super-Kamiokande, Hyper-Kamiokande, and DUNE detections of BDM events induced by the next galactic SN are derived and compared with the existing bounds.",
        "citation_title": "Signatures of afterglows from light dark matter boosted by supernova neutrinos in current and future large underground detectors",
        "date_delivered": "[Submitted on 7 Jul 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We investigate the late-time cosmological dynamics in a simple case of explicit spacetime-symmetry breaking. By expanding in a small symmetry-breaking coefficient we are able to write the Friedmann equations as $\\Lambda$CDM + dynamical dark energy, which we show contains logarithmic dependence of the scale factor. We find that the dark energy equation of state displays divergencies and phantom behaviour for certain values of the symmetry-breaking coefficient, where the NEC is also broken. We discuss the adiabatic sound speed of dark energy and compare the model to current constraints using the Chevallier-Polarski-Linder parametrisation. Remarkably, although the constraints on the same symmetry-breaking coefficient from e.g. gravitational-wave propagation are orders of magnitude stronger than what we obtain in this paper, we are able to cut those constraints, which are more or less symmetric around zero, in half by showing that same coefficient must be negative (or zero) if one wishes to keep the NEC intact.",
        "citation_title": "Dynamical dark energy from spacetime-symmetry breaking -- late-time behaviour and phantom crossing",
        "date_delivered": "[Submitted on 18 Jul 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The incoming Imaging X-ray Polarimetry Explorer (IXPE) observations of X-ray binaries provide a new tool to investigate the underlying accretion geometry. Here we report the first measurements of X-ray polarization of the extra-galactic black-hole X-ray binary LMC X$-$3. We find a polarization fraction of $\\sim 3%$ at a polarization angle of $\\sim 135^\\circ$ in the $2-8$ keV energy band with statistical significance at the 7$\\sigma$ level. This polarization measurement significantly exceeds the minimum detectable polarization threshold of 1.2% for the source, ascertained at a 99% confidence level within the $2-8$ keV energy band. The simultaneous spectro-polarimetric fitting of NICER, Swift/XRT, and IXPE revealed the presence of a disc with a temperature of $\\sim$1 keV and a Comptonized component with a power-law index of $\\sim$ 2.4, confirming the soft nature of the source. The polarization degree increases with energy from $\\sim$3% in the $2-5.7$ keV band to $\\sim$9% in the $5.7-8$ keV band, while the polarization angle is energy independent. The observed energy dependence and the sudden jump of polarization fraction above 5 keV supports the idea of a static slab coronal geometry for the Comptonizing medium of LMC X$-$3.",
        "citation_title": "Unveiling the X-ray polarimetric properties of LMC X-3 with IXPE, NICER, and Swift/XRT",
        "date_delivered": "[Submitted on 13 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Gravitational Waves (GWs) have been detected in the $\\sim$100 Hz and nHz bands, but most of the gravitational spectrum remains unobserved. A variety of detector concepts have been proposed to expand the range of observable frequencies. In this work, we study the capability of GW detectors in the ``mid-band'', the $\\sim$30 mHz -- 10 Hz range between LISA and LIGO, to measure the signals from and constrain the properties of ${\\sim}$1 -- 100 $M_\\odot$ compact binaries. We focus on atom-interferometer-based detectors. We describe a Fisher matrix code, AIMforGW, which we created to evaluate their capabilities, and present numerical results for two benchmarks: terrestrial km-scale detectors, and satellite-borne detectors in medium Earth orbit. Mid-band GW detectors are particularly well-suited to pinpointing the location of GW sources on the sky. We demonstrate that a satellite-borne detector could achieve sub-degree sky localization for any detectable source with chirp mass $\\mathcal{M}_c \\lesssim 50 M_\\odot$. We also compare different detector configurations, including different locations of terrestrial detectors and various choices of the orbit of a satellite-borne detector. As we show, a network of only two terrestrial single-baseline detectors or one single-baseline satellite-borne detector would each provide close-to-uniform sky-coverage, with signal-to-noise ratios varying by less than a factor of two across the entire sky. We hope that this work contributes to the efforts of the GW community to assess the merits of different detector proposals.",
        "citation_title": "Gravitational Wave Measurement in the Mid-Band with Atom Interferometers",
        "date_delivered": "[Submitted on 14 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present an eigenfunction method to analyze 161 visual light curves (LCs) of Type Ia supernovae (SNe Ia) obtained by the Carnegie Supernova Project to characterize their diversity and host-galaxy correlations. The eigenfunctions are based on the delayed-detonation scenario using three parameters: the LC stretch being determined by the amount of deflagration-burning governing the 56Ni production, the main-sequence mass M_MS of the progenitor white dwarf controlling the explosion energy, and its central density rho_c shifting the 56Ni distribution. Our analysis tool (SPAT) extracts the parameters from observations and projects them into physical space using their allowed ranges M_MS < 8 M_sun, rho_c < 7-8x10^9g/cc. The residuals between fits and individual LC-points are ~ 1-3% for ~ 92% of objects. We find two distinct M_MS groups corresponding to a fast (~ 40-65 Myrs) and a slow(~ 200-500 Myrs) stellar evolution. Most underluminous SNe Ia have hosts with low star formation but high M_MS, suggesting slow evolution times of the progenitor system. 91T-likes SNe show very similar LCs and high M_MS and are correlated to star formation regions, making them potentially important tracers of star formation in the early Universe out to z = 4-11. Some 6% outliers with `non-physical' parameters can be attributed to superluminous SNe Ia and subluminous SNe Ia with hosts of active star formation. For deciphering the SNe Ia diversity and high-precision SNe Ia cosmology, the importance is shown for LCs covering out to ~ 60 days past maximum. Finally, our method and results are discussed within the framework of multiple explosion scenarios, and in light of upcoming surveys.",
        "citation_title": "Type Ia Supernova Progenitor Properties and Their Host Galaxies",
        "date_delivered": "[Submitted on 6 Nov 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The strongly-coupled system like the quark-hadron transition (if it is of first order) is becoming an active play-yard for the physics of cosmological first-order phase transitions. However, the traditional field theoretic approach to strongly-coupled first-order phase transitions is of great challenge, driving recent efforts from holographic dual theories with explicit numerical simulations. These holographic numerical simulations have revealed an intriguing linear correlation between the phase pressure difference (pressure difference away from the wall) to the non-relativistic terminal velocity of an expanding planar wall, which has been reproduced analytically alongside both cylindrical and spherical walls from perfect-fluid hydrodynamics in our previous study but only for a bag equation of state. We have also found in our previous study a universal quadratic correlation between the wall pressure difference (pressure difference near the bubble wall) to the non-relativistic terminal wall velocity regardless of wall geometries. In this paper, we will generalize these analytic relations between the phase/wall pressure difference and terminal wall velocity into a more realistic equation of state beyond the simple bag model, providing the most general predictions so far for future tests from holographic numerical simulations of strongly-coupled first-order phase transitions",
        "citation_title": "General bubble expansion at strong coupling",
        "date_delivered": "[Submitted on 13 Nov 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Odd radio circles (ORCs) are mysterious rings of faint, diffuse emission recently discovered in radio surveys, some of which may be associated with galaxies in relatively dense environments. We propose such ORCs to be synchrotron emission from remnants of explosive galactic outflows, calling them OGREs, and discuss their broadband non-thermal emission and evolution. We posit that a large amount of energy was ejected from the central galaxy in the past, creating an outgoing shock that accelerates cosmic rays. Assuming plausible values for the density, temperature and magnetic field of the ambient medium, consistency with the observed spectral index, size and power of the ORCs requires the energy to be as high as ~10^60 erg, suggesting that their sources could be active galactic nuclei. We calculate the spectral energy distributions (SEDs) of the OGREs and their evolution, including synchrotron, inverse Compton (IC) and bremsstrahlung emission from electrons, and pion-decay emission from protons. We find that the SEDs of the younger OGREs are not greatly different from those of older ones currently observable as ORCs if radiative cooling of electrons is effective. As such younger OGREs are expected to be rarer and smaller, they may not be readily observable. However, if radiative cooling of electrons is ineffective, younger OGREs may be detectable in X-rays.",
        "citation_title": "Broadband non-thermal emission of odd radio circles induced by explosive galactic outflow remnants and their evolution",
        "date_delivered": "[Submitted on 20 Nov 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "UVCANDELS is a HST Cycle-26 Treasury Program awarded 164 orbits of primary ultraviolet (UV) F275W imaging and coordinated parallel optical F435W imaging in four CANDELS fields: GOODS-N, GOODS-S, EGS, and COSMOS, covering a total area of $\\sim426$ arcmin$^2$. This is $\\sim2.7$ times larger than the area covered by previous deep-field space UV data combined, reaching a depth of about 27 and 28 ABmag ($5\\sigma$ in $0.2\"$ apertures) for F275W and F435W, respectively. Along with the new photometric catalogs, we present an analysis of the rest-frame UV luminosity function (LF), relying on our UV-optimized aperture photometry method yielding a factor of $1.5\\times$ increase than the H-isophot aperture photometry in the signal-to-noise ratios of galaxies in our F275W imaging. Using well tested photometric redshift measurements we identify 5810 galaxies at redshifts $0.6<z<1$, down to an absolute magnitude of $M_\\text{UV} = -14.2$. In order to minimize the effect of uncertainties in estimating the completeness function, especially at the faint-end, we restrict our analysis to sources above $30\\%$ completeness, which provides a final sample of 4726 galaxies at $-21.5<M_\\text{UV}<-15.5$. We performed a maximum likelihood estimate to derive the best-fit parameters of the UV LF. We report a best-fit faint-end slope of $\\alpha = -1.359^{+0.041}_{-0.041}$ at $z \\sim 0.8$. Creating sub-samples at $z\\sim0.7$ and $z\\sim0.9$, we observe a possible evolution of $\\alpha$ with redshift. The unobscured UV luminosity density at $M_\\text{UV}<-10$ is derived as $\\rho_\\text{UV}=1.339^{+0.027}_{-0.030}\\ (\\times10^{26} \\text{ergs/s/Hz/Mpc}^3)$ using our best-fit LF parameters. The new F275W and F435 photometric catalogs from UVCANDELS have been made publicly available on the Barbara A. Mikulski Archive for Space Telescopes (MAST).",
        "citation_title": "The UV luminosity function at 0.6 < z < 1 from UVCANDELS",
        "date_delivered": "[Submitted on 27 Nov 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Axion-like particles are predicted in many physics scenarios beyond the Standard Model (SM). Their interactions with SM particles may arise from the triangle anomaly of the associated global symmetry, along with other SM global and gauge symmetries, including anomalies with the global baryon number and electromagnetic gauge symmetries. We initiate the phenomenological study of the corresponding ``electrobaryonic axion\", a particle that couples with both the baryon chemical potential and the electromagnetic field. Neutron stars, particularly magnetars, possessing high baryon density and strong magnetic fields, can naturally develop a thin axion hair around their surface. In this study, we calculate this phenomenon, considering the effects of neutron star rotation and general relativity. For axion particles lighter than the neutron star rotation frequency, the anomalous interaction can also induce the emission of axion particles from the neutron star. This emission, in the light axion regime, can have a significant contribution to the neutron star cooling rate.",
        "citation_title": "Electrobaryonic axion: hair of neutron stars",
        "date_delivered": "[Submitted on 30 Nov 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present the SARAO MeerKAT Galactic Plane Survey (SMGPS), a 1.3 GHz continuum survey of almost half of the Galactic Plane (251\u00b0$\\le l \\le$ 358\u00b0and 2\u00b0$\\le l \\le$ 61\u00b0at $|b| \\le 1.5\u00b0$). SMGPS is the largest, most sensitive and highest angular resolution 1 GHz survey of the Plane yet carried out, with an angular resolution of 8\" and a broadband RMS sensitivity of $\\sim$10--20 $\\mu$ Jy/beam. Here we describe the first publicly available data release from SMGPS which comprises data cubes of frequency-resolved images over 908--1656 MHz, power law fits to the images, and broadband zeroth moment integrated intensity images. A thorough assessment of the data quality and guidance for future usage of the data products are given. Finally, we discuss the tremendous potential of SMGPS by showcasing highlights of the Galactic and extragalactic science that it permits. These highlights include the discovery of a new population of non-thermal radio filaments; identification of new candidate supernova remnants, pulsar wind nebulae and planetary nebulae; improved radio/mid-IR classification of rare Luminous Blue Variables and discovery of associated extended radio nebulae; new radio stars identified by Bayesian cross-matching techniques; the realisation that many of the largest radio-quiet WISE HII region candidates are not true HII regions; and a large sample of previously undiscovered background HI galaxies in the Zone of Avoidance.",
        "citation_title": "The SARAO MeerKAT 1.3 GHz Galactic Plane Survey",
        "date_delivered": "[Submitted on 12 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In the Randall-Sundrum (RS) II braneworld scenario, general relativity (GR) is modified by adding an extra dimension such that it is indistinguishable from GR in the weak gravity limit. However, such modifications may leave a mark in the strong field regime. We therefore analyze massive scalar perturbations around rotating black holes in the RS II model. Unlike black holes in GR, these braneworld black holes carry a tidal charge that contains information about the extra spatial dimension, and the rotation parameter for such black holes can exceed unity. Through the method of continued fractions, we investigate the quasinormal mode spectra, and the superradiant instabilities associated with the existence of quasibound states, that is, gravitational atoms. In comparison to the four-dimensional Kerr black hole, we report distinctive signatures of the tidal charge and the rotation parameter, which manifest as signals of the extra dimension, on both the fundamental quasinormal mode and the formation of gravitational atoms. These findings offer insights into testing modifications to GR and detecting ultralight bosonic particles around black holes.",
        "citation_title": "Gravitational atoms in the braneworld scenario",
        "date_delivered": "[Submitted on 12 Dec 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We report the reconstruction of the mass component spectra of cosmic rays (protons, helium, carbon, silicon and iron) and their mean mass composition, at energies from 1.4 to 100 PeV. The results are derived from the archival data of the extensive air shower experiment KASCADE. We use a novel machine learning technique developed specifically for this reconstruction, and post-LHC hadronic interaction models: QGSJet-II.04, EPOS-LHC and Sibyll 2.3c. We have found an excess of the proton component and a deficit of intermediate and heavy nuclei components compared to the original KASCADE results. The spectra of protons and helium show a knee-like behavior at ~ 4.4 PeV and ~ 11 PeV, with significances 5.2${\\sigma}$ and 3.9${\\sigma}$, respectively. The spectrum of the iron component has a hint (2.4${\\sigma}$) of a hardening at ~ 4.5 PeV, which can be interpreted as a counterpart of a hardening in the proton spectrum at 166 TeV, recently reported by the GRAPES-3 experiment. The systematic uncertainties of our analysis were found to be smaller than those of the original KASCADE, as well as those of IceTop and TALE experiments, over the most part of the energy range studied. We also estimated separately the uncertainty related to the difference between the three mentioned hadronic interaction models. We also compute a mean logarithm mass of cosmic ray flux as a function of energy. It is in agreement with the results of IceTop, TALE and LHAASO within the uncertainties.",
        "citation_title": "Energy spectra of elemental groups of cosmic rays with the KASCADE experiment data and machine learning",
        "date_delivered": "[Submitted on 13 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In some quantum gravity (QG) theories, Lorentz symmetry may be broken above the Planck scale. The Lorentz invariance violation (LIV) may induce observable effects at low energies and be detected at high energy astrophysical measurements. The Large High Altitude Air Shower Observatory(LHAASO) has detected the onset, rise, and decay phases of the afterglow of GRB 221009A, covering a wide energy range of photons approximately from $0.2$ to $18$ TeV. This observation provides an excellent opportunity to study the Lorentz invariance violation effect. In this study, we simultaneously utilize the data from the KM2A and WCDA detectors of LHAASO, and apply two event by event methods, namely the pair view method and maximum likelihood method, to investigate LIV. We obtain stringent constraints on the QG energy scale. For instance, through the maximum likelihood method, we determine the 95$\\%$ confidence level lower limits to be $E_{QG,1} > 14.7 (6.5)\\times 10^{19}$GeV for the subluminal (superluminal) scenario of $n = 1$, and $E_{QG,2} > 12.0 (7.2)\\times 10^{11}$GeV for the subluminal (superluminal) scenario of $n = 2$. We find that the rapid rise and slow decay behaviors of the afterglow can impose strong constraints on the subluminal scenario, while the constraints are weaker for the superluminal scenario.",
        "citation_title": "Constraints on Lorentz invariance violation from the LHAASO observation of GRB 221009A",
        "date_delivered": "[Submitted on 14 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Metastable cosmic strings appear in models of new physics with a two-step symmetry breaking $G\\to H\\to 1$, where $\\pi_1(H)\\neq 0$ and $\\pi_1(G)=0$. They decay via the monopole-antimonopole pair creation inside. Conventionally, the breaking rate has been estimated by an infinitely thin string approximation, which requires a large hierarchy between the symmetry breaking scales. In this paper, we reexamine it by taking into account the finite sizes of both the cosmic string and the monopole. We obtain a robust lower limit on the tunneling factor $e^{-S_B}$ even for regimes the conventional estimate is unreliable. In particular, it is relevant to the cosmic string interpretation of the gravitational wave signals recently reported by pulsar timing array experiments.",
        "citation_title": "Revisiting Metastable Cosmic String Breaking",
        "date_delivered": "[Submitted on 25 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We develop a numerical approach to compute polar parity perturbations within fully relativistic models of black hole systems embedded in generic, spherically symmetric, anisotropic fluids. We apply this framework to study gravitational wave generation and propagation from extreme mass-ratio inspirals in the presence of several astrophysically relevant dark matter models, namely the Hernquist, Navarro-Frenk-White, and Einasto profiles. We also study dark matter spike profiles obtained from a fully relativistic calculation of the adiabatic growth of a BH within the Hernquist profile, and provide a closed-form analytic fit of these profiles. Our analysis completes prior numerical work in the axial sector, yielding a fully numerical pipeline to study black hole environmental effects. We study the dependence of the fluxes on the DM halo mass and compactness. We find that, unlike the axial case, polar fluxes are not adequately described by simple gravitational-redshift effects, thus offering an exciting avenue for the study of black hole environments with gravitational waves.",
        "citation_title": "Black holes surrounded by generic matter distributions: polar perturbations and energy flux",
        "date_delivered": "[Submitted on 1 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "\"Changing-look\" active galactic nuclei (CL-AGNs) challenge our basic ideas about the physics of accretion flows and circumnuclear gas around supermassive black holes. Using first-year Sloan Digital Sky Survey V (SDSS-V) repeated spectroscopy of nearly 29,000 previously known AGNs, combined with dedicated follow-up spectroscopy, and publicly available optical light curves, we have identified 116 CL-AGNs where (at least) one broad emission line has essentially (dis-)appeared, as well as 88 other extremely variable systems. Our CL-AGN sample, with 107 newly identified cases, is the largest reported to date, and includes $\\sim0.4\\%$ of the AGNs reobserved in first-year SDSS-V operations. Among our CL-AGNs, 67% exhibit dimming while 33% exhibit brightening. Our sample probes extreme AGN spectral variability on months to decades timescales, including some cases of recurring transitions on surprisingly short timescales ($\\lesssim 2$ months in the rest frame). We find that CL events are preferentially found in lower-Eddington-ratio ($f_{Edd}$) systems: Our CL-AGNs have a $f_{Edd}$ distribution that significantly differs from that of a carefully constructed, redshift- and luminosity-matched control sample (Anderson-Darling test yielding $p_{\\rm AD}\\approx 6\\times10^{-5}$; median $f_{Edd}\\approx0.025$ vs. $0.043$). This preference for low $f_{Edd}$ strengthens previous findings of higher CL-AGN incidence at lower $f_{Edd}$, found in smaller samples. Finally, we show that the broad MgII emission line in our CL-AGN sample tends to vary significantly less than the broad H$\\beta$ emission line. Our large CL-AGN sample demonstrates the advantages and challenges in using multi-epoch spectroscopy from large surveys to study extreme AGN variability and physics.",
        "citation_title": "Exploring Changing-look Active Galactic Nuclei with the Sloan Digital Sky Survey V: First Year Results",
        "date_delivered": "[Submitted on 3 Jan 2024 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "This paper analyzes the possibility of bouncing and non-bouncing universes in the framework of four-dimensional Einstein-Gauss-Bonnet (4D-EGB) gravity, corresponding respectively to negative and positive coupling constants $\\lambda$ of the Gauss-Bonnet term. We also use the Horndeski-type scalar-tensor theory to assess the role of a scalar charge $C$ as a geometrical contribution to the radiation in the Universe. We modify the expansion history of the universe to allow for modifications induced by the 4D-EGB gravity. Using Planck measurements of the cosmic microwave background anisotropies as well as various datasets of baryonic acoustic oscillations, we set the upper bounds $\\lambda \\le 10^{-16} \\text{(km/s/Mpc)}^{-2} $ and $\\lambda \\le 10^{-30} \\text{(km/s/Mpc)}^{-2} $ for the non-bouncing and bouncing scenarios. The upper limit in the latter case is mainly driven by the requirement to conservatively respect the thermal history at energy scales of the standard model of particle physics. We also find that the contribution of the geometrical radiation-like term of the model cannot exceed 10\\% of the current radiation in the Universe. The possibility of an early inflationary phase produced by a single scalar field is also studied and found to be feasible in both bouncing and non-bouncing scenarios. This study shows the feasibility of a bouncing universe, even with normal matter sector, in the 4D-EGB gravity. More theoretical investigation is required to further explore possible observational predictions of the model that can distinguish between general relativity and 4D-EGB gravity.",
        "citation_title": "Observational Feasibility of 4D Einstein-Gauss-Bonnet Cosmology: Bouncing and Non-Bouncing Universes",
        "date_delivered": "[Submitted on 22 Jan 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "New boron abundances or upper limits have been determined for 8 early-B stars in the young Galactic open cluster NGC 3293, using ultraviolet spectra obtained by the Hubble Space Telescope Cosmic Origins Spectrograph. With previous observations, there are now 18 early-B stars in this cluster with boron measurements. Six of the newly observed stars have projected rotational velocities greater than 200 km/s, allowing new constraints on rotationally driven mixing in main-sequence stars. When comparing to synthetic model populations, we find that the majority of our sample stars agree well with the predicted trends of stronger boron depletion for larger rotation and for larger mass or luminosity. Based on those, a smaller than the canonical rotational mixing efficiency,(fc = 0.0165 vs the more standard value of 0.033), appears to be required. However, our five most slowly rotating stars are not well explained by rotational mixing, and we speculate that they originate from binary mergers.",
        "citation_title": "Boron Abundances in Early B Dwarfs of the Galactic Open Cluster NGC 3293",
        "date_delivered": "[Submitted on 23 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present a new method and implementation to obtain Bayesian posteriors on the amplitude parameters $\\{h_0, \\cos \\iota, \\psi, \\phi_0\\}$ of continuous-gravitational waves emitted by known pulsars. This approach leverages the well-established $\\mathcal{F}$-statistic framework and software. We further explore the benefits of employing a likelihood function that is analytically marginalized over $\\phi_0$, which avoids signal degeneracy problems in the $\\psi$-$\\phi_0$ subspace. The method is tested on simulated signals, hardware injections in Advanced-LIGO detector data, and by performing percentile-percentile (PP) self-consistency tests of the posteriors via Monte-Carlo simulations. We apply our methodology to PSR J1526-2744, a recently discovered millisecond pulsar. We find no evidence for a signal and obtain a Bayesian upper limit $h_0^{95\\%}$ on the gravitational-wave amplitude of approximately $7 \\times 10^{-27}$, comparable with a previous frequentist upper limit.",
        "citation_title": "Bayesian $\\mathcal{F}$-statistic-based parameter estimation of continuous gravitational waves from known pulsars",
        "date_delivered": "[Submitted on 30 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present a 400-800 MHz polarimetric analysis of 128 non-repeating fast radio bursts (FRBs) from the first CHIME/FRB baseband catalog, increasing the total number of FRB sources with polarization properties by a factor of ~3. 89 FRBs have >6${\\sigma}$ linearly polarized detections, 29 FRBs fall below this significance threshold and are deemed linearly unpolarized, and for 10 FRBs the polarization data are contaminated by instrumental polarization. For the 89 polarized FRBs, we find Faraday rotation measure (RM) amplitudes, after subtracting approximate Milky Way contributions, in the range 0.5-1160 rad m$^{-2}$ with a median of 53.8 rad m$^{-2}$. Most non-repeating FRBs in our sample have RMs consistent with Milky Way-like host galaxies and their linear polarization fractions range from <10% to 100% with a median of 63%. We see marginal evidence that non-repeating FRBs have more constraining lower limits than repeating FRBs for the host electron-density-weighted line-of-sight magnetic field strength. We classify the non-repeating FRB polarization position angle (PA) profiles into four archetypes: (i) single component with constant PA (57% of the sample), (ii) single component with variable PA (10%), (iii) multiple components with a single constant PA (22%), and (iv) multiple components with different or variable PAs (11%). We see no evidence for population-wide frequency-dependent depolarization and, therefore, the spread in the distribution of fractional linear polarization is likely intrinsic to the FRB emission mechanism. Finally, we present a novel method to derive redshift lower limits for polarized FRBs without host galaxy identification and test this method on 20 FRBs with independently measured redshifts.",
        "citation_title": "Polarization properties of 128 non-repeating fast radio bursts from the first CHIME/FRB baseband catalog",
        "date_delivered": "[Submitted on 30 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Gamma-ray bursts (GRBs) can be probes of the early universe, but currently, only 26% of GRBs observed by the Neil Gehrels Swift Observatory GRBs have known redshifts ($z$) due to observational limitations. To address this, we estimated the GRB redshift (distance) via a supervised statistical learning model that uses optical afterglow observed by Swift and ground-based telescopes. The inferred redshifts are strongly correlated (a Pearson coefficient of 0.93) with the observed redshifts, thus proving the reliability of this method. The inferred and observed redshifts allow us to estimate the number of GRBs occurring at a given redshift (GRB rate) to be 8.47-9 $yr^{-1} Gpc^{-1}$ for $1.9<z<2.3$. Since GRBs come from the collapse of massive stars, we compared this rate with the star formation rate highlighting a discrepancy of a factor of 3 at $z<1$.",
        "citation_title": "Gamma-ray Bursts as Distance Indicators by a Statistical Learning Approach",
        "date_delivered": "[Submitted on 7 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present an extension of the massively parallel, GPU native, astrophysical hydrodynamics code Cholla to magnetohydrodynamics (MHD). Cholla solves the ideal MHD equations in their Eulerian form on a static Cartesian mesh utilizing the Van Leer + Constrained Transport integrator, the HLLD Riemann solver, and reconstruction methods at second and third order. Cholla's MHD module can perform $\\approx260$ million cell updates per GPU-second on an NVIDIA A100 while using the HLLD Riemann solver and second order reconstruction. The inherently parallel nature of GPUs combined with increased memory in new hardware allows Cholla's MHD module to perform simulations with resolutions $\\sim500^3$ cells on a single high end GPU (e.g. an NVIDIA A100 with 80GB of memory). We employ GPU direct MPI to attain excellent weak scaling on the exascale supercomputer \\textit{Frontier}, while using 74,088 GPUs and simulating a total grid size of over 7.2 trillion cells. A suite of test problems highlights the accuracy of Cholla's MHD module and demonstrates that zero magnetic divergence in solutions is maintained to round off error. We also present new testing and continuous integration tools using GoogleTest, GitHub Actions, and Jenkins that have made development more robust and accurate and ensure reliability in the future.",
        "citation_title": "Cholla-MHD: An Exascale-Capable Magnetohydrodynamic Extension to the Cholla Astrophysical Simulation Code",
        "date_delivered": "[Submitted on 7 Feb 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Low-acceleration gravitational anomaly is investigated with a new method of exploiting the normalized velocity profile $\\tilde{v}\\equiv v_p/v_c$ of wide binary stars as a function of the normalized sky-projected radius $s/r_{\\rm{M}}$ where $v_p$ is the sky-projected relative velocity between the pair, $v_c$ is the Newtonian circular velocity at the sky-projected separation $s$, and $r_{\\rm{M}}$ is the MOND radius. With a Monte Carlo method Gaia observed binaries and their virtual Newtonian counterparts are probabilistically distributed on the $s/r_{\\rm{M}}$ versus $\\tilde{v}$ plane and a logarithmic velocity ratio parameter $\\Gamma$ is measured in the bins of $s/r_{\\rm{M}}$. With three samples of binaries covering a broad range in size, data quality, and implied fraction of hierarchical systems including a new sample of 6389 binaries selected with accurate distances and radial velocities, I find a unanimous systematic variation from the Newtonian flat line. With $\\Gamma=0$ at $s/r_{\\rm{M}}\\lesssim 0.15$ or $s\\lesssim 1$~kilo astronomical units (kau), I get $\\Gamma=0.068\\pm 0.015$ (stat) $_{-0.015}^{+0.024}$ (syst) for $s/r_{\\rm{M}} \\gtrsim 0.7$ or $s\\gtrsim 5$~kau. The gravitational anomaly (i.e.\\ acceleration boost) factor given by $\\gamma_g = 10^{2\\Gamma}$ is measured to be $\\gamma_g = 1.37_{-0.09}^{+0.10}$ (stat) $_{-0.09}^{+0.16}$ (syst). With a reduced $\\chi^2$ test of Newtonian and Milgromian nonrelativistic theories, I find that Newtonian gravity is ruled out at $5.8\\sigma$ ($\\chi^2_\\nu=9.4$) by the new sample (and $9.2\\sigma$ by the largest sample used). The Milgromian AQUAL theory is acceptable with $0.5\\lesssim \\chi^2_\\nu\\lesssim 3.1$. These results agree well with earlier results with the ``acceleration-plane analysis'' for a variety of samples and the ``stacked velocity profile analysis'' for a pure binary sample.",
        "citation_title": "Measurements of the Low-Acceleration Gravitational Anomaly from the Normalized Velocity Profile of Gaia Wide Binary Stars and Statistical Testing of Newtonian and Milgromian Theories",
        "date_delivered": "[Submitted on 8 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Recent observations of high-redshift galaxies ($z \\lesssim 7$) reveal that a substantial fraction have turbulent, gas-rich disks with well-ordered rotation and elevated levels of star formation. In some instances, disks show evidence of spiral arms, with bar-like structures. These remarkable observations have encouraged us to explore a new class of dynamically self-consistent models using our hydrodynamic N-body simulation framework that mimic a plausible progenitor of the Milky Way at high redshift. We explore disk gas fractions of $f_{\\rm gas} = 0, 20, 40, 60, 80, 100\\%$ and track the creation of stars and metals. The high gas surface densities encourage vigorous star formation, which in turn couples with the gas to drive turbulence. We explore three distinct histories: (i) there is no ongoing accretion and the gas is used up by the star formation; (ii) the star-forming gas is replenished by cooling in the hot halo gas; (iii) in a companion paper, we revisit the models in the presence of a strong perturbing force. At low $f_{\\rm disk}$ ($<0.3$), where $f_{\\rm disk}$ is the mass fraction of stars relative to dark matter within 2.2 $R_{\\rm disk}$, a bar does not form in a stellar disk; this remains true even when gas dominates the inner disk potential. For a dominant baryon disk ($f_{\\rm disk} \\gtrsim 0.5$) at all gas fractions, the turbulent gas forms a strong \"radial shear flow\" that leads to an intermittent star-forming bar within about 500 Myr; turbulent gas speeds up the formation of bars compared to gas-free models. For $f_{\\rm gas} \\lesssim 60\\%$, all bars survive, but for higher gas fractions, the bar devolves into a central bulge after 1 Gyr. The star-forming bars are reminiscent of recent discoveries in high-redshift ALMA observations of gaseous disks.",
        "citation_title": "Turbulent gas-rich disks at high redshift: bars & bulges in a radial shear flow",
        "date_delivered": "[Submitted on 8 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Context. Weak gravitational lensing offers a powerful method to investigate the projected matter density distribution within galaxy clusters, granting crucial insights into the broader landscape of dark matter on cluster scales. Aims. In this study, we make use of the large photometric galaxy cluster data set derived from the publicly available Third Data Release of the Kilo-Degree Survey, along with the associated shear signal. Our primary objective is to model the peculiar sharp transition in the cluster profile slope, that is what is commonly referred to as the splashback radius. The data set under scrutiny includes 6962 galaxy clusters, selected by AMICO - an optimised detection algorithm of galaxy clusters - on the KiDS-DR3 data, in the redshift range of 0.1 < z < 0.6, all observed at a signal-to-noise ratio greater than 3.5. Methods. Employing a comprehensive Bayesian analysis, we model the stacked excess surface mass density distribution of the clusters. We adopt a model from recent results on numerical simulations that capture the dynamics of both orbiting and infalling materials, separated by the region where the density profile slope undergoes a pronounced deepening. Results. We find that the adopted profile successfully characterizes the cluster masses, consistent with previous works, and models the deepening of the slope of the density profiles measured with weak-lensing data up to the outskirts. Moreover, we measure the splashback radius of galaxy clusters and show that its value is close to the radius within which the enclosed overdensity is 200 times the mean matter density of the Universe, while theoretical models predict a larger value consistent with a low accretion rate. This points to a potential bias of optically selected clusters preferentially characterized by a high density at small scales compared to a pure mass-selected cluster sample.",
        "citation_title": "AMICO galaxy clusters in KiDS-DR3: measuring the splashback radius from weak gravitational lensing",
        "date_delivered": "[Submitted on 9 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We explore three-body binary formation (3BBF), the formation of a bound system via gravitational scattering of three initially unbound bodies (3UB), using direct numerical integrations. For the first time, we consider systems with unequal masses, as well as finite-size and post-Newtonian effects. Our analytically derived encounter rates and numerical scattering results reproduce the 3BBF rate predicted by Goodman \\& Hut (1993) for hard binaries in dense star clusters. We find that 3BBF occurs overwhelmingly through nonresonant encounters and that the two most massive bodies are never the most likely to bind. Instead, 3BBF favors pairing the two least massive bodies (for wide binaries) or the most plus least massive bodies (for hard binaries). 3BBF overwhelmingly favors wide binary formation with super-thermal eccentricities, perhaps helping to explain the eccentric wide binaries observed by Gaia. Hard binary formation is far rarer, but with a thermal eccentricity distribution. The semimajor axis distribution scales cumulatively as $a^3$ for hard and slightly wider binaries. Though mergers are rare between black holes when including relativistic effects, direct collisions occur frequently between main-sequence stars -- more often than hard 3BBF. Yet, these collisions do not significantly suppress hard 3BBF at the low velocity dispersions typical of open or globular clusters. Energy dissipation through gravitational radiation leads to a small probability of a bound, hierarchical triple system forming directly from 3UB.",
        "citation_title": "On Binary Formation from Three Initially Unbound Bodies",
        "date_delivered": "[Submitted on 19 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We study the supersymmetric Q balls which decay at present and find that they create a distinctive spectrum of gamma rays at around O(10) MeV. The charge of the Q ball is lepton numbers in order for the lifetime to be as long as the present age of the universe, and the main decay products are light leptons. However, as the charge of the Q ball decreases, the decay channel into pions becomes kinematically allowed towards the end of the decay, and the pions are produced at rest. Immediately, $\\pi^0$ decays into two photons with the energy of 67.5 MeV, half the pion mass, which exhibits a unique emission line. In addition, $\\pi^\\pm$ decay into $\\mu^\\pm$, which further decay with emitting internal bremsstrahlung, whose spectrum has a sharp cutoff at $\\sim$50 MeV. If the observations would find these peculiar features of the gamma-ray spectrum in the future, it could be a smoking gun of the supersymmetric Q-ball decay at present.",
        "citation_title": "MeV gamma rays from Q-ball decay",
        "date_delivered": "[Submitted on 4 Mar 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Galaxy submillimetre number counts are a fundamental measurement in our understanding of galaxy evolution models. Most early measurements are obtained via single-dish telescopes with substantial source confusion, whereas recent interferometric observations are limited to small areas. We used a large database of ALMA continuum observations to accurately measure galaxy number counts in multiple (sub)millimetre bands, thus bridging the flux density range between single-dish surveys and deep interferometric studies. We continued the Automated Mining of the ALMA Archive in the COSMOS Field project (A$^3$COSMOS) and extended it with observations from the GOODS-South field (A$^3$GOODSS). The database consists of ~4,000 pipeline-processed continuum images from the public ALMA archive, yielding 2,050 unique detected sources. To infer galaxy number counts, we constructed a method to reduce the observational bias inherent to targeted pointings that dominate the database. This method comprises a combination of image selection, masking, and source weighting. The effective area was calculated by accounting for inhomogeneous wavelengths, sensitivities, and resolutions and for spatial overlap between images. We tested and calibrated our method with simulations. We derived the number counts in a consistent and homogeneous way in four different ALMA bands covering a relatively large area. The results are consistent with number counts from the literature within the uncertainties. In Band 7, at the depth of the inferred number counts, ~40% of the cosmic infrared background is resolved into discrete sources. This fraction, however, decreases with wavelength, reaching ~4% in Band 3. Finally, we used the number counts to test models of dusty galaxy evolution, and find a good agreement within the uncertainties.",
        "citation_title": "A$^3$COSMOS and A$^3$GOODSS: Continuum Source Catalogues and Multi-band Number Counts",
        "date_delivered": "[Submitted on 5 Mar 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "When orbiting hotter stars, hot Jupiters are often highly inclined relative to their host star equator planes. By contrast, hot Jupiters orbiting cooler stars are more aligned. Prior attempts to explain this correlation between stellar obliquity and effective temperature have proven problematic. We show how resonance locking -- the coupling of the planet's orbit to a stellar gravity mode (g mode) -- can solve this mystery. Cooler stars with their radiative cores are more likely to be found with g-mode frequencies increased substantially by core hydrogen burning. Strong frequency evolution in resonance lock drives strong tidal evolution; locking to an axisymmetric g mode damps semi-major axes, eccentricities, and as we show for the first time, obliquities. Around cooler stars, hot Jupiters evolve into spin-orbit alignment and may avoid engulfment. Hotter stars lack radiative cores, and therefore preserve congenital spin-orbit misalignments. We focus on resonance locks with axisymmetric modes, supplementing our technical results with simple physical interpretations, and show that non-axisymmetric modes also damp obliquity. Outstanding issues regarding the dissipation of tidally-excited modes and the disabling of resonance locks are discussed quantitatively.",
        "citation_title": "Damping Obliquities of Hot Jupiter Hosts by Resonance Locking",
        "date_delivered": "[Submitted on 8 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Shear Alfven wave parametric decay instability (PDI) provides a potential path toward significant wave dissipation and plasma heating. However, fundamental questions regarding how PDI is excited in a realistic three-dimensional (3D) open system and how critically the finite perpendicular wave scale--as found in both laboratory and space plasmas--affects the excitation remain poorly understood. Here, we present the first 3D, open-boundary, hybrid kinetic-fluid simulations of kinetic Alfven wave PDI in low-beta plasmas. Key findings are that the PDI excitation is strongly limited by the wave damping present, including electron-ion collisional damping (represented by a constant resistivity) and geometrical attenuation associated with the finite-scale Alfven wave, and ion Landau damping of the child acoustic wave. The perpendicular wave scale alone, however, plays no discernible role: waves of different perpendicular scales exhibit similar instability growth as long as the magnitude of the parallel ponderomotive force remains unchanged. These findings are corroborated by theoretical analysis and estimates. The new understanding of 3D kinetic Alfv\u00e9n wave PDI physics is essential for laboratory study of the basic plasma process and may also help evaluate the relevance/role of PDI in low-beta space plasmas.",
        "citation_title": "Effects of wave damping and finite perpendicular scale on three-dimensional Alfven wave parametric decay in low-beta plasmas",
        "date_delivered": "[Submitted on 13 Mar 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In view to scrutinize the idea that nonlocal modifications of General Relativity could dynamically address the dark energy problem, we investigate the evolution of the Universe at infrared scales as an Infinite Derivative Gravity model of the Ricci scalar, without introducing the cosmological constant $\\Lambda$ or any scalar field. The accelerated expansion of the late Universe is shown to be compatible with the emergence of nonlocal gravitational effects at sufficiently low energies. A technique for circumventing the mathematical complexity of the nonlocal cosmological equations is developed and, after drawing a connection with the Starobinsky gravity, verifiable predictions are considered, like a possible decreasing in the strength of the effective gravitational constant. In conclusion, the emergence of nonlocal gravity corrections at given scales could be an efficient mechanism to address the dark energy problem.",
        "citation_title": "Can nonlocal gravity really explain dark energy?",
        "date_delivered": "[Submitted on 17 Mar 2024 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We report the discovery of a $(1.0 \\pm 0.28) \\times 10^{10}$ M$_\\odot$ Supermassive Black Hole (BH) at the centre of NGC 708, the Brightest Cluster Galaxy of Abell 262. Such high BH masses are very rare and allow to investigate BH - host galaxy scaling relations at the high mass end, which in turn provide hints about the (co)evolution of such systems. NGC~708 is found to be an outlier in all the canonical scaling relations except for those linking the BH mass to the core properties. The galaxy mass-to-light ratio points to a Kroupa IMF rather than Salpeter, with this finding confirmed using photometry in two different bands. We perform this analysis using our novel triaxial Schwarzschild code to integrate orbits in a 5-dimensional space, using a semi-parametric deprojected light density to build the potential and non-parametric Line-of-Sight Velocity Distributions (LOSVDs) derived from long-slit spectra recently acquired at Large Binocular Telescope (LBT) to exploit the full information in the kinematic. We find that the galaxy geometry changes as a function of the radius going from prolate, nearly spherical in the central regions to triaxial at large radii, highlighting the need to go beyond constant shape profiles. Our analysis is only the second of its kind and will systematically be used in the future to hunt Supermassive Black Holes in giant ellipticals.",
        "citation_title": "Triaxial Schwarzschild Models of NGC 708: a 10-billion solar mass black hole in a low dispersion galaxy with a Kroupa IMF",
        "date_delivered": "[Submitted on 18 Mar 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "A compact source, G0.02467-0.0727, was detected in ALMA \\threemm observations in continuum and very broad line emission. The continuum emission has a spectral index $\\alpha\\approx3.3$, suggesting that the emission is from dust. The line emission is detected in several transitions of CS, SO, and SO$_2$ and exhibits a line width FWHM $\\approx160$ \\kms. The line profile appears Gaussian. The emission is weakly spatially resolved, coming from an area on the sky $\\lesssim1\"$ in diameter ($\\lesssim10^4$ AU at the distance of the Galactic Center; GC). The centroid velocity is $v_{LSR}\\approx40$-$50$ \\kms, which is consistent with a location in the Galactic Center. With multiple SO lines detected, and assuming local thermodynamic equilibrium (LTE) conditions, $T_\\mathrm{LTE} = 13$ K, which is colder than seen in typical GC clouds, though we cannot rule out low-density, subthermally excited, warmer gas. Despite the high velocity dispersion, no emission is observed from SiO, suggesting that there are no strong ($\\gtrsim10~\\mathrm{km~s}^{-1}$) shocks in the molecular gas. There are no detections at other wavelengths, including X-ray, infrared, and radio.\nWe consider several explanations for the Millimeter Ultra-Broad Line Object (MUBLO), including protostellar outflow, explosive outflow, collapsing cloud, evolved star, stellar merger, high-velocity compact cloud, intermediate mass black hole, and background galaxy. Most of these conceptual models are either inconsistent with the data or do not fully explain it. The MUBLO is, at present, an observationally unique object.",
        "citation_title": "A broad linewidth, compact, millimeter-bright molecular emission line source near the Galactic Center",
        "date_delivered": "[Submitted on 11 Apr 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Andromeda XVIII is an isolated dwarf galaxy 579 kpc away from the nearest large galaxy, M31. It is a candidate ``backsplash galaxy'' that might have been affected by a close passage to M31. We present new Keck/DEIMOS spectroscopy of Andromeda XVIII to assess the likelihood that it is a backsplash galaxy. We estimated the velocities, metallicities ([Fe/H]), and $\\alpha$-enhancements ([$\\alpha$/Fe]) for 56 probable members. We estimated Andromeda XVIII's mean heliocentric velocity, rotation velocity, position angle of the rotation axis, and velocity dispersion using maximum likelihood coupled with a Monte Carlo Markov chain (MCMC). There is no evidence for bulk rotation, though sub-populations might be rotating. The mean heliocentric velocity is -337.2 km s$^{-1}$. The line-of-sight velocity relative to M31 is -36 km s$^{-1}$ (approaching), which is lower than the escape velocity from M31. Based on the abundances of 38 stars with low errors ($\\delta [Fe/H] < 0.3$) compared to a total of 56 probable members, parameters for the simplest chemical evolution models were estimated using maximum likelihood coupled with an MCMC. The metallicity distribution is inconsistent with these models due to a sharp metal-rich cut-off. Together, the metallicity distribution and the mean velocity are consistent with a sudden interruption of star formation. One possible cause for this quenching may be rapid gas loss due to ram pressure stripping during a close passage by M31 in the past. We also consider a past major merger as another possible cause.",
        "citation_title": "Kinematics and metallicity of the dwarf spheroidal galaxy Andromeda XVIII",
        "date_delivered": "[Submitted on 17 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The Event Horizon Telescope (EHT) has revolutionized our ability to study black holes by providing unprecedented spatial resolution and unveiling horizon-scale details. With advancements leading to the next-generation EHT, there is potential to probe even deeper into the black hole's dark region, especially the inner shadow characterized by low-intensity foreground emissions from the jet, thanks to a significant enhancement in dynamic range by two orders of magnitude. We demonstrate how such enhanced observations could transform supermassive black holes into powerful probes for detecting annihilating dark matter, which can form a dense profile in the vicinity of supermassive black holes, by examining the morphology of the black hole image.",
        "citation_title": "Illuminating Black Hole Shadow with Dark Matter Annihilation",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Numerically simulating magnetohydrodynamics (MHD) poses notable challenges, including the suppression of spurious oscillations near discontinuities (e.g., shocks) and preservation of essential physical structures (e.g., the divergence-free constraint of magnetic field and the positivity of density and pressure). This paper develops structure-preserving oscillation-eliminating discontinuous Galerkin (OEDG) schemes for ideal MHD. The schemes leverage a locally divergence-free (LDF) oscillation-eliminating (OE) procedure to suppress spurious oscillations while retaining the LDF property of magnetic field and many desirable attributes of original DG schemes, such as conservation, local compactness, and optimal convergence rates. The OE procedure is based on the solution operator of a novel damping equation, a linear system of ordinary differential equations that are exactly solvable without any discretization. The OE procedure is performed after each Runge-Kutta stage and does not impact DG spatial discretization, facilitating its easy integration into existing DG codes as an independent module. Moreover, this paper presents a rigorous positivity-preserving (PP) analysis of the LDF OEDG schemes on Cartesian meshes, utilizing the optimal convex decomposition technique and the geometric quasi-linearization (GQL) approach. Efficient PP LDF OEDG schemes are derived by incorporating appropriate discretization of Godunov-Powell source terms into only the discrete equations of cell averages, under a condition achievable through a simple PP limiter. Several one- and two-dimensional MHD tests verify the accuracy, effectiveness, and robustness of the proposed structure-preserving OEDG schemes.",
        "citation_title": "Structure-Preserving Oscillation-Eliminating Discontinuous Galerkin Schemes for Ideal MHD Equations: Locally Divergence-Free and Positivity-Preserving",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Using data from the Neutron star Interior Composition ExploreR (NICER) observatory, we identified a permanent spin frequency decrease of $\\Delta\\nu=-(1.04\\pm0.07)\\times 10^{-7}\\,\\mathrm{Hz}$ around MJD 60132 in the rotation-powered pulsar PSR B0540-69, which exhibits a periodic signal at a frequency of $\\nu\\sim 19.6\\,\\mathrm{Hz}$. This points to an anti-glitch event, a sudden decrease of the pulsar's rotational frequency without any major alteration in the pulse profile or any significant increase of the pulsed flux. Additionally, no burst activity was observed in association with the anti-glitch. To date, observations of the few known anti-glitches have been made in magnetars or accreting pulsars. This is the first anti-glitch detected in a rotation-powered pulsar. Given its radiatively quiet nature, this anti-glitch is possibly of internal origin. Therefore, we tentatively frame this event within a proposed mechanism for anti-glitches where the partial `evaporation' of the superfluid component leads to an increase of the normal component's moment of inertia and a decrease of the superfluid one.",
        "citation_title": "Discovery of the first anti-glitch event in the rotation-powered pulsar PSR B0540-69",
        "date_delivered": "[Submitted on 28 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The radio galaxy M87 is well known for its jet, which features a series of bright knots observable from radio to X-ray wavelengths. We analyze the X-ray image and flux variability of the knot HST-1 in the jet. Our analysis includes all 112 available Chandra ACIS-S observations from 2000-2021, with a total exposure time of $\\sim$887 ks. We use de-convolved images to study the brightness profile of the X-ray jet and measure the relative separation between the core and HST-1. From 2003-2005 (which coincides with a bright flare from HST-1), we find a correlation between the flux of HST-1 and its offset from the core. In subsequent data, we find a steady increase in this offset, which implies a bulk superluminal motion for HST-1 of 6.6$\\pm$0.9 c (2.0$\\pm$0.3 pc yr$^{-1}$), in keeping with prior results. We discuss models for the flux-offset correlation that feature either two or four emission regions separated by tens of parsecs. We attribute these results to moving shocks in the jet, that allow us to measure the internal structure of the jet.",
        "citation_title": "Chandra Study of the Proper Motion of HST-1 in the Jet of M87",
        "date_delivered": "[Submitted on 30 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Galactic outflows have a multiphase nature making them challenging to model analytically. Many previous studies have tried to produce models that come closer to reality. In this work, we continue these efforts and describe the interaction of the hot wind fluid with multiple cold cloud populations, with their number density determined by different probability density functions. To do so, we introduced realistic cloud-wind interaction source terms and a time-varying cooling area. We find that the model reproduces well results from small-scale hydrodynamic simulations, but exhibits a general destructive behavior both for a single cloud population as well as multiple ones. We show that including multiple cloud populations can alter the evolution of the wind drastically. We also compare our model to observations and show that the differential acceleration of multiple clouds can lead to a non-negligible velocity `dispersion' relevant for down-the-barrel studies. Furthermore, we compute the emitted cooling surface brightness and find it generally too faint to explain observed Lyman-$\\alpha$ halos.",
        "citation_title": "Strength in numbers: A multiphase wind model with multiple cloud populations",
        "date_delivered": "[Submitted on 30 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Gauge-field configurations with non-trivial topology have profound consequences for the physics of Abelian and non-Abelian gauge theories. Over time, arguments have been gathering for the existence of gauge-field configurations with fractional topological charge, called fractons. Ground-state properties of gauge theories can drastically change in presence of fractons in the path integral. However, understanding the origin of such fractons is usually restricted to semi-classical argumentation. Here, we show that fractons persist in strongly correlated many-body systems, using the multiflavor Schwinger model of quantum electrodynamics as a paradigm example. Through detailed numerical tensor-network analysis, we find strong fracton signatures even in highly discretized lattice models, at sizes that are implementable on already existing quantum-simulation devices. Our work sheds light on how the non-trivial topology of gauge theories persists in challenging non-perturbative regimes, and it shows a path forward to probing it in table-top experiments.",
        "citation_title": "Non-perturbative signatures of fractons in the twisted multi-flavor Schwinger Model",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Second-order topological insulators can be characterized by their bulk polarization, which is believed to be intrinsically connected to the center of the Wannier function. In this study, we demonstrate the existence of second-order topological insulators that feature a pair of partially degenerate photonic bands. These arise from the nonsymmorphic glide symmetry in an all-dielectric photonic crystal. The center of the maximally localized Wannier function (MLWF) is consistently located at the origin but is not equivalent with respect to the sum of constituent polarizations. As a result, topological corner modes can be identified by the distinctly hybridized MLWFs that truncate at the sample boundary. Through full-wave numerical simulations paired with microwave experiments, the second-order topology is clearly confirmed and characterized. These topological corner states exhibit notably unique modal symmetries, which are made possible by the inversion of the Wannier bands. Our results provide an alternative approach to explore higher-order topological physics with significant potential for applications in integrated and quantum photonics.",
        "citation_title": "Topological Corner Modes by Composite Wannier States in Glide-Symmetric Photonic Crystal",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Using molecular simulation and continuum dielectric theory, we consider how electrochemical kinetics are modulated as a function of twist angle in bilayer graphene electrodes. By establishing an effective connection between twist angle and the screening length of charge carriers within the electrode, we investigate how tunable metallicity can result in modified statistics of the electron transfer energy gap. Constant potential molecular simulations show that the activation free energy for electron transfer is an increasing function of the screening length, or decreasing function the density of states at the Fermi energy in the electrode, and subsequently a non-monotonic function of twist angle. We find the twist angle alters the density of states, which tunes the number of thermally-accessible channels for electron transfer, as well as the reorganization energy by altering the stability of the vertically excited state through attenuated image charge interactions. Understanding these effects allows us to cast the Marcus rate of interfacial electron transfer as a function of twist angle, in a manner consistent with a growing body of experimental observations.",
        "citation_title": "Microscopic origin of twist-dependent electron transfer rate in bilayer graphene",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "From the quasisymmetry-group perspective [Phys. Rev. Lett. 126, 120604 (2021)], we show the universal existence of collective, coherent modes of excitations with small momenta in many-body scar models in the degenerate limit, where the energy spacing in the scar tower vanishes. The number of these modes, as well as the quantum numbers carried by them, are given, not by the symmetry of the Hamiltonian, but by the quasisymmetry of the scar tower: hence the name quasi-Goldstone modes. Based on this, we draw a concrete analogy between the paradigm of spontaneous symmetry breaking and the many-body scar physics in the degenerate limit.",
        "citation_title": "Quasi-Nambu-Goldstone modes in many-body scar models",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We demonstrate the dramatic effect of non-uniform, discrete electric polarizability in high-T$_{C}$ superconductors on the spatial fluctuations of the short to medium range Coulomb interactions through a real-space semiclassical model. Although this is a general property, we concentrate on the cuprates as parent compounds, in which the charge carriers are primarily concentrated on the O sublattice. The anisotropic effective Cu-O bond polarization caused by charge transfer energy modulation and the O$^{2-}$ atomic polarizability together generate a non-monotonic screened hole-hole Coulomb interaction at short distances that displays a local minimum at the in-plane second nearest neighbor O-O distance solely along the Cu-O bond direction. This is in accordance with the pseudogap phase anisotropy and the short coherence length observed in many high-T$_C$ superconductors.",
        "citation_title": "Non-uniform and anisotropic electric polarizability resulting in pronounced local repulsion minima in high-temperature superconductors",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We introduce an algorithm to uncover the activated particle rearrangements, or excitations, regulating structural relaxation in glasses at much higher energies than previously achieved. We use it to investigate the density and geometric properties of excitations in a model system. We find that the density of excitations behaves as a shifted power-law, and confirm that this shift accounts for the increase in the activation energy controlling the relaxation dynamics. Remarkably, we find that excitations comprise a core whose properties, including the displacement of the particle moving the most, scale as a power-law of their activation energy and do not depend on temperature. Excitations also present an outer deformation field that depends on the material stability and, hence, on temperature. Our analysis suggests that while excitations suppress the transition of dynamical arrest predicted by mean-field theories, they are strongly influenced by it.",
        "citation_title": "Density and geometry of excitations in supercooled liquids up to the activation energy",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Microscopic colloidal particles are often used as probes to study the non-equilibrium activity of living matter or other complex systems. In many of these contexts hydrodynamic interactions between the probe particle and the system of interest play an important role. However little is known about what effect such interactions could have on the overall non-equilibrium characteristics of the system of interest. In this paper, we study two simple models experimentally and theoretically, which demonstrate that hydrodynamic interactions could either diminish or enhance the total entropy production of the combined system. Importantly, we show that our method of calculating entropy production helps identify heat flows consistently, even in the presence of hydrodynamic interactions. The results indicate that interactions can be finely tuned to optimize not only dynamic properties but also irreversibility and energy dissipation, thereby opening new avenues for tailored control and design of driven mesoscale systems.",
        "citation_title": "Irreversibility of mesoscopic processes with hydrodynamic interactions",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Optically active spin defects in wide band-gap semiconductors serve as a local sensor of multiple degrees of freedom in a variety of \"hard\" and \"soft\" condensed matter systems. Taking advantage of the recent progress on quantum sensing using van der Waals (vdW) quantum materials, here we report direct measurements of spin waves excited in magnetic insulator Y3Fe5O12 (YIG) by boron vacancy $V_B^-$ spin defects contained in few-layer thick hexagonal boron nitride nanoflakes. We show that the ferromagnetic resonance and parametric spin excitations can be effectively detected by $V_B^-$ spin defects under various experimental conditions through optically detected magnetic resonance measurements. The off-resonant dipole interaction between YIG magnons and $V_B^-$ spin defects is mediated by multi-magnon scattering processes, which may find relevant applications in a range of emerging quantum sensing, computing, and metrology technologies. Our results also highlight the opportunities offered by quantum spin defects in layered two-dimensional vdW materials for investigating local spin dynamic behaviors in magnetic solid-state matters.",
        "citation_title": "Sensing Spin Wave Excitations by Spin Defects in Few-Layer Thick Hexagonal Boron Nitride",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Random defects do not constitute the unique source of electron localization in two dimensions. Lattice quasidisorder generated from two inplane superimposed rotated, main and secondary, square lattices, namely monolayers where moir\u00e9 patterns are formed, leads to a sharp localized to delocalized single-particle transition. This is demostrated here for both, discrete and continuum models of moir\u00e9 patterns that arise as the twisting angle $\\theta$ between main and secondary lattices is varied in the interval $[0, \\pi/4]$. Localized to delocalized transition is recognized as the moir\u00e9 patterns depart from being perfect square crystals to non-crystalline structures. Extended single-particle states were found for rotation angles associated with Pythagorean triples that produce perfectly periodic structures. Conversely, angles not arising from such Pythagorean triples lead to non-commensurate or quasidisordered structures, thus originating localized states. These conclusions are drawn from a stationary analysis where the standard IPR parameter measuring localization allowed us to detect the transition. While both, ground state and excited states were analyzed for the discrete model, where the secondary lattice was considered as a perturbation of the main one, the sharp transition was tracked back for the fundamental state in the continuous scenario where the secondary lattice is not a perturbation any more.",
        "citation_title": "Localized and extended phases in square moir\u00e9 patterns",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantification of chaos is a challenging issue in complex dynamical systems. In this paper, we discuss the chaotic properties of generalized Lotka-Volterra and May-Leonard models of biodiversity, via the Hamming distance density. We identified chaotic behavior for different scenarios via the specific features of the Hamming distance and the method of q-exponential fitting. We also investigated the spatial autocorrelation length to find the corresponding characteristic length in terms of the number of species in each system. In particular, the results concerning the characteristic length are in good accordance with the study of the chaotic behavior implemented in this work.",
        "citation_title": "Chaotic behavior in Lotka-Volterra and May-Leonard models of biodiversity",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Coherent optical driving in quantum solids is emerging as a new research frontier, with many demonstrations of exotic non-equilibrium quantum phases. These are based on engineered band structures, and on stimulated nonlinear interactions between driven modes. Enhanced functionalities like ferroelectricity, magnetism and superconductivity have been reported in these non-equilibrium settings. In high-Tc cuprates, coherent driving of certain phonon modes induces a transient state with superconducting-like optical properties, observed far above T$_c$ and throughout the pseudogap phase. Questions remain not only on the microscopic nature of this phenomenon, but also on the macroscopic properties of these transient states, beyond the documented optical conductivities. Crucially, it is not clear if driven cuprates exhibit Meissner-like diamagnetism. Here, the time-dependent magnetic-field amplitude surrounding a driven YBa$_2$Cu$_3$O$_{6.48}$ sample is probed by measuring Faraday rotation in a GaP layer adjacent to the superconductor. For the same driving conditions that result in superconducting-like optical properties, an enhancement of magnetic field at the edge of the sample is detected, indicative of induced diamagnetism. The dynamical field expulsion measured after pumping is comparable in size to the one expected in an equilibrium type II superconductor of similar shape and size with a volume susceptibility $\\chi_v$ of order -0.3. Crucially, this value is incompatible with a photo-induced increase in mobility without superconductivity. Rather, it underscores the notion of a pseudogap phase in which incipient superconducting correlations are enhanced or synchronized by the optical drive.",
        "citation_title": "Magnetic field expulsion in optically driven YBa$_2$Cu$_3$O$_{6.48}$",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "A spin-wave theory that includes the antisymmetric Dzyaloshinskii-Moriya exchange interactions and long-range dipole-dipole interactions is presented for finite-length ferromagnetic spin chains. It is found that three different physical situations arise, depending on the direction chosen in this geometry for the axial vector of the Dzyaloshinskii-Moriya interactions. In some cases this leads to a tilting of the equilibrium orientations near the ends of the chain due to interfacial effects and with consequential effects on the spectrum of discrete dipole-exchange spin waves. When variations are introduced for the dominant bilinear exchange interactions at the ends of the spin chain, it is shown that localized spin waves with spatial decay characteristics may occur.",
        "citation_title": "Effects of Dzyaloshinskii-Moriya interactions and dipole-dipole interactions on spin waves in finite-length ferromagnetic chains",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "A python code (this http URL) is presented for computing the mean-value point (MVP) in the Brillouin zone first introduced by Baldereschi [1]. The code allows calculations of the MVP for any input crystal structure. Having MVP allows approximating the Brillouin zone integrals of relatively smooth, periodic functions defined in the reciprocal space by the value of the same function at only one, mean-value, k-point. This approximation decreases computational cost at a relatively small decrease in accuracy. The MVP coordinates for the 14 Bravais lattices are evaluated and the underlying theory is discussed.",
        "citation_title": "A Python code for calculating the mean-value (Baldereschi's) point for any crystal structure",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The interplay of topological band structures and electronic correlations may lead to novel exotic quantum phenomena with potential applications. First-principles calculations are critical for guiding the experimental discoveries and interpretations, but often fail if electronic correlations cannot be properly treated. Here we show that this issue occurs also in the antiferromagnetic kagome lattice Mn$_3X$ ($X=$ Sn, Ge), which exhibit a large anomalous Hall effect due to topological band structures with Weyl nodes near the Fermi energy. Our systematic investigations reveal a crucial role of the Hund's rule coupling on three key aspects of their magnetic, electronic, and topological properties: (1) the establishment of noncollinear antiferromagnetic orders, (2) the weakly renormalized bands in excellent agreement with ARPES, and (3) a sensitive tuning of the Weyl nodes beyond previous expectations. Our work provides a basis for understanding the topological properties of Mn$_3X$ and challenges previous experimental interpretations based on incorrect band structures.",
        "citation_title": "Hundness and band renormalization in the kagome antiferromagnets Mn$_3X$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Method(s) that can reliably predict phase evolution across thermodynamic parameter space, especially in complex systems are of critical significance in academia as well as in the manufacturing industry. In the present work, phase stability in equimolar AlCuFeMn multi-principal-component alloy (MPCA) was predicted using complementary first-principles density functional theory (DFT) calculations, and ab-initio molecular dynamics (AIMD) simulations. Temperature evolution of completely disordered, partially ordered, and completely ordered phases was examined based on Gibbs free energy. Configurational, electronic, vibrational, and lattice mismatch entropies were considered to compute the Gibbs free energy of the competing phases. Additionally, elemental segregation was studied using ab-initio molecular dynamics (AIMD). The predicted results at 300K align well with room-temperature experimental observations using x-ray diffraction, scanning and transmission electron microscopy on a sample prepared using commercially available pure elements. The adopted method could help in predicting plausible phases in other MPCA systems with complex phase stability.",
        "citation_title": "Understanding the phase stability in multi-principal-component AlCuFeMn alloy",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the present work, interfacial magnetism at metal organic interface is probed using an isotope sensitive interface resolved nuclear resonance scattering technique which is made depth selective under x-rays standing wave conditions. Using GIWAXS and GINRS measurements, this study evidences the presence of symmetry-based PMA which appears at a lower thickness of Fe having distortion in cubic symmetry and disappears at a higher thickness of Fe as its cubic symmetry retains. The non-zero value of quadrupole splitting evidences the strain at the interfacial region which on increasing thickness of Fe relaxes. The diffusion of Fe is traced using XRF and NRR, deep penetration of Fe in Alq3 layer due to soft nature of the organic film is obtained. This thickness-dependent study enables us to understand the magnetic behavior of buried ferromagnetic metal in the vicinity of organic molecules.",
        "citation_title": "Evolution of Interface Magnetism in Fe/Alq3 Bilayer Structure; Thickness-Dependent Interface Resolved Studies Under X-Ray Standing Wave",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The stability of perovskite solar cells is closely related to the defects in perovskite crystals, and there are a large number of crystal defects in the perovskite thin films prepared by the solution method, which is not conducive to the commercial production of PSCs. In this study, resveratrol(RES), a green natural antioxidant abundant in knotweed and grape leaves, was introduced into perovskite films to passivate the defect. RES achieves defect passivation by interacting with uncoordinated Pb2+ in perovskite films. The results show that the quality of the perovskite film is significantly improved, and the energy level structure of the device is optimized, and the power conversion efficiency of the device is increased from 21.62% to 23.44%. In addition, RES can hinder the degradation of perovskite structures by O2- and CO2- free radicals, and the device retained 88% of its initial PCE after over 1000 hours in pure oxygen environment. The device retains 91% of the initial PCE after more than 1000 hours at 25\u00b0C and 50+5% relative humidity. This work provides a strategy for the use of natural and environmentally friendly additives to improve the efficiency and stability of devices, and provides an idea for the development of efficient, stable and environmentally friendly PSCs.",
        "citation_title": "An eco-friendly passivation strategy of resveratrol for highly efficient and antioxidative perovskite solar cells",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Intercalation of epitaxial graphene on SiC(0001) with Sn results in a well-ordered 2D metallic Sn phase with a $(1\\times1)$ structure at the interface between SiC substrate and quasi-freestanding graphene. The 2D\\,Sn phase exhibits exotic electronic properties with Dirac-like and flat bands coexisting close to the Fermi level that exhibit both Zeeman- and Rashba-type spin splittings. Possible inter-layer interactions between the 2D\\,Sn layer and graphene that may result in emerging electronic properties remain unexplored. We use time- and angle-resolved photoemission spectroscopy to reveal a surprisingly short-lived non-equilibrium carrier distribution inside the Dirac cone of graphene. Further, we find that the graphene $\\pi$-band exhibits a transient down-shift that we attribute to charging of the graphene layer with holes. We interpret our results with support from density functional theory calculations of the graphene - 2D\\,Sn heterostructure that reveal a substantial hybridization between graphene $\\pi$-band and Sn $p_z$-states that opens up a $\\sim230$\\,meV band gap inside the Dirac cone and delocalizes the charge carriers over both the graphene and 2D\\,Sn layers. Our results have important implications for the design of future ultrafast optoelectronic devices that may find applications in the fields of light harvesting and detection, as supercapacitors, or in novel quantum computing technologies.",
        "citation_title": "Non-equilibrium carrier dynamics and band structure of graphene on 2D tin",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We implanted Fe$^+$ ions in nanodiamond (ND) powder containing negatively charged nitrogen-vacancy (NV-) centers and studied their Raman spectra and optically detected magnetic resonance (ODMR) in various applied magnetic fields with green light (532 nm) excitation. In Raman spectra, we observed a blue shift of the NV$^-$ peak associated with the conversion of the electronic sp$^3$ configuration to the disordered sp$^2$ one typical for the carbon/graphite structure. In the ODMR spectra, we observed a red shift of the resonance position caused by local heating by an absorptive environment that recovers after annealing. To reveal the red shift mechanism in ODMR, we created a controlled absorptive environment around ND by adding iron-based Fe$_2$O$_3$ and graphitic sp$^2$ powders to the ND suspension. This admixture caused a substantial increase in the observed shift proportional to the applied laser power, corresponding to an increase in the local temperature by 150-180 K. This surprisingly large shift is absent in non-irradiated NV-ND powders, is associated only with the modification of the local temperature by the absorptive environment of NV-NDs and can be studied using ODMR signals of NV$^-$.",
        "citation_title": "Optically detected magnetic resonance study of thermal effects due to absorbing environment around nitrogen-vacancy-nanodiamond powders",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The low efficiency of conventional liquefaction technologies based on the Joule-Thomson expansion makes liquid hydrogen currently not attractive enough for large-scale energy-related technologies that are important for the transition to a carbon-neutral society. Magnetocaloric hydrogen liquefaction has great potential to achieve higher efficiency and is therefore a crucial enabler for affordable liquid hydrogen. Cost-effective magnetocaloric materials with large magnetic entropy and adiabatic temperature changes in the temperature range of 77 $\\sim$ 20 K under commercially practicable magnetic fields are the foundation for the success of magnetocaloric hydrogen liquefaction. Heavy rare-earth-based magnetocaloric intermetallic compounds generally show excellent magnetocaloric performances, but the heavy rare-earth elements (Gd, Tb, Dy, Ho, Er, and Tm) are highly critical in resources. Yttrium and light rare-earth elements (La, Ce, Pr, and Nd) are relatively abundant, but their alloys generally show less excellent magnetocaloric properties. A dilemma appears: higher performance or lower criticality? In this review, we study how cryogenic temperature influences magnetocaloric performance by first reviewing heavy rare-earth-based intermetallic compounds. Next, we look at light rare-earth-based, \"mixed\" rare-earth-based, and Gd-based intermetallic compounds with the nature of the phase transition order taken into consideration, and summarize ways to resolve the dilemma.",
        "citation_title": "A matter of performance & criticality: a review of rare-earth-based magnetocaloric intermetallic compounds for hydrogen liquefaction",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Mixtures with many components can segregate into coexisting phases, e.g., in biological cells and synthetic materials such as metallic glass. The interactions between components dictate what phases form in equilibrium, but quantifying this relationship has proven difficult. We derive scaling relations for the number of coexisting phases in multicomponent liquids with random interactions and compositions, which we verify numerically. Our results indicate that interactions only need to increase logarithmically with the number of components for the liquid to segregate into many phases. In contrast, a stability analysis of the homogeneous state predicts a power-law scaling. This discrepancy implies an enormous parameter regime where the number of coexisting phases exceeds the number of unstable modes, generalizing the nucleation and growth regime of binary mixtures to many components.",
        "citation_title": "Scaling of phase count in multicomponent liquids",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Mn-based nitrides with antiperovskite structures have several properties that can be utilised for antiferromagnetic spintronics. Their magnetic properties depend on the structural quality, composition and doping of the cubic antiperovskite structure. Such nitride thin films are usually produced by reactive physical vapour deposition, where the deposition rate of N can only be controlled by the N2 gas flow. We show that the tuning of the N content can be optimised using low temperature resistivity measurements, which serve as an indicator of the degree of structural disorder. Several Mn3GaNx films were prepared by reactive magnetron sputtering under different N2 gas flows. Under optimised conditions, we obtain films that exhibit a metal-like temperature dependence, a vanishing logarithmic increase in resistivity towards zero, the highest resistivity ratio and a lattice contraction of 0.4 % along the growth direction when heated above that of the N\u00e9el temperature in agreement with the bulk samples.",
        "citation_title": "Optimization of reactively sputtered Mn3GaN films based on resistivity measurements",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Tomography of single-particle-resolved detectors is of primary importance for characterizing particle correlations with applications in quantum metrology, quantum simulation and quantum computing. However, it is a non-trivial task in practice due to the unavoidable presence of noise that affects the measurement but does not originate from the detector. In this work, we address this problem for a three-dimensional single-atom-resolved detector where shot-to-shot atom number fluctuations are a central issue to perform a quantum detector tomography. We overcome this difficulty by exploiting the parallel measurement of counting statistics in sub-volumes of the detector, from which we evaluate the effect of shot-to-shot fluctuations and perform a local tomography of the detector. In addition, we illustrate the validity of our method from applying it to Gaussian quantum states with different number statistics. Finally, we show that the response of Micro-Channel Plate detectors is well-described from using a binomial distribution with the detection efficiency as a single parameter.",
        "citation_title": "Tomography of a single-atom-resolved detector in the presence of shot-to-shot number fluctuations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We demonstrate reliable machine-learned tuning of quantum-dot-based artificial Kitaev chains to Majorana sweet spots, using the covariance matrix adaptation algorithm. We show that a loss function based on local tunnelling-spectroscopy features of a chain with two additional sensor dots added at its ends provides a reliable metric to navigate parameter space and find points where crossed Andreev reflection and elastic cotunneling between neighbouring sites balance in such a way to yield near-zero-energy modes with very high Majorana quality. We simulate tuning of two- and three-site Kitaev chains, where the loss function is found from calculating the low-energy spectrum of a model Hamiltonian that includes Coulomb interactions and finite Zeeman splitting. In both cases, the algorithm consistently converges towards high-quality sweet spots. Since tunnelling spectroscopy provides one global metric for tuning all on-site potentials simultaneously, this presents a promising way towards tuning longer Kitaev chains, which are required for achieving topological protection of the Majorana modes.",
        "citation_title": "Machine-learned tuning of artificial Kitaev chains from tunneling-spectroscopy measurements",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In recent years, experimental and theoretical investigations have shown that anisotropic colloids can self-organise into ordered porous monolayers, where the interplay of localised bonding sites, so called patches, with the particle's shape is responsible for driving the systems away from close-packing and towards porosity. Until now it has been assumed that patchy particles have to be fully bonded with their neighbouring particles for crystals to form, and that, if full bonding cannot be achieved due to the choice of patch placement, disordered assemblies will form instead. In contrast, we show that by deliberately displacing the patches such that full bonding is disfavored, a different route to porous crystalline monolayers emerges, where geometric frustration and partial bonding are pivotal in the structure formation process. The resulting dangling bonds lead to the emergence of effectively chiral units which then act as building blocks for energetically equivalent crystal polymorphs.",
        "citation_title": "Partially bonded crystals: a pathway to porosity and polymorphism",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The promise of a strong magnetoelectric (ME) coupling in a multiferroic (MF) material is not only of fundamental interest, but also forms the basis of next generation memory devices where the direction of magnetization can be reversed by an external electric field. Using group-theory led first-principles calculations, we determine the ME properties of a relatively understudied family of layered oxides with the general formula $A_4B_3$O$_9$. We show how the tetrahedral rotations in these oxides can lead to a variety of hitherto unknown structural phases with different symmetries. In particular, a polar phase in the $Cmc2_1$ space group has been identified where a weak ferromagnetic mode arises spontaneously via a canting of the antiferromagnetically ordered $B$-site spins. In this polar phase, the polar mode couples to the magnetic modes through a rare $\\Gamma$-point ME-MF coupling scheme such that the net magnetization can be directly reversed by an electric field switching of the polar mode. Moreover, in agreement with previous experimental observations, we predict room-temperature magnetism in $A_4B_3$O$_9$ layered oxides which is supported by our calculations of the magnetic exchange interaction parameters, further indicating the potential of these compounds in practical technological applications.",
        "citation_title": "Prediction of Room-Temperature Electric Field Reversal of Magnetization in the Family of $A_4B_3$O$_9$ Layered Oxides",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Analyzing local structures effectively is key to unraveling the origin of many physical phenomena. Unsupervised algorithms offer an effective way of handling systems in which order parameters are unknown or computationally expensive. By combining novel unsupervised algorithm (Pairwise Controlled Manifold Approximation Projection) with atomistic potential descriptors, we distinguish between various chemical environments with minimal computational overhead. In particular, we apply this method to silicon and water systems. The algorithm effectively distinguishes between solid structures and phases of silicon, including solid and liquid phases, and accurately identifies interstitial, monovacancy, and surface atoms in diamond structures. In the case of water, it is capable of identifying an ice nucleus in the liquid phase, demonstrating its applicability in nucleation studies.",
        "citation_title": "Unsupervised identification of local atomic environment from atomistic potential descriptors",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Carrier multiplication (CM), where the absorption of a single photon results in the generation of several electron-hole pairs via impact ionization, plays a pivotal role in the quest for enhancing the performance of solar cells beyond the Shockley-Queisser limit. The combination of its narrow bandgap relative to the photon energy of visible light, along with its low phonon frequencies that hinder efficient energy dissipation into phonons, makes the topological insulator Bi$_2$Se$_3$ an optimal candidate material for efficient CM. Here we use time- and angle-resolved photoemission spectroscopy (trARPES) to trace the number of electron-hole pairs after photoexcitation of Bi$_2$Se$_3$ with visible pump pulses at $\\hbar\\omega=2$ eV. We find that both the number of electrons inside the conduction band as well as the number of holes inside the valence band keep increasing long after the pump pulse is gone, providing direct evidence for CM. We also analyze the transient band structure as well as the hot carrier dynamics inside the conduction band, providing a complete picture of the non-equilibrium carrier dynamics in photoexcited Bi$_2$Se$_3$ which can now serve as a basis for novel optoelectronic applications.",
        "citation_title": "Direct evidence for efficient carrier multiplication in the topological insulator Bi$_2$Se$_3$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Surface diffusion and surface electromigration may lead to a morphological instability of thin solid films and nanowires. In this paper two nonlinear analyzes of a morphological instability are developed for a single-crystal cylindrical nanowire that is subjected to the axial current. These treatments extend the conventional linear stability analyzes without surface electromigration, that manifest a Rayleigh-Plateau instability. A weakly nonlinear analysis is done slightly above the Rayleigh-Plateau (longwave) instability threshold. It results in a one-dimensional Sivashinsky amplitude equation that describes a blow-up of a surface perturbation amplitude in a finite time. This is a signature of a formation of an axisymmetric spike singularity of a cylinder radius, which leads to a wire pinch-off and separation into a disjoint segments. The scaling analysis of the amplitude spike singularity is performed, and the time-and-electric field-dependent dimensions of the spike are characterized. A weakly nonlinear multi-scale analysis is done at the arbitrary distance above a longwave or a shortwave instability threshold. The time-and-electric field-dependent Fourier amplitudes of the major instability modes are derived and characterized.",
        "citation_title": "On Nanowire Morphological Instability and Pinch-Off by Surface Electromigration",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "For nearly 90 years, precession and relaxation processes have been thought to dominate magnetization dynamics. Only recently has it been considered that, on short time scales, an inertia-driven magnetization dynamics should become relevant, leading to additional nutation of the magnetization vector. Here, we trigger magnetic nutation via a sudden excitation of a thin Ni80Fe20 (Permalloy) film with an ultrashort optical pulse, that leads to an abrupt tilting of the effective field acting on the magnetic moments, separating the dynamics of the magnetization from that of its angular momentum. We investigate the resulting magnetization dynamics in the inertial regime experimentally by the time-resolved magneto optical Kerr effect. We find a characteristic oscillation in the Kerr signal in the range of about 0.1 THz superimposed on the precessional oscillations with GHz frequencies. By comparison with atomistic spin dynamics simulations, we demonstrate that this observation cannot be explained by the well-known Landau-Lifshitz-Gilbert equation of motion but can be attributed to inertial contributions leading to nutation of the magnetization vector around its angular momentum. Hence, an optical and non-resonant excitation of inertial magnetization dynamics can trigger and control different magnetic processes, ranging from demagnetization via nutation to precession in a single device. These findings will have profound implications for the understanding of ultrafast spin dynamics and magnetization switching.",
        "citation_title": "Nutation: separating the spin from its magnetic moment",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "High harmonic generation (HHG) is a striking phenomenon, which reflects the ultrafast dynamics of electrons. Recently, it has been demonstrated that HHG can be used to reconstruct not only the energy band structure but also the geometric structure characterized by the Berry curvature. Here, we numerically investigate HHG arising from electrons coupled with a topological spin texture in a spin scalar chiral state where time reversal symmetry is broken. In this system, a sign change in scalar chirality alters the sign of the Berry curvature while keeping the energy band structure unchanged, allowing us to discuss purely geometrical effects on HHG. Notably, we found that, when the optical frequency is significantly lower than the energy gap, the sign of scalar chirality largely affects the longitudinal response parallel to the optical field rather than the transverse response. Our analysis suggests that this can be attributed to interband currents induced by the recombination of electron-hole pairs whose real-space trajectories are modulated by the anomalous velocity term.",
        "citation_title": "High harmonic generation from electrons moving on topological spin textures",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Complex systems with multiple processes evolving on different temporal scales are naturally described by multilayer networks, where each layer represents a different timescale. In this work, we show how the multilayer structure shapes the generation and propagation of information between layers. We derive a general decomposition of the multilayer probability for continuous stochastic processes described by Fokker-Planck operators. In particular, we focus on Gaussian processes, for which this solution can be obtained analytically. By explicitly computing the mutual information between the layers, we derive the fundamental principles that govern how information is propagated by the topology of the multilayer network. In particular, we unravel how edges between nodes in different layers affect their functional couplings. We find that interactions from fast to slow layers alone do not generate information, leaving the layers statistically independent even if they affect their dynamical evolution. On the other hand, interactions from slow to fast nodes lead to non-zero mutual information, which can then be propagated along specific paths of interactions between layers. We employ our results to study the interplay between information and instability, identifying the critical layers that drive information when pushed to the edge of stability. Our work generalizes previous results obtained in the context of discrete stochastic processes, allowing us to understand how the multilayer nature of complex systems affects their functional structure.",
        "citation_title": "Information propagation in Gaussian processes on multilayer networks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Understanding and controlling the electronic properties of two-dimensional materials is crucial for their potential applications in nano- and optoelectronics. Monolayer transition metal dichalcogenides such as WS$_2$ have garnered significant interest due to their strong light-matter interaction and extreme sensitivity of the band structure to the presence of photogenerated electron-hole pairs. In this study, we investigate the transient electronic structure of monolayer WS$_2$ on a graphene substrate after resonant excitation of the A-exciton using time- and angle-resolved photoemission spectroscopy. We observe a pronounced band structure renormalization including a substantial reduction of the transient band gap that is in good quantitative agreement with our {\\it ab initio} theory that reveals the importance of both intrinsic WS$_2$ and extrinsic substrate contributions to the transient band structure of monolayer WS$_2$. Our findings not only deepen the fundamental understanding of band structure dynamics in two-dimensional materials but also offer valuable insights for the development of novel electronic and optoelectronic devices based on monolayer TMDs and their heterostructures with graphene.",
        "citation_title": "k-resolved ultrafast light-induced band renormalization in monolayer WS$_2$ on graphene",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider construction of effective Hamiltonians for periodically driven interacting systems in the case of resonant driving. The standard high-frequency expansion is not expected to converge due to the resonant creation of collective excitations, and one option is to resort to the application of degenerate perturbation theory (DPT) in the extended Floquet-Hilbert space. We propose an extension of DPT whereby the degenerate subspace includes not only the degenerate levels of interest but rather all the states of the system. The resulting approach, which we call extended DPT (EDPT), is shown to resemble a high-frequency expansion, provided the quasienergy matrix is constructed such that each $m$th diagonal block contains energies reduced to the $m$th Floquet zone. The proposed theory is applied to a driven Bose-Hubbard model and is shown to yield more accurate quasienergy spectra than the conventional DPT. The computational complexity of EDPT is intermediate between DPT and the numerically exact approach, thus providing a practical compromise between accuracy and efficiency.",
        "citation_title": "On degenerate perturbation theory for the Floquet-Hilbert space",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose a spectroscopic probe of the breaking and localization of Cooper pairs in an atomic Fermi superfluid interacting with a Rydberg impurity. This is achieved by monitoring the formation of diatomic and triatomic ultralong-range molecular species in the superfluid across the BCS - Bose Einstein condensation (BEC) crossover. The triatomic Rydberg molecule in the BEC regime heralds the trapping of a tightly-bound Cooper pair, reminiscent of pion capture in nuclear matter, while the breaking of a Cooper pair on the BCS side by a diatomic Rydberg molecule is evocative of binary-star tidal disruption by a black hole. Spectroscopy of the Fermi superfluid and Rydberg molecules allows for an estimation of the Cooper-pair size while the Rydberg molecule binding energies discern many-body pairing effects.",
        "citation_title": "Breaking and trapping Cooper pairs by Rydberg-molecule spectroscopy in atomic Fermi superfluids",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Emergent magnetic phenomena at interfaces represent a frontier in materials science, pivotal for advancing technologies in spintronics and magnetic storage. In this letter, we utilize a suite of advanced X-ray spectroscopic and scattering techniques to investigate emergent interfacial ferromagnetism in oxide superlattices comprised of antiferromagnetic CaMnO3 and paramagnetic CaRuO3. Our findings challenge prior theoretical models by demonstrating that the ferromagnetism extends beyond the interfacial layer into multiple unit cells of CaMnO3 and exhibits an asymmetric profile. Complementary density functional calculations reveal that the interfacial ferromagnetism is driven by the double exchange mechanism, facilitated by charge transfer from Ru to Mn ions. Additionally, defect chemistry, particularly the presence of oxygen vacancies, likely plays a crucial role in modifying the magnetic moments at the interface, leading to the observed asymmetry between the top and bottom CaMnO3 interfacial magnetic layers. Our findings underscore the potential of manipulating interfacial ferromagnetism through point defect engineering.",
        "citation_title": "Depth-resolved profile of the interfacial ferromagnetism in $CaMnO_{3}/CaRuO_{3}$ superlattices",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Strain tuning is a powerful experimental method in probing correlated electron systems. Here we study the strain response of the lattice dynamics and electronic structure in semiconductor Ta$_2$NiS$_5$ by polarization-resolved Raman spectroscopy. We observe an increase of the size of the direct semiconducting band gap. Although the majority of the optical phonons show only marginal dependence to applied strain, the frequency of the two B$_{2g}$ phonon modes, which have quadrupolar symmetry and already anomalously soften on cooling under zero strain, increases significantly with tensile strain along the $a$ axis. The corresponding Gr\u00fcneisen parameters are unusually large in magnitude and negative in sign. These effects are well captured by first-principles density functional theory calculations and indicate close proximity of Ta$_2$NiS$_5$ to a structural instability, similar to that encountered in excitonic insulator candidate Ta$_2$NiSe$_5$.",
        "citation_title": "Anomalous phonon Gr\u00fcneisen parameters in semiconductor Ta$_2$NiS$_5$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Calculation of observables with three-dimensional projected entangled pair states is generally hard, as it requires a contraction of complex multi-layer tensor networks. We utilize the multi-layer structure of these tensor networks to largely simplify the contraction. The proposed approach involves the usage of the layer structure both to simplify the search for the boundary projected entangled pair states and the single-layer mapping of the final corner transfer matrix renormalization group contraction. We benchmark our results on the cubic lattice Heisenberg model, reaching the bond dimension D = 7, and find a good agreement with the previous results.",
        "citation_title": "Single-layer tensor network approach for three-dimensional quantum systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Lanthanides, like erbium and dysprosium, have emerged as powerful platforms for quantum-gas research due to their diverse properties, including a significant large spin manifold in their absolute ground state. However, effectively exploiting the spin richness necessitates precise manipulation of spin populations, a challenge yet to be fully addressed in this class of atomic species. In this work, we present an all-optical method for deterministically controlling the spin composition of a dipolar bosonic erbium gas, based on a clock-like transition in the telecom window at 1299 nm. The atoms can be prepared in just a few tens of microseconds in any spin-state composition using a sequence of Rabi-pulse pairs, selectively coupling Zeeman sublevels of the ground state with those of the long-lived clock-like state. Finally, we demonstrate that this transition can also be used to create spin-selective light shifts, thus fully suppressing spin-exchange collisions. These experimental results unlock exciting possibilities for implementing advanced spin models in isolated, clean and fully controllable lattice systems.",
        "citation_title": "Optical Manipulation of Spin States in Ultracold Magnetic Atoms via an Inner-Shell Hz Transition",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Odd viscosity is a property of chiral active fluids with broken time-reversal and parity symmetries. We show that the flow of such a fluid around a rotating axisymmetric body is exactly solvable and use this solution to determine the orientational dynamics of surface-driven microswimmers. Swimmers with a force-dipole moment exhibit precession around the axis of the odd viscosity. In addition, pushers show bimodal chirotaxis, i.e., alignment parallel or antiparallel to the axis, while pullers orbit in a plane perpendicular to it. A chiral swimmer that itself has a broken parity symmetry can exhibit unimodal chirotaxis and always align in the same direction.",
        "citation_title": "Chirotactic response of microswimmers in fluids with odd viscosity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Artificial Intelligence (AI) approaches are increasingly being applied to more and more domains of Science, Engineering, Chemistry, and Industries to not only improve efficiencies and enhance productivity, but also enable new capabilities. The new opportunities range from automated molecule design and screening, properties prediction, gaining insights of chemical reactions, to computer-aided design, predictive maintenance of systems, robotics, and autonomous vehicles. This review focuses on the new applications of AI in manufacturing and healthcare. For the Manufacturing Industries, we focus on AI and algorithms for (1) Battery, (2) Flow Chemistry, (3) Additive Manufacturing, (4) Sensors, and (5) Machine Vision. For Healthcare applications, we focus on: (1) Medical Vision (2) Diagnosis, (3) Protein Design, and (4) Drug Discovery. In the end, related topics are discussed, including physics integrated machine learning, model explainability, security, and governance during model deployment.",
        "citation_title": "AI for Manufacturing and Healthcare: a chemistry and engineering perspective",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We observe an inverse turbulent-wave cascade, from small to large lengthscales, in a homogeneous 2D Bose gas driven isotropically on a lengthscale much smaller than its size. Starting with an equilibrium condensed gas, at long drive times we observe a nonthermal steady state. At increasing lengthscales, starting from the forcing one, the steady-state momentum distribution features in turn: (i) a power-law spectrum, with an exponent close to the analytical result for a particle cascade in weak-wave turbulence, and (ii) a spectrum intriguingly reminiscent of a nonthermal fixed point associated with universal coarsening in an isolated 2D gas. In further experiments, based on anisotropic driving, we also reveal the qualitative picture of the cascade-formation dynamics.",
        "citation_title": "Observation of an inverse turbulent-wave cascade in a driven quantum gas",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Artificial neuronal devices are the basic building blocks for neuromorphic computing systems, which have been motivated by realistic brain emulation. Aiming for these applications, various device concepts have been proposed to mimic the neuronal dynamics and functions. While till now, the artificial neuron devices with high efficiency, high stability and low power consumption are still far from practical application. Due to the special insulator-metal phase transition, Vanadium Dioxide (VO2) has been considered as an idea candidate for neuronal device fabrication. However, its intrinsic insulating state requires the VO2 neuronal device to be driven under large bias voltage, resulting in high power consumption and low frequency. Thus in the current study, we have addressed this challenge by preparing oxygen vacancies modulated VO2 film(VO2-x) and fabricating the VO2-x neuronal devices for Spiking Neural Networks (SNNs) construction. Results indicate the neuron devices can be operated under lower voltage with improved processing speed. The proposed VO2-x based back-propagation SNNs (BP-SNNs) system, trained with the MNIST dataset, demonstrates excellent accuracy in image recognition. Our study not only demonstrates the VO2-x based neurons and SNN system for practical application, but also offers an effective way to optimize the future neuromorphic computing systems by defect engineering strategy.",
        "citation_title": "Oxygen vacancies modulated VO2 for neurons and Spiking Neural Network construction",
        "date_delivered": "[Submitted on 16 Apr 2024]"
    },
    {
        "abstract": "We develop a Schwinger-Keldysh field theory (SKFT) for open quantum systems interacting with a dissipative environment and apply it to the spin-boson model as an archetypical example where the environment is composed of a bosonic bath. Prior SKFT developments of this type have been confined to the Markovian regime, as an alternative to a conventional description by the Lindblad quantum master equation (QME) which is a time-local matrix differential equation. Here we combine SKFT with a two-particle irreducible (2PI) action that resums a class of Feynman diagrams to infinite order. We obtain the time-evolution of the spin density matrix in the form of a system of integro-differential equations applicable to both Markovian and non-Markovian regimes. The latter regime--where taking into account memory effects becomes essential--poses a challenge for standard methods when trying to incorporate arbitrary properties of the system, bath, and length of time evolution. The SKFT+2PI-computed time evolution of the spin expectation values in the Markovian regime reproduces the solution of the Lindblad QME, as long as the system-bath coupling in the latter is adjusted by increasing it. In the non-Markovian regime, SKFT+2PI yields a nonperturbative solution that mimics results from both hierarchical equations of motion and tensor networks methods that we employ as benchmarks. Our SKFT+2PI approach can also access challenging cases, such as zero-temperature and sub-Ohmic bath, as well as arbitrary long evolution times. Taking into account favorable numerical cost of solving the integro-differential equations with increasing number of spins, time steps or dimensionality the SKFT+2PI approach offers a promising route for simulation of driven-dissipative systems in quantum computing or quantum magnonics and spintronics in the presence of a variety of (single or multiple) dissipative environments.",
        "citation_title": "Schwinger-Keldysh nonequilibrium quantum field theory of open quantum systems beyond the Markovian regime: Application to the spin-boson model",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study thermalization slowing down of a quantum many-body system upon approach to two distinct integrability limits. Motivated by previous studies of classical systems, we identify two thermalization time scales: one quantum Lyapunov time scale is extracted by quantifying operator growth in time in an appropriately defined basis, while another ergodization time scale is related to statistics of fluctuations of the time-evolved operator around its mean value based on the eigenstate thermalization hypothesis. Using a paradigmatic Quantum Ising chain we find that both timescales diverge upon approach to integrability. The relative strength of the divergence of the scales depends on the particular integrable limit. This allows us to define two different universality classes of quantum thermalization: short- and long-range networks.",
        "citation_title": "Thermalization slowing down of weakly nonintegrable quantum spin dynamics",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantum Darwinism is a paradigm to understand how classically objective reality emerges from within a fundamentally quantum universe. Despite the growing attention that this field of research as been enjoying, it is currently not known what specific properties a given Hamiltonian describing a generic quantum system must have to allow the emergence of classicality. Therefore, in the present work, we consider a broadly applicable generic model of an arbitrary finite-dimensional system interacting with an environment formed from an arbitrary collection of finite-dimensional degrees of freedom via an unspecified, potentially time-dependent Hamiltonian containing at most two-body interaction terms. We show that such models support quantum Darwinism if the set of operators acting on the system which enter the Hamiltonian satisfy a set of commutation relations with a pointer observable and with one other. We demonstrate our results by analyzing a wide range of example systems: a qutrit interacting with a qubit environment, a qubit-qubit model with interactions alternating in time, and a series of collision models including a minimal model of a quantum Maxwell demon.",
        "citation_title": "Classifying two-body Hamiltonians for Quantum Darwinism",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Superconducting quantum processors are a compelling platform for analog quantum simulation due to the precision control, fast operation, and site-resolved readout inherent to the hardware. Arrays of coupled superconducting qubits natively emulate the dynamics of interacting particles according to the Bose-Hubbard model. However, many interesting condensed-matter phenomena emerge only in the presence of electromagnetic fields. Here, we emulate the dynamics of charged particles in an electromagnetic field using a superconducting quantum simulator. We realize a broadly adjustable synthetic magnetic vector potential by applying continuous modulation tones to all qubits. We verify that the synthetic vector potential obeys requisite properties of electromagnetism: a spatially-varying vector potential breaks time-reversal symmetry and generates a gauge-invariant synthetic magnetic field, and a temporally-varying vector potential produces a synthetic electric field. We demonstrate that the Hall effect--the transverse deflection of a charged particle propagating in an electromagnetic field--exists in the presence of the synthetic electromagnetic field.",
        "citation_title": "Implementing a synthetic magnetic vector potential in a 2D superconducting qubit array",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Recently, two-dimensional terahertz spectroscopy (2DTS) has attracted increasing attention for studying complex solids. A number of recent studies have applied 2DTS either with long pulses or away from any material resonances, situations that yield unconventional 2DTS spectra that are often difficult to interpret. Here, we clarify the generic origins of observed spectral features by examining 2DTS spectra of ZnTe, a model system with a featureless optical susceptibility at low terahertz frequencies. These results also reveal possible artifacts that may arise from electro-optic sampling in collinear 2DTS experiments, including the observation of spurious rectified or second harmonic signals.",
        "citation_title": "Excitation-Dependent Features and Artifacts in 2-D Terahertz Spectroscopy",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Realism about quantum theory naturally leads to realism about the quantum state of the universe. It leaves open whether it is a pure state represented by a wave function, or an impure one represented by a density matrix. I characterize and elaborate on Density Matrix Realism, the thesis that the universal quantum state is objective but can be impure. To clarify the thesis, I compare it with Wave Function Realism, explain the conditions under which they are empirically equivalent, consider two generalizations of Density Matrix Realism, and answer some frequently asked questions. I end by highlighting an implication for scientific realism.",
        "citation_title": "Density Matrix Realism",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The quantum dynamics of a dense and dipole-dipole coupled ensemble of two-level emitters interacting via their environmental thermostat is investigated. The static dipole-dipole interaction strengths are being considered strong enough but smaller than the transition frequency. Therefore, the established thermal equilibrium of ensemble's quantum dynamics is described with respect to the dipole-dipole coupling strengths. We have demonstrated the quantum nature of the spontaneously scattered light field in this process for weaker thermal baths as well as non-negligible dipole-dipole couplings compared to the emitter's transition frequency. Furthermore, the collectively emitted photon intensity suppresses or enhances depending on the environmental thermal baths intensities.",
        "citation_title": "Dense dipole-dipole-coupled two-level systems in a thermal bath",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Clustering methods must be tailored to the dataset it operates on, as there is no objective or universal definition of ``cluster,'' but nevertheless arbitrariness in the clustering method must be minimized. This paper develops a quantitative ``stability'' method of determining clusters, where stable or persistent clustering signals are used to indicate real structures have been identified in the underlying dataset. This method is based on modulating clustering methods by controlling a parameter -- through a thermodynamic analogy, the modulation parameter is considered ``time'' and the evolving clustering methodologies can be considered a ``heat flow.'' When the information entropy of the heat flow is stable over a wide range of times -- either globally or in the local sense which we define -- we interpret this stability as an indication that essential features of the data have been found, and create clusters on this basis.",
        "citation_title": "Stability of Information in the Heat Flow Clustering",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Using an exact holographic duality formula between the inhomogeneous 2d Ising model and 3d quantum gravity, we provide a formula for \"real\" zeroes of the 2d Ising partition function on finite graphs in terms of the geometry of a 2d triangulation embedded in the three-dimensional Euclidean space. The complex phase of those zeroes is given by the dihedral angles of the triangulation, which reflect its extrinsic curvature within the ambient 3d space, while the modulus is given by the angles within the 2d triangles, thus encoding the intrinsic geometry of the triangulation. Our formula can not cover the whole set of Ising zeroes, but we conjecture that a suitable complexification of these \"real\" zeroes would provide a more thorough formula. Nevertheless, in the thermodynamic limit, in the case of flat planar 2d triangulations, our Ising zeroes formula gives the critical couplings for isoradial graphs, confirming its generality. This approach shows an intricate, but precise, new relation between statistical mechanics and quantum geometry.",
        "citation_title": "2d Ising Critical Couplings from Quantum Gravity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the classical dynamics of a system of a pair of Kerr-Duffing nonlinear oscillators coupled by a nonlinear interaction and subject to a parametric drive. Within a rotating wave approximation (RWA), we analyze the steady-state solutions for the oscillation amplitude of the two oscillators. In the most relevant case of identical oscillators, we separately investigate configurations in which only one oscillator is parametrically driven, or both of them are simultaneously driven. In the latter regime, special attention is paid to the symmetric case where the parametric drives acting on the two oscillators are equal: for an increasing value of the detuning of the parametric drive, a transition to a multi-stable, symmetry-breaking regime is found, where the two oscillators display different oscillation amplitudes and phases.",
        "citation_title": "Nonlinearity-induced symmetry breaking in a system of two parametrically driven Kerr-Duffing oscillators",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Employing universal relations for the Onsager coefficients in the linear regime at the symmetric point of the single impurity Anderson model, we calculate the conditions under which the quantum scattering phase shift should satisfy to produce the asymptotic Carnot's limit for the thermoelectric efficiency. We show that a single quantum dot connected by metallic leads at the Kondo regime cannot achieve the conditions that cause the best thermoelectric efficiency. We study a system of serial double quantum dots without inter-dot correlations. We show that maintaining one dot in the electron-hole symmetric point makes it possible to obtain conditions for the quantum phase shift linked to charge fluctuations in the other quantum dot that satisfy the conditions associated with enhancing the thermoelectric efficiency. We also discuss the presence of bound states in the continuum (BICs) and quasi-BICs associated with the quantum scattering interference process that improves thermoelectric efficiency. We identify two types of quasi-BICs that occur at low and high temperatures: The first is associated with single Fano resonances, and the last is with several Fano processes. We also discussed possible temperature values and conditions that could be linked with the experimental realization of our results.",
        "citation_title": "Universality and the thermoelectric transport properties of a double quantum dot system: Seeking for conditions that improve the thermoelectric efficiency",
        "date_delivered": "[Submitted on 17 Feb 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The frictionless, directional propagation of particles at the boundary of topological materials is one of the most striking phenomena in transport. These chiral edge modes lie at the heart of the integer and fractional quantum Hall effects, and their extraordinary robustness against noise and disorder reflects the quantization of Hall conductivity in these systems. Despite their central importance, controllable injection of edge modes, and direct imaging of their propagation, structure, and dynamics, is challenging. Here, we demonstrate the distillation of individual chiral edge states in a rapidly-rotating bosonic superfluid confined by an optical boundary. Tuning the wall sharpness, we reveal the smooth crossover between soft wall behaviour in which the propagation speed is proportional to wall steepness, and the hard wall regime exhibiting chiral free particles. From the skipping motion of atoms along the boundary, we spectroscopically infer the energy gap between the ground and first excited edge bands, and reveal its evolution from the bulk Landau level splitting for a soft boundary, to the hard wall limit.",
        "citation_title": "Observation of chiral edge transport in a rapidly-rotating quantum gas",
        "date_delivered": "[Submitted on 20 Apr 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Fast measurements of quantum devices is important in areas such as quantum sensing, quantum computing and nanodevice quality analysis. Here, we develop a superconductor-semiconductor multi-module microwave assembly to demonstrate charge state readout at the state-of-the-art. The assembly consist of a superconducting readout resonator interfaced to a silicon-on-insulator (SOI) chiplet containing quantum dots (QDs) in a high-$\\kappa$ nanowire transistor. The superconducting chiplet contains resonant and coupling elements as well as $LC$ filters that, when interfaced with the silicon chip, result in a resonant frequency $f=2.12$ GHz, a loaded quality factor $Q=850$, and a resonator impedance $Z=470$ $\\Omega$. Combined with the large gate lever arms of SOI technology, we achieve a minimum integration time for single and double QD transitions of 2.77 ns and 13.5 ns, respectively. We utilize the assembly to measure charge noise over 9 decades of frequency up to 500 kHz and find a 1/$f$ dependence across the whole frequency spectrum as well as a charge noise level of 4 $\\mu$eV/$\\sqrt{\\text{Hz}}$ at 1 Hz. The modular microwave circuitry presented here can be directly utilized in conjunction with other quantum device to improve the readout performance as well as enable large bandwidth noise spectroscopy, all without the complexity of superconductor-semiconductor monolithic fabrication.",
        "citation_title": "Multi-module microwave assembly for fast read-out and charge noise characterization of silicon quantum dots",
        "date_delivered": "[Submitted on 26 Apr 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The anisotropic XY ferromagnet has been studied by Monte Carlo simulation in a three dimensional simple cubic lattice. The increase in critical temperature (ferro-para transition) has been noticed with increasing the strength of anisotropy. The effects of random fields (both with full circular symmetry and in angular window) on the critical temperature are investigated systematically in the anisotropic XY ferromagnet in three dimensions. Reduction in the critical temperature of anisotropic XY ferromagnet has been observed in the presence of random field. The compensating field (the required amount of field which preserves the critical temperature for isotropic XY ferromagnet) has been studied as a function of the strength of anisotropy. The compensating field was found to depend linearly on the strength of anisotropy. We have also studied the effects of random field confined in the angular window and observed the reduction of the critical temperature with increase of the angular extension. The critical behaviours are formalized by the usual finite size analysis and the estimation of critical exponents for the susceptibility and the specific heat. .",
        "citation_title": "Critical behaviours of anisotropic XY ferromagnet in the presence of random field",
        "date_delivered": "[Submitted on 3 Jun 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We consider a one-dimensional system of non-interacting fermions featuring both boundary driving and continuous monitoring of the bulk particle density. Due to the measurements, the expectation values of the local density and current operators are random variables whose average behavior is described by a well studied Lindblad master equation. By means of exact numerical computations, we go beyond the averaged dynamics and study their full probability distribution functions, focusing on the late-time stationary regime. We find that, contrary to the averaged values, the spatial profiles of the median density and current are non-trivial, exhibiting qualitative differences as a function of the monitoring strength. At weak monitoring, the medians are close to the means, displaying diffusive spatial profiles. At strong monitoring, we find that the median density and current develop a domain-wall and single-peak profile, respectively, which are suggestive of a Zeno-like localization in typical quantum trajectories. While we are not able to identify a sharp phase transition as a function of the monitoring rate, our work highlights the usefulness of characterizing typical behavior beyond the averaged values in the context of monitored many-body quantum dynamics.",
        "citation_title": "Density and current statistics in boundary-driven monitored fermionic chains",
        "date_delivered": "[Submitted on 16 Jun 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We propose a phase-biased non-Hermitian Josephson junction (NHJJ) composed of two superconductors mediated by a short non-Hermitian link. Such a NHJJ is described by an effective non-Hermitian Hamiltonian derived based on the Lindblad formalism in the weak coupling regime. By solving the Bogoliubov-de Gennes equation, we find that its Andreev spectrum as a function of phase difference exhibits Josephson gaps, i.e. finite phase windows with no Andreev (quasi-)bound states. The complex Andreev spectrum and the presence of Josephson gaps constitute particular spectral features of the NHJJ. Moreover, we propose complex supercurrents arising from inelastic Cooper pair tunneling to characterize the anomalous transport in the NHJJ. Additional numerical simulations complement our analytical predictions. We demonstrate that the Josephson effect is strongly affected by non-Hermitian physics.",
        "citation_title": "Anomalous Andreev Spectrum and Transport in Non-Hermitian Josephson Junctions",
        "date_delivered": "[Submitted on 10 Jul 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "The simultaneous occurrence of electric-field controlled superconductivity and spin-orbit interaction makes two-dimensional electron systems (2DES) constructed from perovskite transition metal oxides promising candidates for the next generation of spintronics and quantum computing. It is, however, essential to understand the electronic bands thoroughly and verify the predicted electronic states experimentally in these 2DES to advance technological applications. Here, we present novel insights into the electronic states of the 2DES at oxide interfaces through comprehensive investigations of Shubnikov-de Haas oscillations in two different systems: EuO/KTaO$_3$ (EuO/KTO) and LaAlO$_3$/SrTiO$_3$ (LAO/STO). To accurately resolve these oscillations, we conducted transport measurements in high magnetic fields up to 60 T and low temperatures down to 100 mK. For 2D confined electrons at both interfaces, we observed a progressive increase of oscillations frequency and cyclotron mass with the magnetic field. We interpret these intriguing findings by considering the existence of non-trivial electronic bands, for which the $E-k$ dispersion incorporates both linear and parabolic dispersion relations. In addition to providing experimental evidence for topological-like electronic states in KTO-2DES and STO-2DES, the unconventional oscillations presented in this study establish a new paradigm for quantum oscillations in 2DES based on perovskite transition metal oxides, where the oscillations frequency exhibits quadratic dependence on the magnetic field.",
        "citation_title": "Unconventional quantum oscillations and evidence of non-trivial electronic states in quasi-two-dimensional electron system at complex oxide interfaces",
        "date_delivered": "[Submitted on 10 Jul 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Understanding the intricate relationship between Fermi surface (FS) and electron transport in materials is crucial for advancing our knowledge of condensed matter physics. Commonly, FS properties are used to predict current within the framework of Fermi-liquid theory, where locally on the FS electron behavior can be estimated by nearly free election gas with slowly switched-on interactions. In this study, we go beyond the limits of this theory and explore how the geometry of the FS can result in strongly correlated Fermi-liquid or non-Fermi liquid electron behavior in metals and semi-metals and consequent anomalous transport effects: firstly, we establish that a semi-metallic FS with non-zero genus signals Fermi liquid breakdown, expanding our understanding of FS topology (Lemma 1). Secondly, we explicitly give dispersion relation in the form of a general polynomial of degree $2n, n \\ge 2$, which generates $n$ hole in the Fermi surface (Lemma 2). Additionally, we propose that hyperbolicity of the spinless FS leads to spin-correlated transport, offering insights into the implementation of the spin degree of freedom into the complexification of the reciprocal space(Proposition 1). These results deepen our understanding of FS geometry's impact on electronic behaviors, providing a theoretical foundation for the further development of a novel geometrized theoretical formalism of anomalous quantum transport within a semiclassical approach.",
        "citation_title": "Correlated electrons tunneling through pseudo Fermi arcs in hyperbolic Fermi surfaces of topological materials",
        "date_delivered": "[Submitted on 10 Aug 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Systems with conserved dipole moment have drawn considerable interest in light of their realization in recent experiments on tilted optical lattices. An important question for such systems is delineating the conditions under which they admit a unique gapped ground state that is consistent with all symmetries. Here, we study one-dimensional translation-invariant lattices that conserve U(1) charge and $\\mathbb{Z}_L$ dipole moment, where discreteness of the dipole symmetry is enforced by periodic boundary conditions, with $L$ the system size. We show that in these systems, a symmetric, gapped, and non-degenerate ground state requires not only integer charge filling, but also a fixed value of the dipole filling, while other fractional dipole fillings enforce either a gapless or symmetry-breaking ground state. In contrast with prior results in the literature, we find that the dipole filling constraint depends both on the charge filling as well as the system size, emphasizing the subtle interplay of dipole symmetry with boundary conditions. We support our results with numerical simulations and exact results.",
        "citation_title": "Filling constraints on translation invariant dipole conserving systems",
        "date_delivered": "[Submitted on 30 Aug 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Nontrivial bulk topological invariants of quantum materials can leave their signatures on charge, thermal and spin transports. In two dimensions, their imprints can be experimentally measured from well-developed multiterminal Hall bar arrangements. Here, we numerically compute the low temperature ($T$) thermal ($\\kappa_{xy}$) and zero temperature spin ($\\sigma^{sp}_{xy}$) Hall conductivities, and longitudinal thermal conductance ($G^{th}_{xx}$) of various prominent two-dimensional fully gapped topological superconductors, belonging to distinct Altland-Zirnbauer symmetry classes, namely $p+ip$ (class D), $d+id$ (class C) and $p \\pm ip$ (class DIII) paired states, in mesoscopic six-terminal Hall bar setups from the scattering matrix formalism using Kwant. In both clean and weak disorder limits, the time-reversal symmetry breaking $p+ip$ and $d+id$ pairings show half-quantized and quantized $\\kappa_{xy}$ [in units of $\\kappa_0=\\pi^2 k^2_B T/(3h)$], respectively, while the latter one in addition accommodates a quantized $\\sigma^{sp}_{xy}$ [in units of $\\sigma^{sp}_0=\\hbar/(8 \\pi)$]. By contrast, the time-reversal invariant $p \\pm ip$ pairing only displays a quantized $G^{th}_{xx}$ at low $T$ up to a moderate strength of disorder. In the strong disorder regime, all these topological responses ($\\kappa_{xy}$, $\\sigma^{sp}_{xy}$, and $G^{th}_{xx}$) vanish. Possible material platforms hosting such paired states and manifesting these robust topological thermal and spin responses are discussed.",
        "citation_title": "Quantized thermal and spin transports of dirty planar topological superconductors",
        "date_delivered": "[Submitted on 31 Aug 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We observe strongly anisotropic third-harmonic generation mediated by resonant sum-frequency driving of Raman phonons with THz light, extending light-induced dual control of structural and optical properties in solids. Either strong enhancement or strong suppression of the third harmonic covering six orders of magnitude can be achieved, a result of interference between purely electronic and phonon-mediated contributions to the polarization field. These findings enrich capabilities for tailoring nonlinear optics via phononics and for the spectroscopy of crystalline structural dynamics.",
        "citation_title": "Phonon-Mediated Third-Harmonic Generation in Diamond",
        "date_delivered": "[Submitted on 31 Aug 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Cell rearrangements are fundamental mechanisms driving large-scale deformations of living tissues. In three-dimensional (3D) space-filling cell aggregates, cells rearrange through local topological transitions of the network of cell-cell interfaces, which is most conveniently described by the vertex model. Since these transitions are not yet mathematically properly formulated, the 3D vertex model is generally difficult to implement. The few existing implementations rely on highly customized and complex software-engineering solutions, which cannot be transparently delineated and are thus mostly non-reproducible. To solve this outstanding problem, we propose a reformulation of the vertex model. Our approach, called Graph Vertex Model (GVM), is based on storing the topology of the cell network into a knowledge graph with a particular data structure that allows performing cell-rearrangement events by simple graph transformations. Importantly, when these same transformations are applied to a two-dimensional (2D) polygonal cell aggregate, they reduce to a well-known T1 transition, thereby generalizing cell-rearrangements in 2D and 3D space-filling packings. This result suggests that the GVM's graph data structure may be the most natural representation of cell aggregates and tissues. We also develop a Python package that implements GVM, relying on a graph-database-management framework Neo4j. We use this package to characterize an order-disorder transition in 3D cell aggregates, driven by active noise and we find aggregates undergoing efficient ordering close to the transition point. In all, our work showcases knowledge graphs as particularly suitable data models for structured storage, analysis, and manipulation of tissue data.",
        "citation_title": "Graph topological transformations in space-filling cell aggregates",
        "date_delivered": "[Submitted on 9 Sep 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We show that, simultaneous local scaling of coordinate and time keeping the velocity unaltered is a symmetry of an It\u00f4-process. Using this symmetry, any It\u00f4-process can be mapped to a universal additive Gaussian-noise form. We use this mapping to separate the canonical and micro-canonical part of stochastic dynamics of a Brownian particle undergoing coordinate dependent diffusion. We identify the equilibrium distribution of the system and associated entropy induced by coordinate dependence of diffusion. Equilibrium physics of such a Brownian particle in a heat-bath of constant temperature is that of an It\u00f4-process.",
        "citation_title": "Equilibrium with coordinate dependent diffusion: Comparison of different stochastic processes",
        "date_delivered": "[Submitted on 10 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Cavity-electromechanical systems are extensively used for sensing and controlling the vibrations of mechanical resonators down to their quantum limit. The nonlinear radiation-pressure interaction in these systems could result in an unstable response of the mechanical resonator showing features such as frequency-combs, period-doubling bifurcations and chaos. However, due to weak light-matter interaction, typically these effects appear at very high driving strengths. By using polariton modes formed by a strongly coupled flux-tunable transmon and a microwave cavity, here we demonstrate an electromechanical device and achieve a single-photon coupling rate $g_0/2\\pi$ of $160~$kHz, which is nearly 4\\% of the mechanical frequency $\\omega_m$. Due to large $g_0/\\omega_m$ ratio, the device shows an unstable mechanical response resulting in frequency combs in sub-single photon limit. We systematically investigate the boundary of the unstable response and identify two important regimes governed by the optomechanical backaction and the nonlinearity of the electromagnetic mode. Such an improvement in the single-photon coupling rate and the observations of microwave frequency combs at single-photon levels may have applications in the quantum control of the motional states and critical parametric sensing. Our experiments strongly suggest the requirement of newer approaches to understand instabilities.",
        "citation_title": "Single-photon induced instabilities in a cavity electromechanical device",
        "date_delivered": "[Submitted on 13 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "For a circuit made of thermodynamic devices in stationary nonequilibrium, we determine the mean currents (of energy, matter, charge, etc) exchanged with external reservoirs driving the circuit out of equilibrium. Starting from the conductance matrix describing the nonlinear current--force characteristics of each device, we obtain the conductance matrix of the composite device. This generalizes the rule of resistance addition (serial association) or conductance addition (parallel association) in stationary out-of-equilibrium thermodynamics and for multiple coupled potentials and currents of different natures. Our work emphasizes the pivotal role of conservation laws when creating circuits of complex devices. Finally, two examples illustrate the determination of the conservation laws for the serial and parallel associations of thermodynamic devices.",
        "citation_title": "Thermodynamic Circuits I: Association of devices in stationary nonequilibrium",
        "date_delivered": "[Submitted on 22 Sep 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We construct a doped holographic superconductor in the Gubser-Rocha model, and realize a superconducting dome in the middle of the temperature-doping phase diagram. It is worth noting that unlike the previous researches, the profile of our dome shrinks inward near zero temperature. From the numerical observation for the coupling dependence of the phase diagram, we find that the coupling between the two gauge fields plays a crucial role in the formation of dome. We also analytically calculate the DC conductivity of the normal phase of the system in the momentum dissipation and obtain the resistivity which is proportional to temperature. The AC conductivity is calculated numerically.",
        "citation_title": "Doped Holographic Superconductors in Gubser-Rocha model",
        "date_delivered": "[Submitted on 26 Sep 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We point out that Percus's collision integral for one-dimensional hard rods [J. K. Percus, Physics of Fluids 12, 1560-1563 (1969)] does not preserve the thermal equilibrium state in an external trapping potential. We derive a revised Enskog equation for hard rods and show that it preserves this thermal state exactly. In contrast to recent proposed kinetic equations for dynamics in integrability-breaking traps, both our kinetic equation and its thermal states are explicitly nonlocal in space. Our equation differs from earlier proposals at third order in spatial derivatives and we attribute this discrepancy to the choice of collision integral underlying our approach.",
        "citation_title": "Revised Enskog equation for hard rods",
        "date_delivered": "[Submitted on 27 Sep 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Nodal line semimetals feature topologically protected band crossings between the bulk valence and conduction bands that extend along a finite dimension in the form of a line or a loop. While ZrSiS and similar materials have attracted extensive research as hosts for the nodal line semimetallic phase, an alternative avenue has emerged in the form of isostructural rare-earth (RE) based RESbTe materials. Such systems possess intriguing potentialities for harboring elements of magnetic ordering and electronic correlations owing to the presence of 4f electrons intrinsic to the RE elements. In this study, we have carried out angle resolved photoemission spectroscopy (ARPES) and thermodynamic measurements in conjunction with first principles computations on PrSbTe to elucidate its electronic structure and topological characteristics. Magnetic and thermal characterizations indicate the presence of well-localized 4f states with the absence of any discernible phase transition down to 2 K. The ARPES results reveal the presence of gapless Dirac crossings that correspond to a nodal-line along the XR direction in the three-dimensional Brillouin zone. Furthermore, Dirac crossing that makes up nodal line, which forms a diamond-shaped nodal plane centered at the center of the Brillouin zone is also identified within the experimental resolution. This study on the electronic structure of PrSbTe contributes to the understanding of the pivotal role played by spin-orbit coupling in the context of the RESbTe family of materials",
        "citation_title": "Electronic structure in a rare-earth based nodal-line semimetal candidate PrSbTe",
        "date_delivered": "[Submitted on 3 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Langevin equations or generalized Langevin equations (GLEs) are popular models for describing the motion of a particle in a fluid medium in an effective manner. Here we examine particles immersed in an inherently nonequilibrium fluid, i.e., an active bath, which are subject to an external force. Specifically, we consider two types of forces that are highly relevant for microrheological studies: A harmonic, trapping force and a constant, \"drag\" force. We study such systems by molecular simulations and use the simulation data to derive an effective GLE description. We find that, in an active bath, the external force in the GLE is not equal to the physical external force, but rather a renormalized external force, which can be significantly smaller. The effect cannot be attributed to the mere temperature renormalization, which is also observed.",
        "citation_title": "Force renormalization for probes immersed in an active bath",
        "date_delivered": "[Submitted on 4 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We study Anderson localization in disordered tight-binding models on hyperbolic lattices. Such lattices are geometries intermediate between ordinary two-dimensional crystalline lattices, which localize at infinitesimal disorder, and Bethe lattices, which localize at strong disorder. Using state-of-the-art computational group theory methods to create large systems, we approximate the thermodynamic limit through appropriate periodic boundary conditions and numerically demonstrate the existence of an Anderson localization transition on the $\\{8,3\\}$ and $\\{8,8\\}$ lattices. We find unusually large critical disorder strengths, determine critical exponents, and observe a strong finite-size effect in the level statistics.",
        "citation_title": "Anderson localization transition in disordered hyperbolic lattices",
        "date_delivered": "[Submitted on 12 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Inertial effects should be considered for micro- and nano-swimmers moving in a low-density medium confined by irregular structures that create entropic barriers, where viscous effects are no longer paramount. Here, we present a separation mechanism of self-propelled particles in a two-dimensional asymmetric channel, which leads to the drift of particles of different masses in opposite directions. In particular, this mechanism is based on the combined action of the spatial asymmetry of the channel structure, the temporal asymmetry inherent in particles dynamics, and an external static force. This work is relevant for potential applications that can be found in the development of lab-on-a-chip devices and artificial channels for separating particles of different masses.",
        "citation_title": "Mass-based separation of active Brownian particles in an asymmetric channel",
        "date_delivered": "[Submitted on 24 Oct 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Building on recent simulation work, it is demonstrated using molecular-dynamics (MD) simulations of two-component fluid mixtures that the chemical contribution to the Soret effect in two-component non-ideal fluid mixtures arises due to differences in how the partial pressures of the components respond to temperature and density gradients. Further insight is obtained by reviewing the connection between activity and deviations from Raoult's law in the measurement of the vapor pressure of a liquid mixture. A new parameter $\\gamma_{s}^{S}$, defined in a manner similar to the activity coefficient, is used to characterize differences deviations from ``ideal'' behavior. It is then shown that the difference $\\gamma_{2}^{S}-\\gamma_{1}^{S}$ is predictive of the sign of the Soret coefficient and is correlated to its magnitude. We hence connect the Soret effect to the relative volatility of the components of a fluid mixture, with the more volatile component enriched in the low-density, high-temperature region, and the less volatile component enriched in the high-density, low-temperature region. Because $\\gamma_{s}^{S}$ is closely connected to the activity coefficient, this suggests the possibility that measurement of partial vapor pressures might be used to indirectly determine the Soret coefficient. It is proposed that the insight obtained here is quite general and should be applicable to a wide range of materials systems. An attempt is made to understand how these results might apply to other materials systems including interstitials in solids and multicomponent solids with interdiffusion occurring via a vacancy mechanism.",
        "citation_title": "Connection between partial pressure, volatility, and the Soret effect elucidated using simulations of non-ideal supercritical fluid mixtures",
        "date_delivered": "[Submitted on 30 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Recent progress towards universal machine-learned interatomic potentials holds considerable promise for materials discovery. Yet the accuracy of these potentials for predicting phase stability may still be limited. In contrast, cluster expansions provide accurate phase stability predictions but are computationally demanding to parameterize from first principles, especially for structures of low dimension or with a large number of components, such as interfaces or multimetal catalysts. We overcome this trade-off via transfer learning. Using Bayesian inference, we incorporate prior statistical knowledge from machine-learned and physics-based potentials, enabling us to sample the most informative configurations and to efficiently fit first-principles cluster expansions. This algorithm is tested on Pt:Ni, showing robust convergence of the mixing energies as a function of sample size with reduced statistical fluctuations.",
        "citation_title": "Cluster expansion by transfer learning for phase stability predictions",
        "date_delivered": "[Submitted on 10 Nov 2023 (v1), last revised 1 May 2024 (this version, v4)]"
    },
    {
        "abstract": "The Parisi formula for the free energy is among the crown jewels in the theory of spin glasses. We present a simpler proof of the lower bound in the case of the spherical mean-field model. Our method follows the TAP approach developed recently in e.g. (Subag, 2018): we obtain an ultrametric tree of pure states, each with approximately the same free energy as the entire model, which are hierarchically arranged in accordance with the Parisi ansatz. We construct this tree ``layer by layer'' given the minimizer to Parisi's variational problem. On overlap intervals with full RSB, the tree is built by an optimization algorithm due to Subag. On overlap intervals with finite RSB, the tree is constructed by a new truncated second moment argument; a similar argument also characterizes the free energy of the resulting pure states. Notably we do not use the Aizenman--Sims--Starr scheme, and require interpolation bounds only up to the 1RSB level. Our methods also yield results for large deviations of the ground state, including the entire upper tail rate function for all 1RSB models without external field.",
        "citation_title": "A Constructive Proof of the Spherical Parisi Formula",
        "date_delivered": "[Submitted on 27 Nov 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We classify connected \u00e9tale algebras $A$'s in pre-modular fusion categories $\\mathcal B$ with $\\text{rank}(\\mathcal B)\\le3$ including degenerate and non-(pseudo-)unitary ones. We comment on Lagrangian algebras and physical applications to ground state degeneracy and proof of spontaneous $\\mathcal B$-symmetry breaking.",
        "citation_title": "Classification of connected \u00e9tale algebras in pre-modular fusion categories up to rank three",
        "date_delivered": "[Submitted on 27 Nov 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We explore the pairing state and gap structure of UTe$_2$ using a six-orbital model which we call the $f$-$d$-$p$ model. Our model accurately reproduces the quasi-two-dimensional Fermi surfaces consistent with recent de Haas-van Alphen oscillation measurements and the $(0, \\pm \\pi, 0)$ antiferromagnetic spin fluctuations observed by neutron scattering. We incorporate on-site Coulomb repulsion for $f$ electrons and solve the linearized Eliashberg equation within the third-order perturbation theory to investigate the superconducting symmetry in UTe$_2$. The most likely state is found to be an $s$-wave state with a highly anisotropic superconducting gap structure that exhibits a point-node-like behavior of the specific heat at low temperatures.",
        "citation_title": "Unconventional $s$-Wave Pairing with Point-Node-Like Gap Structure in UTe$_2$",
        "date_delivered": "[Submitted on 18 Dec 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We classify connected \u00e9tale algebras in (possibly non-unitary) modular fusion categories $\\mathcal B$'s with $\\text{rank}(\\mathcal B)\\le5$. We also comment on Lagrangian algebra, anyon condensation, and physical applications. Concretely, we prove certain spontaneous $\\mathcal B$-symmetry breaking and predict ground state degeneracies in massive renormalization group flows from non-unitary minimal models.",
        "citation_title": "Classification of connected \u00e9tale algebras in modular fusion categories up to rank five",
        "date_delivered": "[Submitted on 20 Dec 2023 (v1), last revised 2 May 2024 (this version, v5)]"
    },
    {
        "abstract": "We investigate disordered-driven transitions between trivial and topological insulator (TI) phases in two-dimensional (2D) systems. Our study primarily focuses on the BHZ model with Anderson disorder, while other standard 2DTI models exhibit equivalent features. The analysis is based on the local Chern marker (LCM), a local quantity that allows for the characterization of topological transitions in finite and disordered systems. Our simulations indicate that disorder-driven trivial to topological insulator transitions are nicely characterized by $\\mathcal{C}_0$, the disorder averaged LCM near the central cell of the system. We show that $\\mathcal{C}_0$ is characterized by a single-parameter scaling, namely, $\\mathcal{C}_0(M, W, L) \\equiv \\mathcal{C}_0(z)$ with $z = [W^\\mu-W_c^\\mu(M)]L$, where $M$ is the Dirac mass, $W$ is the disorder strength and $L$ is the system size, while $W_c(M) \\propto \\sqrt{M}$ and $\\mu \\approx 2$ stand for the critical disorder strength and critical exponent, respectively. Our numerical results are in agreement with a theoretical prediction based on a first-order Born approximation (1BA) analysis. These observations lead us to speculate that the universal scaling function we have found is rather general for amorphous and disorder-driven topological phase transitions.",
        "citation_title": "Phase transitions and scale invariance in topological Anderson insulators",
        "date_delivered": "[Submitted on 5 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "These lecture notes, adapted from the habilitation thesis of the author, survey in a first part various exact results obtained in the past few decades about KPZ fluctuations in one dimension, with a special focus on finite volume effects describing the relaxation to its stationary state of a finite system starting from a given initial condition. The second part is more specifically devoted to an approach allowing to express in a simple way the statistics of the current in the totally asymmetric simple exclusion process in terms of a contour integral on a compact Riemann surface, whose infinite genus limit leads to KPZ fluctuations in finite volume.",
        "citation_title": "KPZ fluctuations in finite volume",
        "date_delivered": "[Submitted on 26 Jan 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The stable and metastable configurations of interstitial Mg in GaN and its migration energy barriers are studied from first-principles calculations. In addition to the conventional octahedral (O, global energy minimum) and tetrahedral (T, metastable) interstitial sites, we discover two new metastable interstitial complexes with formation energy lower than or close to that of T configuration but higher than O. Except for Mg at O site which only has +2 charge state, all other configurations also permit charge states +1 or 0. The minimum migration energy barrier for Mg$^{++}$ between O sites is found to be 1.95 eV. We further find that, when Fermi energy is close to the conduction band, the migration between O sites via metastable configurations occur through a recombination-enhanced mechanism in which the charge state changes from +2 at O site to 0 at metastable sites by consecutive capture of two electrons during the migration. This process greatly reduces the migration energy barrier to as low as 1.47 eV. This value is consistent with experiments, and we also discuss the role of intrinsic defects in the migration of Mg.",
        "citation_title": "First-Principles Study of Recombination-Enhanced Migration of an Interstitial Magnesium in Gallium Nitride",
        "date_delivered": "[Submitted on 9 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present an innovative cluster-based method employing linear combinations of diverse cluster mean-field (cMF) states, and apply it to describe the ground state of strongly-correlated spin systems. In cluster mean-field theory, the ground state wavefunction is expressed as a factorized tensor product of optimized cluster states. While our prior work concentrated on a single cMF tiling, this study removes that constraint by combining different tilings of cMF states. Selection criteria, including translational symmetry and spatial proximity, guide this process. We present benchmark calculations for the one- and two-dimensional $J_1-J_2$ and $XXZ$ Heisenberg models. Our findings highlight two key aspects. First, the method offers a semi-quantitative description of the $0.4 \\lessapprox J_2/J_1 \\lessapprox 0.6$ regime of the $J_1-J_2$ model - a particularly challenging regime for existing methods. Second, our results demonstrate the capability of our method to provide qualitative descriptions for all the models and regimes considered, establishing it as a valuable reference. However, the inclusion of additional (weak) correlations is necessary for quantitative agreement, and we explore methods to incorporate these extra correlations.",
        "citation_title": "Linear combinations of cluster mean-field states applied to spin systems",
        "date_delivered": "[Submitted on 9 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Low-temperature thermal fluctuations offer an essential window in characterizing the true nature of a quantum state of matter, a quintessential example being Fermi liquid theory. Here, we examine the leading thermal fluctuation of the superfluid density across numerous families ranging from relatively conventional to highly unconventional superconductors (MgB$_2$, bismuthates, doped buckyballs, heavy fermions, UTe$_2$, doped SrTiO$_3$, Chevrel clusters, intermetallics, organic superconductors, transition metal dichalcogenides, ruthenates, iron-pnictides, cuprates, and kagome metals). Amazingly, in all of them an unprecedented universal $T^3$ depletion materializes in the low-temperature superfluid density, even in the believed-to-be-conventional MgB$_2$. This reveals a new quantum superfluid state of matter and requires a necessary change of paradigm in describing modern superconductors. We demonstrate that such unorthodox yet generic behavior can be described by a proper particle-conserving theory of bosonic superfluidity hosting a long-lived `true condensate'.",
        "citation_title": "Universal low-temperature fluctuation of unconventional superconductors revealed: 'Smoking gun' leaves proper bosonic superfluidity the last theory standing",
        "date_delivered": "[Submitted on 13 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this work, we introduce Variational Umbrella Seeding, a novel technique for computing nucleation barriers. This new method, a refinement of the original seeding approach, is far less sensitive to the choice of order parameter for measuring the size of a nucleus. Consequently, it surpasses seeding in accuracy, and Umbrella Sampling in computational speed. We test the method extensively and demonstrate excellent accuracy for crystal nucleation of nearly hard spheres and of two distinct models of water: mW and TIP4P/ICE. This method can easily be extended to calculate nucleation barriers for homogeneous melting, condensation, and cavitation.",
        "citation_title": "Variational Umbrella Seeding for Calculating Nucleation Barriers",
        "date_delivered": "[Submitted on 21 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present a thorough experimental investigation on single crystals of the rare-earth based frustrated quantum antiferromagnet Pr$_3$BWO$_9$, a purported spin-liquid candidate on the breathing kagome lattice. This material possesses a disordered ground state with an unusual excitation spectrum involving a coexistence of sharp spin-waves and broad continuum excitations. Nevertheless, we show through a combination of thermodynamic, magnetometric and spectroscopic probes with detailed theoretical modeling that it should be understood in a completely different framework. The crystal field splits the lowest quasi-doublet states into two singlets moderately coupled through frustrated superexchange, resulting in a simple effective Hamiltonian of an Ising model in a transverse magnetic field. While our neutron spectroscopy data do point to significant correlations within the kagome planes, the dominant interactions are out-of-plane, forming frustrated triangular spin-tubes through two competing ferro-antiferromagnetic bonds. The resulting ground state is a simple quantum paramagnet, but with significant modifications to both thermodynamic and dynamic properties due to small perturbations to the transverse field Ising model in the form of hyperfine enhanced nuclear moments and weak structural disorder.",
        "citation_title": "Excitation Spectrum and Spin Hamiltonian of the Frustrated Quantum Ising Magnet Pr$_3$BWO$_9$",
        "date_delivered": "[Submitted on 21 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We investigate classes of interacting quantum spin systems in a single-mode cavity with a Dicke coupling, as a paradigmatic example of strongly correlated light-matter systems. Coming from the limit of weak light-matter couplings and large number of matter entities, we map the relevant low-energy sector of a broad class of models in the non-superradiant phases onto the exactly solvable Dicke model. We apply the outcomes to the Dicke-Ising model as a paradigmatic example, in agreement with results obtained by mean-field theory. We further accompany and verify our findings with finite-size calculations, using exact diagonalization and the series expansion method pcst++.",
        "citation_title": "(Almost) Everything is a Dicke model -- Mapping non-superradiant correlated light-matter systems to the exactly solvable Dicke model",
        "date_delivered": "[Submitted on 23 Feb 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The multi-reference coupled-cluster Monte Carlo (MR-CCMC) algorithm is a determinant-based quantum Monte Carlo (QMC) algorithm that is conceptually similar to Full Configuration Interaction QMC (FCIQMC). It has been shown to offer a balanced treatment of both static and dynamic correlation while retaining polynomial scaling, although application to large systems with significant strong correlation remained impractical. In this paper, we document recent algorithmic advances that enable rapid convergence and a more black-box approach to the multi-reference problem. These include a logarithmically scaling metric-tree based excitation acceptance algorithm to search for determinants connected to the reference space at the desired excitation level and a symmetry-screening procedure for the reference space. We show that, for moderately sized reference spaces, the new search algorithm brings about an approximately 8-fold acceleration of one MR-CCMC iteration, while the symmetry screening procedure reduces the number of active reference space determinants at essentially no loss of accuracy. We also introduce a stochastic implementation of an approximate wall projector, which is the infinite imaginary time limit of the exponential projector, using a truncated expansion of the wall function in Chebyshev polynomials. Notably, this wall-Chebyshev projector can be used to accelerate any projector-based QMC algorithm. We show that it requires significantly fewer applications of the Hamiltonian to achieve the same statistical convergence. We benchmark these acceleration methods on the beryllium and carbon dimers, using initiator FCIQMC and MR-CCMC with basis sets up to cc-pVQZ quality.",
        "citation_title": "Rapidly convergent quantum Monte Carlo using a Chebyshev projector",
        "date_delivered": "[Submitted on 26 Feb 2024 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Moir\u00e9 superlattices based on two-dimensional transition metal dichalcogenides (TMDs) have emerged as a highly versatile and fruitful platform for exploring correlated topological electronic phases. One of the most remarkable examples is the recently discovered fractional quantum anomalous Hall effect (FQAHE) under zero magnetic field. Here we propose a minimal structure that hosts long-lived excitons -- a ubiquitous bosonic excitation in TMD semiconductors -- with narrow topological bosonic bands. The nontrivial exciton topology originates from hybridization of moir\u00e9 interlayer excitons, and is tunable by controlling twist angle and electric field. At small twist angle, the lowest exciton bands are isolated from higher energy bands, and provides a solid-state realization of bosonic Kane-Mele model with topological flatbands, which could potentially support the bosonic version of FQAHE.",
        "citation_title": "Long-lived Topological Flatband Excitons in Semiconductor Moir\u00e9 Heterostructures: a Bosonic Kane-Mele Model Platform",
        "date_delivered": "[Submitted on 29 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Thermal Hall effect (THE) in insulator is a remarkable phenomenon that arises from the motion of chargeless quasi-particles under a magnetic field. While magnons or exotic spin excitations were considered as the origin of THE in some magnetic materials, there are more and more evidences suggesting that phonons play a significant role. However, the mechanism behind phonon THE is still unknown. Here we report the observation of THE, including planar THE, in a broad range of non-magnetic insulators and semiconductors: SrTiO3, SiO2 (quartz), MgO, MgAl2O4, Si and Ge. While the presence of antiferrodistortive domains in SrTiO3 and chiral phonons in SiO2 may complicate the interpretation of THE, the striking observations of THE in trivial insulators MgO and MgAl2O4, as well as in high-purity intrinsic semiconductors Si and Ge, demonstrate that phonon THE is a universal property of crystals. Without other effects on phonons such as from magnons, this universal phonon THE is characterized by a scaling law of |\\k{appa}_xy| ~ \\k{appa}_xx^2. Our results experimentally discover a fundamental physics of phonons in magnetic field, which should come from the direct coupling between atom vibrations and the field. Starting from this universal phonon THE in crystals, all previous interpretations of THE in magnetic or non-magnetic materials need to be reconsidered.",
        "citation_title": "Discovery of universal phonon thermal Hall effect in crystals",
        "date_delivered": "[Submitted on 3 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "CdTe and its alloy CdTeSe are widely used in optoelectronic devices, such as radiation detectors and solar cells, due to their superior electrical properties. However, the formation of defects and defect complexes in these materials can significantly affect their performance. As a result, understanding the defect formation and recombination processes in CdTe and CdTeSe alloy is of great importance. In recent years, density functional theory (DFT) calculations have emerged as a powerful tool for investigating the properties of defects in semiconductors. In this paper, we use DFT+U calculations to comprehensively study the properties of intrinsic defects as well as extrinsic defects induced by commonly used dopants, such as Cu and group V elements, in CdTe and CdTeSe alloy. This work provides insights into the effects of these defects on the electrical and optical properties of the material.",
        "citation_title": "Point defects in CdTe and CdTeSe alloy: a first principles investigation with DFT+U",
        "date_delivered": "[Submitted on 11 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Understanding what makes high-dimensional data learnable is a fundamental question in machine learning. On the one hand, it is believed that the success of deep learning lies in its ability to build a hierarchy of representations that become increasingly more abstract with depth, going from simple features like edges to more complex concepts. On the other hand, learning to be insensitive to invariances of the task, such as smooth transformations for image datasets, has been argued to be important for deep networks and it strongly correlates with their performance. In this work, we aim to explain this correlation and unify these two viewpoints. We show that by introducing sparsity to generative hierarchical models of data, the task acquires insensitivity to spatial transformations that are discrete versions of smooth transformations. In particular, we introduce the Sparse Random Hierarchy Model (SRHM), where we observe and rationalize that a hierarchical representation mirroring the hierarchical model is learnt precisely when such insensitivity is learnt, thereby explaining the strong correlation between the latter and performance. Moreover, we quantify how the sample complexity of CNNs learning the SRHM depends on both the sparsity and hierarchical structure of the task.",
        "citation_title": "How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model",
        "date_delivered": "[Submitted on 16 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "A (2+1)D topological ordered phase with U(1) symmetry may or may not have a symmetric gapped edge state, even if both thermal and electric Hall conductivity are vanishing. It is recently discovered that there are \"higher\" versions of Hall conductivity valid for fermionic fractional quantum Hall (FQH) states, which obstructs symmetry-preserving gapped edge state beyond thermal and electric Hall conductivity. In this paper, we show that one can extract higher Hall conductivity from a single wave function of an FQH state, by evaluating the expectation value of the \"partial rotation\" unitary which is a combination of partial spatial rotation and a U(1) phase rotation. This result is verified numerically with the fermionic Laughlin state with $\\nu=1/3$, $1/5$, as well as the non-Abelian Moore-Read state. Together with topological entanglement entropy, we prove that the expectation values of the partial rotation completely determines if a bosonic/fermionic Abelian topological order with U(1) symmetry has a symmetry-preserving gappable edge state or not. We also show that thermal and electric Hall conductivity of Abelian topological order can be extracted by partial rotations. Even in non-Abelian FQH states, partial rotation provides the Lieb-Schultz-Mattis type theorem constraining the low-energy spectrum of the bulk-boundary system. The generalization of higher Hall conductivity to the case with Lie group symmetry is also presented.",
        "citation_title": "Higher Hall conductivity from a single wave function: Obstructions to symmetry-preserving gapped edge of (2+1)D topological order",
        "date_delivered": "[Submitted on 16 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Empirical A-site cation substitution has advanced the stability and efficiency of hybrid organic-inorganic lead halide perovskites solar cells and the functionality of X-ray detectors. Yet, the fundamental mechanisms underpinning their unique performance remain elusive. This multi-modal study unveils the link between nanoscale structural dynamics and macroscopic optoelectronic properties in these materials by utilising X-ray diffuse scattering, inelastic neutron spectroscopy and optical microscopy complemented by state-of-the-art machine learning-assisted molecular dynamics simulations. Our approach uncovers the presence of dynamic, lower-symmetry local nanodomains embedded within the higher-symmetry average phase in various perovskite compositions. The properties of these nanodomains are tunable via the A-site cation selection: methylammonium induces a high density of anisotropic, planar nanodomains of out-of-phase octahedral tilts, while formamidinium favours sparsely distributed isotropic, spherical nanodomains with in-phase tilting, even when crystallography reveals cubic symmetry on average. The observed variations in the properties of dynamic nanodomains are in agreement with our simulations and are directly linked to the differing macroscopic optoelectronic and ferroelastic behaviours of these compositions. By demonstrating the influence of A-site cation on local nanodomains and consequently, on macroscopic properties, we propose leveraging this relationship to engineer the optoelectronic response of these materials, propelling further advancements in perovskite-based photovoltaics, optoelectronics, and X-ray imaging.",
        "citation_title": "Dynamic Nanodomains Dictate Macroscopic Properties in Lead Halide Perovskites",
        "date_delivered": "[Submitted on 22 Apr 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The versatility and wide-ranging applicability of the Ising model, originally introduced to study phase transitions in magnetic materials, have made it a cornerstone in statistical physics and a valuable tool for evaluating the performance of emerging computer hardware. Here, we present a novel implementation of the two-dimensional Ising model on a Cerebras Wafer-Scale Engine (WSE), a revolutionary processor that is opening new frontiers in computing. In our deployment of the checkerboard algorithm, we optimized the Ising model to take advantage of the unique WSE architecture. Specifically, we employed a compressed bit representation storing 16 spins on each int16 word, and efficiently distributed the spins over the processing units enabling seamless weak scaling and limiting communications to only immediate neighboring units. Our implementation can handle up to 754 simulations in parallel, achieving an aggregate of over 61.8 trillion flip attempts per second for Ising models with up to 200 million spins. This represents a gain of up to 148 times over previously reported single-device with a highly optimized implementation on NVIDIA V100 and up to 88 times in productivity compared to NVIDIA H100. Our findings highlight the significant potential of the WSE in scientific computing, particularly in the field of materials modeling.",
        "citation_title": "Record Acceleration of the Two-Dimensional Ising Model Using High-Performance Wafer Scale Engine",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We developed a general framework for hybrid quantum-classical computing of molecular and periodic embedding approaches based on an orbital space separation of the fragment and environment degrees of freedom. We demonstrate its potential by presenting a specific implementation of periodic range-separated DFT coupled to a quantum circuit ansatz, whereby the variational quantum eigensolver and the quantum equation-of-motion algorithm are used to obtain the low-lying spectrum of the embedded fragment Hamiltonian. Application of this scheme to study localized electronic states in materials is showcased through the accurate prediction of the optical properties of the neutral oxygen vacancy in magnesium oxide (MgO). Despite some discrepancies in the position of the main absorption band, the method demonstrates competitive performance compared to state-of-the-art ab initio approaches, particularly evidenced by the excellent agreement with the experimental photoluminescence emission peak.",
        "citation_title": "A general framework for active space embedding methods: applications in quantum computing",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Chiral magnets are materials which possess unique helical arrangements of magnetic moments, which give rise to nonreciprocal transport and fascinating physics phenomena. On the one hand, their exploration is guided by the prospects of unconventional signal processing, computation schemes and magnetic memory. On the other hand, progress in applications is hindered by the challenging materials synthesis, limited scalability and typically low critical temperature. Here, we report the creation and exploration of artificial chiral magnets (ACMs) at room temperature. By employing a mass production compatible deposition technology, we synthesize ACMs, which consist of helical Ni surfaces on central cylinders. Using optical microscopy, we reveal nonreciprocal magnon transport at GHz frequencies. It is controlled by programmable toroidal moments which result from the ACM's geometrical handedness and field-dependent spin chirality. We present materials-by-design rules which optimize the helically curved ferromagnets for 3D nonreciprocal transport at room temperature and zero magnetic field.",
        "citation_title": "Room temperature realization of artificial chiral magnets with reprogrammable magnon nonreciprocity at zero field",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.",
        "citation_title": "KAN: Kolmogorov-Arnold Networks",
        "date_delivered": "[Submitted on 30 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Mixed spin-(1/2,1/2,1) trimmer with two different Land\u00e9 g-factors and two different exchange couplings is considered. The main feature of the model is non-conserving magnetization. The Hamiltonian of the system is diagonalized analytically. We presented a detailed analysis of the ground state properties, revealing several possible ground state phase diagrams and magnetization profiles. The main focus is on how non-conserving magnetization affects quantum entanglement. We have found that non-conserving magnetization can bring to the continuous dependence of the entanglement quantifying parameter (negativity) on magnetic field within the same eigenstate, while for the case of uniform $g$-factors it is a constant. The main result is an essential enhancement of the entanglement in case of uniform couplings for one pair of spins caused by an arbitrary small difference in the values of $g$-factors. This enhancement is robust and brings to almost 7-fold increasing of the negativity. We have also found weakening of entanglement for other cases. Thus, non-conserving magnetization offers a broad opportunity to manipulate the entanglement by means of magnetic field.",
        "citation_title": "Quantum Entanglement In Mixed-Spin Trimmer: Effects of A Magnetic Field And Heterogeneous g-Factors",
        "date_delivered": "[Submitted on 30 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Stretching an elastic material along one axis typically induces contraction along the transverse axes, a phenomenon known as the Poisson effect. From these strains, one can compute the specific volume, which generally either increases or, in the incompressible limit, remains constant as the material is stretched. However, in networks of semiflexible or stiff polymers, which are typically highly compressible yet stiffen significantly when stretched, one instead sees a significant reduction in specific volume under finite strains. This volume reduction is accompanied by increasing alignment of filaments along the strain axis and a nonlinear elastic response, with stiffening of the apparent Young's modulus. For semiflexible networks, in which entropic bending elasticity governs the linear elastic regime, the nonlinear Poisson effect is caused by the nonlinear force-extension relationship of the constituent filaments, which produces a highly asymmetric response of the constituent polymers to stretching and compression. The details of this relationship depend on the geometric and elastic properties of the underlying filaments, which can vary greatly in experimental systems. Here, we provide a comprehensive characterization of the nonlinear Poisson effect in an affine network model and explore the influence of filament properties on essential features of the macroscopic response, including strain-driven alignment and volume reduction.",
        "citation_title": "Nonlinear Poisson effect in affine semiflexible polymer networks",
        "date_delivered": "[Submitted on 1 May 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Capillary forces guide the motion of biomolecular condensates, water-borne insects, and breakfast cereal. These surface-mediated interactions can be harnessed to build units into materials with exotic properties deriving from mesoscale structure. Droplets are promising building blocks for these materials, finding applications in tissue engineering, adaptive optics, and structural colour. However, the instability of water droplets at many liquid-liquid interfaces hampers the use of capillarity for the assembly of droplet-based materials. Here, we use nanoparticle surfactants to form solid-like oil-water interfaces at which aqueous droplets sit for extended periods. We find that microlitre-sized droplets at these interfaces attract each other over millimetric scales. We rationalize this interaction with a modified theory of capillarity. Applying printing methods allows us to finely control initial droplet positions, from which they self-assemble into cellular materials. Finally, by functionalising the interface with gold nanoparticles, we use plasmon-assisted optofluidics to manipulate these droplet-based materials with temperature gradients.",
        "citation_title": "Capillary-Assisted Printing of Droplets at a Solid-Like Liquid-Liquid Interface",
        "date_delivered": "[Submitted on 1 May 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Backpropagation-optimized artificial neural networks, while precise, lack robustness, leading to unforeseen behaviors that affect their safety. Biological neural systems do solve some of these issues already. Thus, understanding the biological mechanisms of robustness is an important step towards building trustworthy and safe systems. Unlike artificial models, biological neurons adjust connectivity based on neighboring cell activity. Robustness in neural representations is hypothesized to correlate with the smoothness of the encoding manifold. Recent work suggests power law covariance spectra, which were observed studying the primary visual cortex of mice, to be indicative of a balanced trade-off between accuracy and robustness in representations. Here, we show that unsupervised local learning models with winner takes all dynamics learn such power law representations, providing upcoming studies a mechanistic model with that characteristic. Our research aims to understand the interplay between geometry, spectral properties, robustness, and expressivity in neural representations. Hence, we study the link between representation smoothness and spectrum by using weight, Jacobian and spectral regularization while assessing performance and adversarial robustness. Our work serves as a foundation for future research into the mechanisms underlying power law spectra and optimally smooth encodings in both biological and artificial systems. The insights gained may elucidate the mechanisms that realize robust neural networks in mammalian brains and inform the development of more stable and reliable artificial systems.",
        "citation_title": "Exploring mechanisms of Neural Robustness: probing the bridge between geometry and spectrum",
        "date_delivered": "[Submitted on 5 Feb 2024]"
    },
    {
        "abstract": "Power systems are very large and complex, it can be influenced by many unexpected events this makes power system optimization problems difficult to solve, hence methods for solving these problems ought to be an active research topic. This review presents an overview of important mathematical comparaison of loss minimization algorithm and particle swarm optimization algorithm in terms of the performances of electric distribution.",
        "citation_title": "Comparative approach: Electric distribution optimization with loss minimization algorithm and particle swarm optimization",
        "date_delivered": "[Submitted on 19 Feb 2024]"
    },
    {
        "abstract": "The visual sensing system is one of the most important parts of the welding robots to realize intelligent and autonomous welding. The active visual sensing methods have been widely adopted in robotic welding because of their higher accuracies compared to the passive visual sensing methods. In this paper, we give a comprehensive review of the active visual sensing methods for robotic welding. According to their uses, we divide the state-of-the-art active visual sensing methods into four categories: seam tracking, weld bead defect detection, 3D weld pool geometry measurement and welding path planning. Firstly, we review the principles of these active visual sensing methods. Then, we give a tutorial of the 3D calibration methods for the active visual sensing systems used in intelligent welding robots to fill the gaps in the related fields. At last, we compare the reviewed active visual sensing methods and give the prospects based on their advantages and disadvantages.",
        "citation_title": "The active visual sensing methods for robotic welding: review, tutorial and prospect",
        "date_delivered": "[Submitted on 6 Mar 2024]"
    },
    {
        "abstract": "This report presents the test results Python library BaumEvA, which implements evolutionary algorithms for optimizing various types of problems, including computer vision tasks accompanied by the search for optimal model architectures. Testing was carried out to evaluate the effectiveness and reliability of the pro-posed methods, as well as to determine their applicability in various fields. Dur-ing testing, various test functions and parameters of evolutionary algorithms were used, which made it possible to evaluate their performance in a wide range of conditions. Test results showed that the library provides effective and reliable methods for solving optimization problems. However, some limitations were identified related to computational resources and execution time of algorithms on problems with large dimensions. The report includes a detailed description of the tests performed, the results obtained and conclusions about the applicability of the genetic algorithm in various tasks. Recommendations for choosing algorithm pa-rameters and using the library to achieve the best results are also provided. The report may be useful to developers involved in the optimization of complex com-puting systems, as well as to researchers studying the possibilities of using evo-lutionary algorithms in various fields of science and technology.",
        "citation_title": "Technical Report on BaumEvA Evolutionary Optimization Python-Library Testing",
        "date_delivered": "[Submitted on 6 Mar 2024]"
    },
    {
        "abstract": "This paper addresses the challenge of planning a sequence of tasks to be performed by multiple robots while minimizing the overall completion time subject to timing and precedence constraints. Our approach uses the Timed Partial Orders (TPO) model to specify these constraints. We translate this problem into a Traveling Salesman Problem (TSP) variant with timing and precedent constraints, and we solve it as a Mixed Integer Linear Programming (MILP) problem. Our contributions include a general planning framework for TPO specifications, a MILP formulation accommodating time windows and precedent constraints, its extension to multi-robot scenarios, and a method to quantify plan robustness. We demonstrate our framework on several case studies, including an aircraft turnaround task involving three Jackal robots, highlighting the approach's potential applicability to important real-world problems. Our benchmark results show that our MILP method outperforms state-of-the-art open-source TSP solvers OR-Tools.",
        "citation_title": "Optimal Planning for Timed Partial Order Specifications",
        "date_delivered": "[Submitted on 8 Mar 2024]"
    },
    {
        "abstract": "This article presents a comprehensive sentiment analysis (SA) of comments on YouTube videos related to Sidewalk Delivery Robots (SDRs). We manually annotated the collected YouTube comments with three sentiment labels: negative (0), positive (1), and neutral (2). We then constructed models for text sentiment classification and tested the models' performance on both binary and ternary classification tasks in terms of accuracy, precision, recall, and F1 score. Our results indicate that, in binary classification tasks, the Support Vector Machine (SVM) model using Term Frequency-Inverse Document Frequency (TF-IDF) and N-gram get the highest accuracy. In ternary classification tasks, the model using Bidirectional Encoder Representations from Transformers (BERT), Long Short-Term Memory Networks (LSTM) and Gated Recurrent Unit (GRU) significantly outperforms other machine learning models, achieving an accuracy, precision, recall, and F1 score of 0.78. Additionally, we employ the Latent Dirichlet Allocation model to generate 10 topics from the comments to explore the public's underlying views on SDRs. Drawing from these findings, we propose targeted recommendations for shaping future policies concerning SDRs. This work provides valuable insights for stakeholders in the SDR sector regarding social perception, interaction, and safety.",
        "citation_title": "Understanding Social Perception, Interactions, and Safety Aspects of Sidewalk Delivery Robots Using Sentiment Analysis",
        "date_delivered": "[Submitted on 9 Mar 2024]"
    },
    {
        "abstract": "This paper addresses the increasing significance of UAVs (Unmanned Aerial Vehicles) and the emergence of UAV swarms for collaborative operations in various domains. However, the effectiveness of UAV swarms can be severely compromised by jamming technology, necessitating robust antijamming strategies. While existing methods such as frequency hopping and physical path planning have been explored, there remains a gap in research on path planning for UAV swarms when the jammer's location is unknown. To address this, a novel approach, where UAV swarms leverage collective intelligence to predict jamming areas, evade them, and efficiently reach target destinations, is proposed. This approach utilizes Graph Convolutional Networks (GCN) to predict the location and intensity of jamming areas based on information gathered from each UAV. A multi-agent control algorithm is then employed to disperse the UAV swarm, avoid jamming, and regroup upon reaching the target. Through simulations, the effectiveness of the proposed method is demonstrated, showcasing accurate prediction of jamming areas and successful evasion through obstacle avoidance algorithms, ultimately achieving the mission objective. Proposed method offers robustness, scalability, and computational efficiency, making it applicable across various scenarios where UAV swarms operate in potentially hostile environments.",
        "citation_title": "Anti-Jamming Path Planning Using GCN for Multi-UAV",
        "date_delivered": "[Submitted on 13 Mar 2024]"
    },
    {
        "abstract": "One critical bottleneck that impedes the development and deployment of autonomous transportation in open-pit mines is guaranteed robustness and trustworthiness in prohibitively extreme scenarios. In this research, a novel scenarios engineering (SE) methodology for the autonomous mining truck is proposed for open-pit mines. SE increases the trustworthiness and robustness of autonomous trucks from four key components: Scenario Feature Extractor, Intelligence & Index (I&I), Calibration & Certification (C&C), and Verification & Validation (V&V). Scenario feature extractor is a comprehensive pipeline approach that captures complex interactions and latent dependencies in complex mining scenarios. I&I effectively enhances the quality of the training dataset, thereby establishing a solid foundation for autonomous transportation in mining areas. C&C is grounded in the intrinsic regulation, capabilities, and contributions of the intelligent systems employed in autonomous transportation to align with traffic participants in the real world and ensure their performance through certification. V&V process ensures that the autonomous transportation system can be correctly implemented, while validation focuses on evaluating the ability of the well-trained model to operate efficiently in the complex and dynamic conditions of the open-pit mines. This methodology addresses the unique challenges of autonomous transportation in open-pit mining, promoting productivity, safety, and performance in mining operations.",
        "citation_title": "Scenarios Engineering driven Autonomous Transportation in Open-Pit Mines",
        "date_delivered": "[Submitted on 15 Mar 2024]"
    },
    {
        "abstract": "Due to the limited driving range, inadequate charging facilities, and time-consuming recharging, the process of finding an optimal charging route for electric vehicles (EVs) differs from that of other vehicle types. The time and location of EV charging during a trip impact not only the individual EV's travel time but also the travel time of other EVs, due to the queuing that may arise at the charging station(s). This issue is at large seen as a significant constraint for uplifting EV sales in many countries. In this study, we present a novel Electric Vehicle Route Planning problem, which involves finding the fastest route with recharging for an EV routing request. We model the problem as a new graph problem and present that the problem is NP-hard. We propose a novel two-phase algorithm to traverse the graph to find the best possible charging route for each EV. We also introduce the notion of `influence factor' to propose heuristics to find the best possible route for an EV with the minimum travel time that avoids using charging stations and time to recharge at those stations which can lead to better travel time for other EVs. The results show that our method can decrease total travel time of the EVs by 50\\% in comparison with the state-of-the-art on a real dataset, where the benefit of our approach is more significant as the number of EVs on the road increases.",
        "citation_title": "Proactive Route Planning for Electric Vehicles",
        "date_delivered": "[Submitted on 15 Mar 2024]"
    },
    {
        "abstract": "The tremendous development in large language models (LLM) has led to a new wave of innovations and applications and yielded research results that were initially forecast to take longer. In this work, we tap into these recent developments and present a meta-study about the potential of large language models if deployed in social robots. We place particular emphasis on the applications of social robots: education, healthcare, and entertainment. Before being deployed in social robots, we also study how these language models could be safely trained to ``understand'' societal norms and issues, such as trust, bias, ethics, cognition, and teamwork. We hope this study provides a resourceful guide to other robotics researchers interested in incorporating language models in their robots.",
        "citation_title": "Large Language Models for Human-Robot Interaction: Opportunities and Risks",
        "date_delivered": "[Submitted on 26 Mar 2024]"
    },
    {
        "abstract": "Autonomous vehicle control is growing in availability for new vehicles and there is a potential need to retrofit older vehicles with this capability. Additionally, automotive cybersecurity has become a significant concern in recent years due to documented attacks on vehicles. As a result, researchers have been exploring reverse engineering techniques to automate vehicle control and improve vehicle security and threat analysis. In prior work, a vehicle's accelerator and brake pedal controller area network (CAN) channels were identified using reverse engineering techniques without prior knowledge of the vehicle. However, the correlation results for deceleration were lower than those for acceleration, which may be able to be improved by incorporating data from an additional telemetry device. In this paper, a method that uses IMU and GPS data to reverse-engineer a vehicle's steering wheel position CAN channels, without prior knowledge of the vehicle, is presented. Using GPS data is shown to greatly improve correlation values for deceleration, particularly for the brake pedal CAN channels. This work demonstrates the efficacy of using these data sources for automotive CAN reverse engineering. This has potential uses in automotive vehicle control and for improving vehicle security and threat analysis.",
        "citation_title": "Analysis of the Efficacy of the Use of Inertial Measurement and Global Positioning System Data to Reverse Engineer Automotive CAN Bus Steering Signals",
        "date_delivered": "[Submitted on 27 Mar 2024]"
    },
    {
        "abstract": "Accurate dynamic models are crucial for many robotic applications. Traditional approaches to deriving these models are based on the application of Lagrangian or Newtonian mechanics. Although these methods provide a good insight into the physical behaviour of the system, they rely on the exact knowledge of parameters such as inertia, friction and joint flexibility. In addition, the system is often affected by uncertain and nonlinear effects, such as saturation and dead zones, which can be difficult to model. A popular alternative is the application of Machine Learning (ML) techniques - e.g., Neural Networks (NNs) - in the context of a \"black-box\" methodology. This paper reports on our experience with this approach for a real-life 6 degrees of freedom (DoF) manipulator. Specifically, we considered several NN architectures: single NN, multiple NNs, and cascade NN. We compared the performance of the system by using different policies for selecting the NN hyperparameters. Our experiments reveal that the best accuracy and performance are obtained by a cascade NN, in which we encode our prior physical knowledge about the dependencies between joints, complemented by an appropriate optimisation of the hyperparameters.",
        "citation_title": "Joint torques prediction of a robotic arm using neural networks",
        "date_delivered": "[Submitted on 28 Mar 2024]"
    },
    {
        "abstract": "Sampling critical testing scenarios is an essential step in intelligence testing for Automated Vehicles (AVs). However, due to the lack of prior knowledge on the distribution of critical scenarios in sampling space, we can hardly efficiently find the critical scenarios or accurately evaluate the intelligence of AVs. To solve this problem, we formulate the testing as a continuous optimization process which iteratively generates potential critical scenarios and meanwhile evaluates these scenarios. A bi-level loop is proposed for such life-long learning and testing. In the outer loop, we iteratively learn space knowledge by evaluating AV in the already sampled scenarios and then sample new scenarios based on the retained knowledge. Outer loop stops when all generated samples cover the whole space. While to maximize the coverage of the space in each outer loop, we set an inner loop which receives newly generated samples in outer loop and outputs the updated positions of these samples. We assume that points in a small sphere-like subspace can be covered (or represented) by the point in the center of this sphere. Therefore, we can apply a multi-rounds heuristic strategy to move and pack these spheres in space to find the best covering solution. The simulation results show that faster and more accurate evaluation of AVs can be achieved with more critical scenarios.",
        "citation_title": "Life-long Learning and Testing for Automated Vehicles via Adaptive Scenario Sampling as A Continuous Optimization Process",
        "date_delivered": "[Submitted on 28 Mar 2024]"
    },
    {
        "abstract": "This paper addresses the challenge of co-designing morphology and control in soft robots via a novel neural network evolution approach. We propose an innovative method to implicitly dual-encode soft robots, thus facilitating the simultaneous design of morphology and control. Additionally, we introduce the large language model to serve as the control center during the evolutionary process. This advancement considerably optimizes the evolution speed compared to traditional soft-bodied robot co-design methods. Further complementing our work is the implementation of Gaussian positional encoding - an approach that augments the neural network's comprehension of robot morphology. Our paper offers a new perspective on soft robot design, illustrating substantial improvements in efficiency and comprehension during the design and evolutionary process.",
        "citation_title": "CUDA-Accelerated Soft Robot Neural Evolution with Large Language Model Supervision",
        "date_delivered": "[Submitted on 12 Apr 2024]"
    },
    {
        "abstract": "Spiking Neural Network (SNN) is acknowledged as the next generation of Artificial Neural Network (ANN) and hold great promise in effectively processing spatial-temporal information. However, the choice of timestep becomes crucial as it significantly impacts the accuracy of the neural network training. Specifically, a smaller timestep indicates better performance in efficient computing, resulting in reduced latency and operations. While, using a small timestep may lead to low accuracy due to insufficient information presentation with few spikes. This observation motivates us to develop an SNN that is more reliable for adaptive timestep by introducing a novel regularisation technique, namely Spatial-Temporal Regulariser (STR). Our approach regulates the ratio between the strength of spikes and membrane potential at each timestep. This effectively balances spatial and temporal performance during training, ultimately resulting in an Anytime Optimal Inference (AOI) SNN. Through extensive experiments on frame-based and event-based datasets, our method, in combination with cutoff based on softmax output, achieves state-of-the-art performance in terms of both latency and accuracy. Notably, with STR and cutoff, SNN achieves 2.14 to 2.89 faster in inference compared to the pre-configured timestep with near-zero accuracy drop of 0.50% to 0.64% over the event-based datasets. Code available: this https URL",
        "citation_title": "Direct Training Needs Regularisation: Anytime Optimal Inference Spiking Neural Network",
        "date_delivered": "[Submitted on 15 Apr 2024]"
    },
    {
        "abstract": "Artificial neuronal devices are the basic building blocks for neuromorphic computing systems, which have been motivated by realistic brain emulation. Aiming for these applications, various device concepts have been proposed to mimic the neuronal dynamics and functions. While till now, the artificial neuron devices with high efficiency, high stability and low power consumption are still far from practical application. Due to the special insulator-metal phase transition, Vanadium Dioxide (VO2) has been considered as an idea candidate for neuronal device fabrication. However, its intrinsic insulating state requires the VO2 neuronal device to be driven under large bias voltage, resulting in high power consumption and low frequency. Thus in the current study, we have addressed this challenge by preparing oxygen vacancies modulated VO2 film(VO2-x) and fabricating the VO2-x neuronal devices for Spiking Neural Networks (SNNs) construction. Results indicate the neuron devices can be operated under lower voltage with improved processing speed. The proposed VO2-x based back-propagation SNNs (BP-SNNs) system, trained with the MNIST dataset, demonstrates excellent accuracy in image recognition. Our study not only demonstrates the VO2-x based neurons and SNN system for practical application, but also offers an effective way to optimize the future neuromorphic computing systems by defect engineering strategy.",
        "citation_title": "Oxygen vacancies modulated VO2 for neurons and Spiking Neural Network construction",
        "date_delivered": "[Submitted on 16 Apr 2024]"
    },
    {
        "abstract": "ChatGPT has changed the AI community and an active research line is the performance evaluation of ChatGPT. A key challenge for the evaluation is that ChatGPT is still closed-source and traditional benchmark datasets may have been used by ChatGPT as the training data. In this paper, (i) we survey recent studies which uncover the real performance levels of ChatGPT in seven categories of NLP tasks, (ii) review the social implications and safety issues of ChatGPT, and (iii) emphasize key challenges and opportunities for its evaluation. We hope our survey can shed some light on its blackbox manner, so that researchers are not misleaded by its surface generation.",
        "citation_title": "A Survey on the Real Power of ChatGPT",
        "date_delivered": "[Submitted on 22 Apr 2024]"
    },
    {
        "abstract": "The pre-trained Large Language Models (LLMs) can be adapted for many downstream tasks and tailored to align with human preferences through fine-tuning. Recent studies have discovered that LLMs can achieve desirable performance with only a small amount of high-quality data, suggesting that a large amount of the data in these extensive datasets is redundant or even harmful. Identifying high-quality data from vast datasets to curate small yet effective datasets has emerged as a critical challenge. In this paper, we introduce SHED, an automated dataset refinement framework based on Shapley value for instruction fine-tuning. SHED eliminates the need for human intervention or the use of commercial LLMs. Moreover, the datasets curated through SHED exhibit transferability, indicating they can be reused across different LLMs with consistently high performance. We conduct extensive experiments to evaluate the datasets curated by SHED. The results demonstrate SHED's superiority over state-of-the-art methods across various tasks and LLMs; notably, datasets comprising only 10% of the original data selected by SHED achieve performance comparable to or surpassing that of the full datasets.",
        "citation_title": "SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning",
        "date_delivered": "[Submitted on 23 Apr 2024]"
    },
    {
        "abstract": "This paper evaluated the effectiveness of using generative AI to simplify science communication and enhance public trust in science. By comparing lay summaries of journal articles from PNAS, yoked to those generated by AI, this work assessed linguistic simplicity across such summaries and public perceptions. Study 1a analyzed simplicity features of PNAS abstracts (scientific summaries) and significance statements (lay summaries), observing that lay summaries were indeed linguistically simpler, but effect size differences were small. Study 1b used GPT-4 to create significance statements based on paper abstracts and this more than doubled the average effect size without fine-tuning. Finally, Study 2 experimentally demonstrated that simply-written GPT summaries facilitated more favorable public perceptions of scientists (their credibility, trustworthiness) than more complexly-written human PNAS summaries. AI has the potential to engage scientific communities and the public via a simple language heuristic, advocating for its integration into scientific dissemination for a more informed society.",
        "citation_title": "Science Written by Generative AI is Perceived as Less Intelligent, but More Credible and Trustworthy than Science Written by Humans",
        "date_delivered": "[Submitted on 23 Apr 2024]"
    },
    {
        "abstract": "Counterfactual examples are useful for exploring the decision boundaries of machine learning models and determining feature attributions. How can we apply counterfactual-based methods to analyze and explain LLMs? We identify the following key challenges. First, the generated textual counterfactuals should be meaningful and readable to users and thus can be mentally compared to draw conclusions. Second, to make the solution scalable to long-form text, users should be equipped with tools to create batches of counterfactuals from perturbations at various granularity levels and interactively analyze the results. In this paper, we tackle the above challenges and contribute 1) a novel algorithm for generating batches of complete and meaningful textual counterfactuals by removing and replacing text segments in different granularities, and 2) LLM Analyzer, an interactive visualization tool to help users understand an LLM's behaviors by interactively inspecting and aggregating meaningful counterfactuals. We evaluate the proposed algorithm by the grammatical correctness of its generated counterfactuals using 1,000 samples from medical, legal, finance, education, and news datasets. In our experiments, 97.2% of the counterfactuals are grammatically correct. Through a use case, user studies, and feedback from experts, we demonstrate the usefulness and usability of the proposed interactive visualization tool.",
        "citation_title": "Interactive Analysis of LLMs using Meaningful Counterfactuals",
        "date_delivered": "[Submitted on 23 Apr 2024]"
    },
    {
        "abstract": "Tool-augmented Large Language Models (LLMs) have shown impressive capabilities in remote sensing (RS) applications. However, existing benchmarks assume question-answering input templates over predefined image-text data pairs. These standalone instructions neglect the intricacies of realistic user-grounded tasks. Consider a geospatial analyst: they zoom in a map area, they draw a region over which to collect satellite imagery, and they succinctly ask \"Detect all objects here\". Where is `here`, if it is not explicitly hardcoded in the image-text template, but instead is implied by the system state, e.g., the live map positioning? To bridge this gap, we present GeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual, and click-based actions on a real UI platform. Through in-depth evaluation of state-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights towards stronger agents for RS applications.",
        "citation_title": "Evaluating Tool-Augmented Agents in Remote Sensing Platforms",
        "date_delivered": "[Submitted on 23 Apr 2024]"
    },
    {
        "abstract": "This research proposes a novel approach to the Word Sense Disambiguation (WSD) task in the Georgian language, based on supervised fine-tuning of a pre-trained Large Language Model (LLM) on a dataset formed by filtering the Georgian Common Crawls corpus. The dataset is used to train a classifier for words with multiple senses. Additionally, we present experimental results of using LSTM for WSD. Accurately disambiguating homonyms is crucial in natural language processing. Georgian, an agglutinative language belonging to the Kartvelian language family, presents unique challenges in this context. The aim of this paper is to highlight the specific problems concerning homonym disambiguation in the Georgian language and to present our approach to solving them. The techniques discussed in the article achieve 95% accuracy for predicting lexical meanings of homonyms using a hand-classified dataset of over 7500 sentences.",
        "citation_title": "Homonym Sense Disambiguation in the Georgian Language",
        "date_delivered": "[Submitted on 24 Apr 2024]"
    },
    {
        "abstract": "In recent years, generative artificial intelligence models, represented by Large Language Models (LLMs) and Diffusion Models (DMs), have revolutionized content production methods. These artificial intelligence-generated content (AIGC) have become deeply embedded in various aspects of daily life and work, spanning texts, images, videos, and audio. The authenticity of AI-generated content is progressively enhancing, approaching human-level creative standards. However, these technologies have also led to the emergence of Fake Artificial Intelligence Generated Content (FAIGC), posing new challenges in distinguishing genuine information. It is crucial to recognize that AIGC technology is akin to a double-edged sword; its potent generative capabilities, while beneficial, also pose risks for the creation and dissemination of FAIGC. In this survey, We propose a new taxonomy that provides a more comprehensive breakdown of the space of FAIGC methods today. Next, we explore the modalities and generative technologies of FAIGC, categorized under AI-generated disinformation and AI-generated misinformation. From various perspectives, we then introduce FAIGC detection methods, including Deceptive FAIGC Detection, Deepfake Detection, and Hallucination-based FAIGC Detection. Finally, we discuss outstanding challenges and promising areas for future research.",
        "citation_title": "Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of Theories, Detection Methods, and Opportunities",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "Large Language Models (LLMs) have shown promising capabilities in handling clinical text summarization tasks. In this study, we demonstrate that a small open-source LLM can be effectively trained to generate high-quality clinical notes from outpatient patient-doctor dialogues. We achieve this through a comprehensive domain- and task-specific adaptation process for the LLaMA-2 13 billion parameter model. This process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced an enhanced approach, termed DistillDirect, for performing on-policy reinforcement learning with Gemini Pro serving as the teacher model. Our resulting model, LLaMA-Clinic, is capable of generating clinical notes that are comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as \"acceptable\" or higher across all three criteria: real-world readiness, completeness, and accuracy. Notably, in the more challenging \"Assessment and Plan\" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness compared to physician-authored notes (4.1/5). Additionally, we identified caveats in public clinical note datasets, such as ACI-BENCH. We highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format. Overall, our research demonstrates the potential and feasibility of training smaller, open-source LLMs to assist with clinical documentation, capitalizing on healthcare institutions' access to patient records and domain expertise. We have made our newly created synthetic clinic dialogue-note dataset and the physician feedback dataset publicly available to foster future research in this field.",
        "citation_title": "Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering task with answer options for evaluation. However, in real clinical settings, many clinical decisions, such as treatment recommendations, involve answering open-ended questions without pre-set options. Meanwhile, existing studies mainly use accuracy to assess model performance. In this paper, we comprehensively benchmark diverse LLMs in healthcare, to clearly understand their strengths and weaknesses. Our benchmark contains seven tasks and thirteen datasets across medical language generation, understanding, and reasoning. We conduct a detailed evaluation of the existing sixteen LLMs in healthcare under both zero-shot and few-shot (i.e., 1,3,5-shot) learning settings. We report the results on five metrics (i.e. matching, faithfulness, comprehensiveness, generalizability, and robustness) that are critical in achieving trust from clinical users. We further invite medical experts to conduct human evaluation.",
        "citation_title": "Large Language Models in Healthcare: A Comprehensive Benchmark",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "Obtaining sufficient information in one's mother tongue is crucial for satisfying the information needs of the users. While high-resource languages have abundant online resources, the situation is less than ideal for very low-resource languages. Moreover, the insufficient reporting of vital national and international events continues to be a worry, especially in languages with scarce resources, like \\textbf{Mizo}. In this paper, we conduct a study to investigate the effectiveness of a simple methodology designed to generate a holistic summary for Mizo news articles, which leverages English-language news to supplement and enhance the information related to the corresponding news events. Furthermore, we make available 500 Mizo news articles and corresponding enriched holistic summaries. Human evaluation confirms that our approach significantly enhances the information coverage of Mizo news articles. The mizo dataset and code can be accessed at \\url{this https URL",
        "citation_title": "Exploring News Summarization and Enrichment in a Highly Resource-Scarce Indian Language: A Case Study of Mizo",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "Ensuring the resilience of Large Language Models (LLMs) against malicious exploitation is paramount, with recent focus on mitigating offensive responses. Yet, the understanding of cant or dark jargon remains unexplored. This paper introduces a domain-specific Cant dataset and CantCounter evaluation framework, employing Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis stages. Experiments reveal LLMs, including ChatGPT, are susceptible to cant bypassing filters, with varying recognition accuracy influenced by question types, setups, and prompt clues. Updated models exhibit higher acceptance rates for cant queries. Moreover, LLM reactions differ across domains, e.g., reluctance to engage in racism versus LGBT topics. These findings underscore LLMs' understanding of cant and reflect training data characteristics and vendor approaches to sensitive topics. Additionally, we assess LLMs' ability to demonstrate reasoning capabilities. Access to our datasets and code is available at this https URL.",
        "citation_title": "Can't say cant? Measuring and Reasoning of Dark Jargons in Large Language Models",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "As NLP models become more complex, understanding their decisions becomes more crucial. Counterfactuals (CFs), where minimal changes to inputs flip a model's prediction, offer a way to explain these models. While Large Language Models (LLMs) have shown remarkable performance in NLP tasks, their efficacy in generating high-quality CFs remains uncertain. This work fills this gap by investigating how well LLMs generate CFs for two NLU tasks. We conduct a comprehensive comparison of several common LLMs, and evaluate their CFs, assessing both intrinsic metrics, and the impact of these CFs on data augmentation. Moreover, we analyze differences between human and LLM-generated CFs, providing insights for future research directions. Our results show that LLMs generate fluent CFs, but struggle to keep the induced changes minimal. Generating CFs for Sentiment Analysis (SA) is less challenging than NLI where LLMs show weaknesses in generating CFs that flip the original label. This also reflects on the data augmentation performance, where we observe a large gap between augmenting with human and LLMs CFs. Furthermore, we evaluate LLMs' ability to assess CFs in a mislabelled data setting, and show that they have a strong bias towards agreeing with the provided labels. GPT4 is more robust against this bias and its scores correlate well with automatic metrics. Our findings reveal several limitations and point to potential future work directions.",
        "citation_title": "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study",
        "date_delivered": "[Submitted on 26 Apr 2024]"
    },
    {
        "abstract": "The integration of Artificial Intelligence (AI) in healthcare presents a transformative potential for enhancing operational efficiency and health outcomes. Large Language Models (LLMs), such as ChatGPT, have shown their capabilities in supporting medical decision-making. Embedding LLMs in medical systems is becoming a promising trend in healthcare development. The potential of ChatGPT to address the triage problem in emergency departments has been examined, while few studies have explored its application in outpatient departments. With a focus on streamlining workflows and enhancing efficiency for outpatient triage, this study specifically aims to evaluate the consistency of responses provided by ChatGPT in outpatient guidance, including both within-version response analysis and between-version comparisons. For within-version, the results indicate that the internal response consistency for ChatGPT-4.0 is significantly higher than ChatGPT-3.5 (p=0.03) and both have a moderate consistency (71.2% for 4.0 and 59.6% for 3.5) in their top recommendation. However, the between-version consistency is relatively low (mean consistency score=1.43/3, median=1), indicating few recommendations match between the two versions. Also, only 50% top recommendations match perfectly in the comparisons. Interestingly, ChatGPT-3.5 responses are more likely to be complete than those from ChatGPT-4.0 (p=0.02), suggesting possible differences in information processing and response generation between the two versions. The findings offer insights into AI-assisted outpatient operations, while also facilitating the exploration of potentials and limitations of LLMs in healthcare utilization. Future research may focus on carefully optimizing LLMs and AI integration in healthcare systems based on ergonomic and human factors principles, precisely aligning with the specific needs of effective outpatient triage.",
        "citation_title": "Evaluating the Application of ChatGPT in Outpatient Triage Guidance: A Comparative Study",
        "date_delivered": "[Submitted on 27 Apr 2024]"
    },
    {
        "abstract": "Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, we measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.",
        "citation_title": "LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report",
        "date_delivered": "[Submitted on 29 Apr 2024]"
    },
    {
        "abstract": "Graphics Processing Units (GPUs) have become the leading hardware accelerator for deep learning applications and are used widely in training and inference of transformers; transformers have achieved state-of-the-art performance in many areas of machine learning and are especially used in most modern Large Language Models (LLMs). However, GPUs require large amounts of energy, which poses environmental concerns, demands high operational costs, and causes GPUs to be unsuitable for edge computing. We develop an accelerator for transformers, namely, Llama 2, an open-source state-of-the-art LLM, using high level synthesis (HLS) on Field Programmable Gate Arrays (FPGAs). HLS allows us to rapidly prototype FPGA designs without writing code at the register-transfer level (RTL). We name our method HLSTransform, and the FPGA designs we synthesize with HLS achieve up to a 12.75x reduction and 8.25x reduction in energy used per token on the Xilinx Virtex UltraScale+ VU9P FPGA compared to an Intel Xeon Broadwell E5-2686 v4 CPU and NVIDIA RTX 3090 GPU respectively, while increasing inference speeds by up to 2.46x compared to CPU and maintaining 0.53x the speed of an RTX 3090 GPU despite the GPU's 4 times higher base clock rate. With the lack of existing open-source FPGA accelerators for transformers, we open-source our code and document our steps for synthesis. We hope this work will serve as a step in democratizing the use of FPGAs in transformer inference and inspire research into energy-efficient inference methods as a whole. The code can be found on this https URL.",
        "citation_title": "HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis",
        "date_delivered": "[Submitted on 29 Apr 2024]"
    },
    {
        "abstract": "Does Knowledge Distillation (KD) really work? Conventional wisdom viewed it as a knowledge transfer procedure where a perfect mimicry of the student to its teacher is desired. However, paradoxical studies indicate that closely replicating the teacher's behavior does not consistently improve student generalization, posing questions on its possible causes. Confronted with this gap, we hypothesize that diverse attentions in teachers contribute to better student generalization at the expense of reduced fidelity in ensemble KD setups. By increasing data augmentation strengths, our key findings reveal a decrease in the Intersection over Union (IoU) of attentions between teacher models, leading to reduced student overfitting and decreased fidelity. We propose this low-fidelity phenomenon as an underlying characteristic rather than a pathology when training KD. This suggests that stronger data augmentation fosters a broader perspective provided by the divergent teacher ensemble and lower student-teacher mutual information, benefiting generalization performance. These insights clarify the mechanism on low-fidelity phenomenon in KD. Thus, we offer new perspectives on optimizing student model performance, by emphasizing increased diversity in teacher attentions and reduced mimicry behavior between teachers and student.",
        "citation_title": "Why does Knowledge Distillation Work? Rethink its Attention and Fidelity Mechanism",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "There are a thousand ways to caption an image. Contrastive Language Pretraining (CLIP) on the other hand, works by mapping an image and its caption to a single vector -- limiting how well CLIP-like models can represent the diverse ways to describe an image. In this work, we introduce Llip, Latent Language Image Pretraining, which models the diversity of captions that could match an image. Llip's vision encoder outputs a set of visual features that are mixed into a final representation by conditioning on information derived from the text. We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks even with large-scale encoders. Llip improves zero-shot classification by an average of 2.9% zero-shot classification benchmarks with a ViT-G/14 encoder. Specifically, Llip attains a zero-shot top-1 accuracy of 83.5% on ImageNet outperforming a similarly sized CLIP by 1.4%. We also demonstrate improvement on zero-shot retrieval on MS-COCO by 6.0%. We provide a comprehensive analysis of the components introduced by the method and demonstrate that Llip leads to richer visual representations.",
        "citation_title": "Modeling Caption Diversity in Contrastive Vision-Language Pretraining",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Mitigating cybersecurity risk in electric vehicle (EV) charging demand forecasting plays a crucial role in the safe operation of collective EV chargings, the stability of the power grid, and the cost-effective infrastructure expansion. However, existing methods either suffer from the data privacy issue and the susceptibility to cyberattacks or fail to consider the spatial correlation among different stations. To address these challenges, a federated graph learning approach involving multiple charging stations is proposed to collaboratively train a more generalized deep learning model for demand forecasting while capturing spatial correlations among various stations and enhancing robustness against potential attacks. Firstly, for better model performance, a Graph Neural Network (GNN) model is leveraged to characterize the geographic correlation among different charging stations in a federated manner. Secondly, to ensure robustness and deal with the data heterogeneity in a federated setting, a message passing that utilizes a global attention mechanism to aggregate personalized models for each client is proposed. Thirdly, by concerning cyberattacks, a special credit-based function is designed to mitigate potential threats from malicious clients or unwanted attacks. Extensive experiments on a public EV charging dataset are conducted using various deep learning techniques and federated learning methods to demonstrate the prediction accuracy and robustness of the proposed approach.",
        "citation_title": "Federated Graph Learning for EV Charging Demand Forecasting with Personalization Against Cyberattacks",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Neural networks have become a widely adopted tool for tackling a variety of problems in machine learning and artificial intelligence. In this contribution we use the mathematical framework of local stability analysis to gain a deeper understanding of the learning dynamics of feed forward neural networks. Therefore, we derive equations for the tangent operator of the learning dynamics of three-layer networks learning regression tasks. The results are valid for an arbitrary numbers of nodes and arbitrary choices of activation functions. Applying the results to a network learning a regression task, we investigate numerically, how stability indicators relate to the final training-loss. Although the specific results vary with different choices of initial conditions and activation functions, we demonstrate that it is possible to predict the final training loss, by monitoring finite-time Lyapunov exponents or covariant Lyapunov vectors during the training process.",
        "citation_title": "On the weight dynamics of learning networks",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "To create useful reinforcement learning (RL) agents, step zero is to design a suitable reward function that captures the nuances of the task. However, reward engineering can be a difficult and time-consuming process. Instead, human-in-the-loop (HitL) RL allows agents to learn reward functions from human feedback. Despite recent successes, many of the HitL RL methods still require numerous human interactions to learn successful reward functions. To improve the feedback efficiency of HitL RL methods (i.e., require less feedback), this paper introduces Sub-optimal Data Pre-training, SDP, an approach that leverages reward-free, sub-optimal data to improve scalar- and preference-based HitL RL algorithms. In SDP, we start by pseudo-labeling all low-quality data with rewards of zero. Through this process, we obtain free reward labels to pre-train our reward model. This pre-training phase provides the reward model a head start in learning, whereby it can identify that low-quality transitions should have a low reward, all without any actual feedback. Through extensive experiments with a simulated teacher, we demonstrate that SDP can significantly improve or achieve competitive performance with state-of-the-art (SOTA) HitL RL algorithms across nine robotic manipulation and locomotion tasks.",
        "citation_title": "Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference dataset through a natural loss function that integrates preference loss with a regularization term across the model's entire output distribution rather than limiting it to the preference dataset. Although SPO does not require the assumption of an existing underlying reward model, we demonstrate that, under the Bradley-Terry (BT) model assumption, it converges to a softmax of scaled rewards, with the distribution's \"softness\" adjustable via the softmax exponent, an algorithm parameter. We showcase SPO's methodology, its theoretical foundation, and its comparative advantages in simplicity, computational efficiency, and alignment precision.",
        "citation_title": "Soft Preference Optimization: Aligning Language Models to Expert Distributions",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Unlike traditional educational chatbots that rely on pre-programmed responses, large-language model-driven chatbots, such as ChatGPT, demonstrate remarkable versatility and have the potential to serve as a dynamic resource for addressing student needs from understanding advanced concepts to solving complex problems. This work explores the impact of such technology on student learning in an interdisciplinary, project-oriented data visualization course. Throughout the semester, students engaged with ChatGPT across four distinct projects, including data visualizations and implementing them using a variety of tools including Tableau, D3, and Vega-lite. We collected conversation logs and reflection surveys from the students after each assignment. In addition, we conducted interviews with selected students to gain deeper insights into their overall experiences with ChatGPT. Our analysis examined the advantages and barriers of using ChatGPT, students' querying behavior, the types of assistance sought, and its impact on assignment outcomes and engagement. Based on the findings, we discuss design considerations for an educational solution that goes beyond the basic interface of ChatGPT, specifically tailored for data visualization education.",
        "citation_title": "ChatGPT in Data Visualization Education: A Student Perspective",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In many practical applications, it is often difficult and expensive to obtain large-scale labeled data to train state-of-the-art deep neural networks. Therefore, transferring the learned knowledge from a separate, labeled source domain to an unlabeled or sparsely labeled target domain becomes an appealing alternative. However, direct transfer often results in significant performance decay due to domain shift. Domain adaptation (DA) aims to address this problem by aligning the distributions between the source and target domains. Multi-source domain adaptation (MDA) is a powerful and practical extension in which the labeled data may be collected from multiple sources with different distributions. In this survey, we first define various MDA strategies. Then we systematically summarize and compare modern MDA methods in the deep learning era from different perspectives, followed by commonly used datasets and a brief benchmark. Finally, we discuss future research directions for MDA that are worth investigating.",
        "citation_title": "More is Better: Deep Domain Adaptation with Multiple Sources",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Teaching programming in early childhood (4-9) to enhance computational thinking has gained popularity in the recent movement of computer science for all. However, current practices ignore some fundamental issues resulting from young children's developmental readiness, such as the sustained capability to keyboarding, the decomposition of complex tasks to small tasks, the need for intuitive mapping from abstract programming to tangible outcomes, and the limited amount of screen time exposure. To address these issues in this paper, we present a novel methodology with an AI-powered integration platform to effectively teach computational thinking for young children. The system features a hybrid pedagogy that supports both the top-down and bottom-up approach for teaching computational thinking. Young children can describe their desired task in natural language, while the system can respond with an easy-to-understand program consisting of the right level of decomposed sub-tasks. A tangible robot can immediately execute the decomposed program and demonstrate the program's outcomes to young children. The system is equipped with an intelligent chatbot that can interact with young children through natural languages, and children can speak to the chatbot to complete all the needed programming tasks, while the chatbot orchestrates the execution of the program onto the robot. This would completely eliminates the need of keyboards for young children to program. By developing such a system, we aim to make the concept of computational thinking more accessible to young children, fostering a natural understanding of programming concepts without the need of explicit programming skills. Through the interactive experience provided by the robotic agent, our system seeks to engage children in an effective manner, contributing to the field of educational technology for early childhood computer science education.",
        "citation_title": "From Keyboard to Chatbot: An AI-powered Integration Platform with Large-Language Models for Teaching Computational Thinking for Young Children",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We propose a novel computational approach to automatically analyze the physical process behind printing of early modern letterpress books via clustering the running titles found at the top of their pages. Specifically, we design and compare custom neural and feature-based kernels for computing pairwise visual similarity of a scanned document's running titles and cluster the titles in order to track any deviations from the expected pattern of a book's printing. Unlike body text which must be reset for every page, the running titles are one of the static type elements in a skeleton forme i.e. the frame used to print each side of a sheet of paper, and were often re-used during a book's printing. To evaluate the effectiveness of our approach, we manually annotate the running title clusters on about 1600 pages across 8 early modern books of varying size and formats. Our method can detect potential deviation from the expected patterns of such skeleton formes, which helps bibliographers understand the phenomena associated with a text's transmission, such as censorship. We also validate our results against a manual bibliographic analysis of a counterfeit early edition of Thomas Hobbes' Leviathan (1651).",
        "citation_title": "Clustering Running Titles to Understand the Printing of Early Modern Books",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate remarkable adaptability across zero-shot classification tasks without additional training. However, their performance diminishes in the presence of domain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision. Our method employs a unique, minimally invasive text prompt tuning process, wherein multiple predicted classes are aggregated into a single new text prompt, used as pseudo label to re-classify inputs in a transductive manner. Additionally, we pioneer the standardization of TTA benchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that, without requiring additional transformations nor new trainable modules, CLIPArTT enhances performance dynamically across non-corrupted datasets such as CIFAR-10, corrupted datasets like CIFAR-10-C and CIFAR-10.1, alongside synthetic datasets such as VisDA-C. This research underscores the potential for improving VLMs' adaptability through novel test-time strategies, offering insights for robust performance across varied datasets and environments. The code can be found at: this https URL",
        "citation_title": "CLIPArTT: Light-weight Adaptation of CLIP to New Domains at Test Time",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantum machine learning is a new research field combining quantum information science and machine learning. Quantum computing technologies seem to be particularly well suited to solving problems in the health sector in an efficient way, because they may deal with large datasets more efficiently than classical AI.\nAlzheimer's disease is a neurodegenerative brain disorder that mostly affects elderly people, causing important cognitive impairments. It is the most common cause of dementia and it has an effect on memory, thought, learning abilities and movement control. This type of disease has no cure, consequently an early diagnosis is fundamental for reducing its impact. The analysis of handwriting can be effective for diagnosing, as many researches have conjectured. The DARWIN (Diagnosis AlzheimeR WIth haNdwriting) dataset contains handwriting samples from people affected by Alzheimer's disease and a group of healthy people. Here we apply quantum AI to this use-case. In particular, we use this dataset to test kernel methods for classification task and compare their performances with the ones obtained via quantum machine learning methods. We find that quantum and classical algorithms achieve similar performances and in some cases quantum methods perform even better.\nOur results pave the way for future new quantum machine learning applications in early-screening diagnostics in the healthcare domain.",
        "citation_title": "Quantum AI for Alzheimer's disease early screening",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Optimizing a text-to-image diffusion model with a given reward function is an important but underexplored research area. In this study, we propose Deep Reward Tuning (DRTune), an algorithm that directly supervises the final output image of a text-to-image diffusion model and back-propagates through the iterative sampling process to the input noise. We find that training earlier steps in the sampling process is crucial for low-level rewards, and deep supervision can be achieved efficiently and effectively by stopping the gradient of the denoising network input. DRTune is extensively evaluated on various reward models. It consistently outperforms other algorithms, particularly for low-level control signals, where all shallow supervision methods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0) model via DRTune to optimize Human Preference Score v2.1, resulting in the Favorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances image quality compared to SDXL 1.0 and reaches comparable quality compared with Midjourney v5.2.",
        "citation_title": "Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "One of the most important processing steps in any analysis pipeline is handling missing data. Traditional approaches simply delete any sample or feature with missing elements. Recent imputation methods replace missing data based on assumed relationships between observed data and the missing elements. However, there is a largely under-explored alternative amid these extremes. Partial deletion approaches remove excessive amounts of missing data, as defined by the user. They can be used in place of traditional deletion or as a precursor to imputation. In this manuscript, we expand upon the Mr. Clean suite of algorithms, focusing on the scenario where all missing data is removed. We show that the RowCol Integer Program can be recast as a Linear Program, thereby reducing runtime. Additionally, the Element Integer Program can be reformulated to reduce the number of variables and allow for high levels of parallelization. Using real-world data sets from genetic, gene expression, and single cell RNA-seq experiments we demonstrate that our algorithms outperform existing deletion techniques over several missingness values, balancing runtime and data retention. Our combined greedy algorithm retains the maximum number of valid elements in 126 of 150 scenarios and stays within 1\\% of maximum in 23 of the remaining experiments. The reformulated Element IP complements the greedy algorithm when removing all missing data, boasting a reduced runtime and increase in valid elements in larger data sets, over its generic counterpart. These two programs greatly increase the amount of valid data retained over traditional deletion techniques and further improve on existing partial deletion algorithms.",
        "citation_title": "Improving Data Cleaning Using Discrete Optimization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Emerging multi-model workloads with heavy models like recent large language models significantly increased the compute and memory demands on hardware. To address such increasing demands, designing a scalable hardware architecture became a key problem. Among recent solutions, the 2.5D silicon interposer multi-chip module (MCM)-based AI accelerator has been actively explored as a promising scalable solution due to their significant benefits in the low engineering cost and composability. However, previous MCM accelerators are based on homogeneous architectures with fixed dataflow, which encounter major challenges from highly heterogeneous multi-model workloads due to their limited workload adaptivity. Therefore, in this work, we explore the opportunity in the heterogeneous dataflow MCM AI accelerators. We identify the scheduling of multi-model workload on heterogeneous dataflow MCM AI accelerator is an important and challenging problem due to its significance and scale, which reaches O(10^18) scale even for a single model case on 6x6 chiplets. We develop a set of heuristics to navigate the huge scheduling space and codify them into a scheduler with advanced techniques such as inter-chiplet pipelining. Our evaluation on ten multi-model workload scenarios for datacenter multitenancy and AR/VR use-cases has shown the efficacy of our approach, achieving on average 35.3% and 31.4% less energy-delay product (EDP) for the respective applications settings compared to homogeneous baselines.",
        "citation_title": "SCAR: Scheduling Multi-Model AI Workloads on Heterogeneous Multi-Chiplet Module Accelerators",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts.",
        "citation_title": "Obtaining Favorable Layouts for Multiple Object Generation",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Statistical learning theory and the Probably Approximately Correct (PAC) criterion are the common approach to mathematical learning theory. PAC is widely used to analyze learning problems and algorithms, and have been studied thoroughly. Uniform worst case bounds on the convergence rate have been well established using, e.g., VC theory or Radamacher complexity. However, in a typical scenario the performance could be much better. In this paper, we consider PAC learning using a somewhat different tradeoff, the error exponent - a well established analysis method in Information Theory - which describes the exponential behavior of the probability that the risk will exceed a certain threshold as function of the sample size. We focus on binary classification and find, under some stability assumptions, an improved distribution dependent error exponent for a wide range of problems, establishing the exponential behavior of the PAC error probability in agnostic learning. Interestingly, under these assumptions, agnostic learning may have the same error exponent as realizable learning. The error exponent criterion can be applied to analyze knowledge distillation, a problem that so far lacks a theoretical analysis.",
        "citation_title": "Error Exponent in Agnostic PAC Learning",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "IMSI (International Mobile Subscriber Identity) catchers, also known as \"Stingrays\" or \"cell site simulators,\" are rogue devices that pose a significant threat to cellular network security [1]. IMSI catchers can intercept and manipulate cellular communications, compromising the privacy and security of mobile devices and their users. With the advent of 4G and 5G networks, IMSI catchers have become more sophisticated and pose new challenges to cellular network security [2]. This paper provides an overview of the impact of IMSI catcher deployments on cellular network security in the context of 4G and 5G networks. It discusses the challenges posed by IMSI catchers, including the unauthorized collection of IMSI numbers, interception of communications, and potential misuse of subscriber information. It also highlights the potential consequences of IMSI catcher deployments, including the compromise of user privacy, financial fraud, and unauthorized surveillance. The paper further reviews the countermeasures that can be employed to mitigate the risks posed by IMSI catchers. These countermeasures include network-based solutions such as signal analysis, encryption, and authentication mechanisms, as well as user-based solutions such as mobile applications and device settings. The paper also discusses the limitations and effectiveness of these countermeasures in the context of 4G and 5G networks. Finally, the paper identifies research gaps and future directions for enhancing cellular network security against IMSI catchers in the era of 4G and 5G networks. This includes the need for improved encryption algorithms, authentication mechanisms, and detection techniques to effectively detect and prevent IMSI catcher deployments. The paper also emphasizes the importance of regulatory and policy measures to govern the deployment and use of IMSI catchers to protect user privacy and security.",
        "citation_title": "The Impact of IMSI Catcher Deployments on Cellular Network Security: Challenges and Countermeasures in 4G and 5G Networks",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Recent breakthroughs in single-image 3D portrait reconstruction have enabled telepresence systems to stream 3D portrait videos from a single camera in real-time, potentially democratizing telepresence. However, per-frame 3D reconstruction exhibits temporal inconsistency and forgets the user's appearance. On the other hand, self-reenactment methods can render coherent 3D portraits by driving a personalized 3D prior, but fail to faithfully reconstruct the user's per-frame appearance (e.g., facial expressions and lighting). In this work, we recognize the need to maintain both coherent identity and dynamic per-frame appearance to enable the best possible realism. To this end, we propose a new fusion-based method that fuses a personalized 3D subject prior with per-frame information, producing temporally stable 3D videos with faithful reconstruction of the user's per-frame appearances. Trained only using synthetic data produced by an expression-conditioned 3D GAN, our encoder-based method achieves both state-of-the-art 3D reconstruction accuracy and temporal consistency on in-studio and in-the-wild datasets.",
        "citation_title": "Coherent 3D Portrait Video Reconstruction via Triplane Fusion",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "A long continuous integration (CI) build forces developers to wait for CI feedback before starting subsequent development activities, leading to time wasted. In addition to a variety of build scheduling and test selection heuristics studied in the past, new artifact-based build technologies like Bazel have built-in support for advanced performance optimizations such as parallel build and incremental build (caching of build results). However, little is known about the extent to which new build technologies like Bazel deliver on their promised benefits, especially for long-build duration projects.\nIn this study, we collected 383 Bazel projects from GitHub, then studied their parallel and incremental build usage of Bazel in 4 popular CI services, and compared the results with Maven projects. We conducted 3,500 experiments on 383 Bazel projects and analyzed the build logs of a subset of 70 buildable projects to evaluate the performance impact of Bazel's parallel builds. Additionally, we performed 102,232 experiments on the 70 buildable projects' last 100 commits to evaluate Bazel's incremental build performance. Our results show that 31.23% of Bazel projects adopt a CI service but do not use Bazel in the CI service, while for those who do use Bazel in CI, 27.76% of them use other tools to facilitate Bazel's execution. Compared to sequential builds, the median speedups for long-build duration projects are 2.00x, 3.84x, 7.36x, and 12.80x, at parallelism degrees 2, 4, 8, and 16, respectively, even though, compared to a clean build, applying incremental build achieves a median speedup of 4.22x (with a build system tool-independent CI cache) and 4.71x (with a build system tool-specific cache) for long-build duration projects. Our results provide guidance for developers to improve the usage of Bazel in their projects.",
        "citation_title": "Does Using Bazel Help Speed Up Continuous Integration Builds?",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Motion prediction is a challenging problem in autonomous driving as it demands the system to comprehend stochastic dynamics and the multi-modal nature of real-world agent interactions. Diffusion models have recently risen to prominence, and have proven particularly effective in pedestrian motion prediction tasks. However, the significant time consumption and sensitivity to noise have limited the real-time predictive capability of diffusion models. In response to these impediments, we propose a novel diffusion-based, acceleratable framework that adeptly predicts future trajectories of agents with enhanced resistance to noise. The core idea of our model is to learn a coarse-grained prior distribution of trajectory, which can skip a large number of denoise steps. This advancement not only boosts sampling efficiency but also maintains the fidelity of prediction accuracy. Our method meets the rigorous real-time operational standards essential for autonomous vehicles, enabling prompt trajectory generation that is vital for secure and efficient navigation. Through extensive experiments, our method speeds up the inference time to 136ms compared to standard diffusion model, and achieves significant improvement in multi-agent motion prediction on the Argoverse 1 motion forecasting dataset.",
        "citation_title": "ADM: Accelerated Diffusion Model via Estimated Priors for Robust Motion Prediction under Uncertainties",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Customer service is how companies interface with their customers. It can contribute heavily towards the overall customer satisfaction. However, high-quality service can become expensive, creating an incentive to make it as cost efficient as possible and prompting most companies to utilize AI-powered assistants, or \"chat bots\". On the other hand, human-to-human interaction is still desired by customers, especially when it comes to complex scenarios such as disputes and sensitive topics like bill payment.\nThis raises the bar for customer service agents. They need to accurately understand the customer's question or concern, identify a solution that is acceptable yet feasible (and within the company's policy), all while handling multiple conversations at once.\nIn this work, we introduce \"Ask Me Anything\" (AMA) as an add-on feature to an agent-facing customer service interface. AMA allows agents to ask questions to a large language model (LLM) on demand, as they are handling customer conversations -- the LLM provides accurate responses in real-time, reducing the amount of context switching the agent needs. In our internal experiments, we find that agents using AMA versus a traditional search experience spend approximately 10% fewer seconds per conversation containing a search, translating to millions of dollars of savings annually. Agents that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating its usefulness as an AI-assisted feature for customer care.",
        "citation_title": "\"Ask Me Anything\": How Comcast Uses LLMs to Assist Agents in Real Time",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Spectral estimation is a fundamental task in signal processing. Recent algorithms in quantum phase estimation are concerned with the large noise, large frequency regime of the spectral estimation problem. The recent work in Ding-Epperly-Lin-Zhang shows that the ESPRIT algorithm exhibits superconvergence behavior for the spike locations in terms of the maximum frequency. This note provides a perturbative analysis to explain this behavior. It also extends the discussion to the case where the noise grows with the sampling frequency.",
        "citation_title": "A perturbative analysis for noisy spectral estimation",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We consider the problem of the discrete-time approximation of the solution of a one-dimensional SDE with piecewise locally Lipschitz drift and continuous diffusion coefficients with polynomial growth. In this paper, we study the strong convergence of a (semi-explicit) exponential-Euler scheme previously introduced in Bossy et al. (2021). We show the usual 1/2 rate of convergence for the exponential-Euler scheme when the drift is continuous. When the drift is discontinuous, the convergence rate is penalised by a factor {$\\epsilon$} decreasing with the time-step. We examine the case of the diffusion coefficient vanishing at zero, which adds a positivity preservation condition and a convergence analysis that exploits the negative moments and exponential moments of the scheme with the help of change of time technique introduced in Berkaoui et al. (2008). Asymptotic behaviour and theoretical stability of the exponential scheme, as well as numerical experiments, are also presented.",
        "citation_title": "Strong convergence of the exponential Euler scheme for SDEs with superlinear growth coefficients and one-sided Lipschitz drift",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The list-labeling problem captures the basic task of storing a dynamically changing set of up to $n$ elements in sorted order in an array of size $m = (1 + \\Theta(1))n$. The goal is to support insertions and deletions while moving around elements within the array as little as possible.\nUntil recently, the best known upper bound stood at $O(\\log^2 n)$ amortized cost. This bound, which was first established in 1981, was finally improved two years ago, when a randomized $O(\\log^{3/2} n)$ expected-cost algorithm was discovered. The best randomized lower bound for this problem remains $\\Omega(\\log n)$, and closing this gap is considered to be a major open problem in data structures.\nIn this paper, we present the See-Saw Algorithm, a randomized list-labeling solution that achieves a nearly optimal bound of $O(\\log n \\operatorname{polyloglog} n)$ amortized expected cost. This bound is achieved despite at least three lower bounds showing that this type of result is impossible for large classes of solutions.",
        "citation_title": "Nearly Optimal List Labeling",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper introduces ReeSPOT, a novel Reeb graph-based method to model patterns of life in human trajectories (akin to a fingerprint). Human behavior typically follows a pattern of normalcy in day-to-day activities. This is marked by recurring activities within specific time periods. In this paper, we model this behavior using Reeb graphs where any deviation from usual day-to-day activities is encoded as nodes in the Reeb graph. The complexity of the proposed algorithm is linear with respect to the number of time points in a given trajectory. We demonstrate the usage of ReeSPOT and how it captures the critically significant spatial and temporal deviations using the nodes of the Reeb graph. Our case study presented in this paper includes realistic human movement scenarios: visiting uncommon locations, taking odd routes at infrequent times, uncommon time visits, and uncommon stay durations. We analyze the Reeb graph to interpret the topological structure of the GPS trajectories. Potential applications of ReeSPOT include urban planning, security surveillance, and behavioral research.",
        "citation_title": "ReeSPOT: Reeb Graph Models Semantic Patterns of Normalcy in Human Trajectories",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Computational electromagnetics (CEM) is employed to numerically solve Maxwell's equations, and it has very important and practical applications across a broad range of disciplines, including biomedical engineering, nanophotonics, wireless communications, and electrodynamics. The main limitation of existing CEM methods is that they are computationally demanding. Our work introduces a leap forward in scientific computing and CEM by proposing an original solution of Maxwell's equations that is grounded on graph neural networks (GNNs) and enables the high-performance numerical resolution of these fundamental mathematical expressions. Specifically, we demonstrate that the update equations derived by discretizing Maxwell's partial differential equations can be innately expressed as a two-layer GNN with static and pre-determined edge weights. Given this intuition, a straightforward way to numerically solve Maxwell's equations entails simple message passing between such a GNN's nodes, yielding a significant computational time gain, while preserving the same accuracy as conventional transient CEM methods. Ultimately, our work supports the efficient and precise emulation of electromagnetic wave propagation with GNNs, and more importantly, we anticipate that applying a similar treatment to systems of partial differential equations arising in other scientific disciplines, e.g., computational fluid dynamics, can benefit computational sciences",
        "citation_title": "Solving Maxwell's equations with Non-Trainable Graph Neural Network Message Passing",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present extended Galerkin neural networks (xGNN), a variational framework for approximating general boundary value problems (BVPs) with error control. The main contributions of this work are (1) a rigorous theory guiding the construction of new weighted least squares variational formulations suitable for use in neural network approximation of general BVPs (2) an ``extended'' feedforward network architecture which incorporates and is even capable of learning singular solution structures, thus greatly improving approximability of singular solutions. Numerical results are presented for several problems including steady Stokes flow around re-entrant corners and in convex corners with Moffatt eddies in order to demonstrate efficacy of the method.",
        "citation_title": "Extended Galerkin neural network approximation of singular variational problems with error control",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Research data sets are growing to unprecedented sizes and network modeling is commonly used to extract complex relationships in diverse domains, such as genetic interactions involved in disease, logistics, and social communities. As the number of nodes increases in a network, an increasing sparsity of edges is a practical limitation due to memory restrictions. Moreover, many of these sparse networks exhibit very large numbers of nodes with no adjacent edges, as well as disjoint components of nodes with no edges connecting them. A prevalent aim in network modeling is the identification of clusters, or communities, of nodes that are highly interrelated. Several definitions of strong community structure have been introduced to facilitate this task, each with inherent assumptions and biases. We introduce an intuitive objective function for quantifying the quality of clustering results in large sparse networks. We utilize a two-step method for identifying communities which is especially well-suited for this domain as the first step efficiently divides the network into the disjoint components, while the second step optimizes clustering of the produced components based on the new objective. Using simulated networks, optimization based on the new objective function consistently yields significantly higher accuracy than those based on the modularity function, with the widest gaps appearing for the noisiest networks. Additionally, applications to benchmark problems illustrate the intuitive correctness of our approach. Finally, the practicality of our approach is demonstrated in real-world data in which we identify complex genetic interactions in large-scale networks comprised of tens of thousands of nodes. Based on these three different types of trials, our results clearly demonstrate the usefulness of our two-step procedure and the accuracy of our simple objective.",
        "citation_title": "Sifting out communities in large sparse networks",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In this paper we look at $k$-stroll, point-to-point orienteering, as well as the deadline TSP problem on graphs with bounded doubling dimension and bounded treewidth and present approximation schemes for them. Given a weighted graph $G=(V,E)$, start node $s\\in V$, distances $d:E\\rightarrow \\mathbb{Q}^+$ and integer $k$. In the $k$-stroll problem the goal is to find a path starting at $s$ of minimum length that visits at least $k$ vertices. The dual problem to $k$-stroll is the rooted orienteering in which instead of $k$ we are given a budget $B$ and the goal is to find a walk of length at most $B$ starting at $s$ that visits as many vertices as possible. In the P2P orienteering we are given start and end nodes $s,t$ for the path. In the deadline TSP we are given a deadline $D(v)$ for each $v\\in V$ and the goal is to find a walk starting at $s$ that visits as many vertices as possible before their deadline. The best approximation for rooted or P2P orienteering is $(2+\\epsilon)$-approximation [12] and $O(\\log n)$-approximation for deadline TSP [3]. There is no known approximation scheme for deadline TSP for any metric (not even trees). Our main result is the first approximation scheme for deadline TSP on metrics with bounded doubling dimension. To do so we first show if $G$ is a metric with doubling dimension $\\kappa$ and aspect ratio $\\Delta$, there is a $(1+\\epsilon)$-approximation that runs in time $n^{O\\left(\\left(\\log\\Delta/\\epsilon\\right)^{2\\kappa+1}\\right)}$. We then extend these to obtain an approximation scheme for deadline TSP when the distances and deadlines are integer which runs in time $n^{O\\left(\\left(\\log \\Delta/\\epsilon\\right)^{2\\kappa+2}\\right)}$. For graphs with treewidth $\\omega$ we show how to solve $k$-stroll and P2P orienteering exactly in polynomial time and a $(1+\\epsilon)$-approximation for deadline TSP in time $n^{O((\\omega\\log\\Delta/\\epsilon)^2)}$.",
        "citation_title": "Approximation Schemes for Orienteering and Deadline TSP in Doubling Metrics",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We introduce RatchetEHR, a novel transformer-based framework designed for the predictive analysis of electronic health records (EHR) data in intensive care unit (ICU) settings, with a specific focus on bloodstream infection (BSI) prediction. Leveraging the MIMIC-IV dataset, RatchetEHR demonstrates superior predictive performance compared to other methods, including RNN, LSTM, and XGBoost, particularly due to its advanced handling of sequential and temporal EHR data. A key innovation in RatchetEHR is the integration of the Graph Convolutional Transformer (GCT) component, which significantly enhances the ability to identify hidden structural relationships within EHR data, resulting in more accurate clinical predictions. Through SHAP value analysis, we provide insights into influential features for BSI prediction. RatchetEHR integrates multiple advancements in deep learning which together provide accurate predictions even with a relatively small sample size and highly imbalanced dataset. This study contributes to medical informatics by showcasing the application of advanced AI techniques in healthcare and sets a foundation for further research to optimize these capabilities in EHR data analysis.",
        "citation_title": "ICU Bloodstream Infection Prediction: A Transformer-Based Approach for EHR Analysis",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Machine learning (ML) techniques have been applied to high-level synthesis (HLS) flows for quality-of-result (QoR) prediction and design space exploration (DSE). Nevertheless, the scarcity of accessible high-quality HLS datasets and the complexity of building such datasets present challenges. Existing datasets have limitations in terms of benchmark coverage, design space enumeration, vendor extensibility, or lack of reproducible and extensible software for dataset construction. Many works also lack user-friendly ways to add more designs, limiting wider adoption of such datasets.\nIn response to these challenges, we introduce HLSFactory, a comprehensive framework designed to facilitate the curation and generation of high-quality HLS design datasets. HLSFactory has three main stages: 1) a design space expansion stage to elaborate single HLS designs into large design spaces using various optimization directives across multiple vendor tools, 2) a design synthesis stage to execute HLS and FPGA tool flows concurrently across designs, and 3) a data aggregation stage for extracting standardized data into packaged datasets for ML usage. This tripartite architecture ensures broad design space coverage via design space expansion and supports multiple vendor tools. Users can contribute to each stage with their own HLS designs and synthesis results and extend the framework itself with custom frontends and tool flows. We also include an initial set of built-in designs from common HLS benchmarks curated open-source HLS designs.\nWe showcase the versatility and multi-functionality of our framework through six case studies: I) Design space sampling; II) Fine-grained parallelism backend speedup; III) Targeting Intel's HLS flow; IV) Adding new auxiliary designs; V) Integrating published HLS data; VI) HLS tool version regression benchmarking.\nCode at this https URL.",
        "citation_title": "HLSFactory: A Framework Empowering High-Level Synthesis Datasets for Machine Learning and Beyond",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The behavior and decision making of groups or communities can be dramatically influenced by individuals pushing particular agendas, e.g., to promote or disparage a person or an activity, to call for action, etc.. In the examination of online influence campaigns, particularly those related to important political and social events, scholars often concentrate on identifying the sources responsible for setting and controlling the agenda (e.g., public media). In this article we present a methodology for detecting specific instances of agenda control through social media where annotated data is limited or non-existent. By using a modest corpus of Twitter messages centered on the 2022 French Presidential Elections, we carry out a comprehensive evaluation of various approaches and techniques that can be applied to this problem. Our findings demonstrate that by treating the task as a textual entailment problem, it is possible to overcome the requirement for a large annotated training dataset.",
        "citation_title": "Uncovering Agendas: A Novel French & English Dataset for Agenda Detection on Social Media",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Safe control for dynamical systems is critical, yet the presence of unknown dynamics poses significant challenges. In this paper, we present a learning-based control approach for tracking control of a class of high-order systems, operating under the constraint of partially observable states. The uncertainties inherent within the systems are modeled by kernel ridge regression, leveraging the proposed strategic data acquisition approach with limited state measurements. To achieve accurate trajectory tracking, a state observer that seamlessly integrates with the control law is devised. The analysis of the guaranteed control performance is conducted using Lyapunov theory due to the deterministic prediction error bound of kernel ridge regression, ensuring the adaptability of the approach in safety-critical scenarios. To demonstrate the effectiveness of our proposed approach, numerical simulations are performed, underscoring its contributions to the advancement of control strategies.",
        "citation_title": "Kernel-based Learning for Safe Control of Discrete-Time Unknown Systems under Incomplete Observations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We introduce WorkBench: a benchmark dataset for evaluating agents' ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents' errors can result in the wrong action being taken, such as an email being sent to the wrong person. WorkBench reveals weaknesses in agents' ability to undertake common business activities, raising questions about their use in high-stakes workplace settings. WorkBench is publicly available as a free resource at this https URL.",
        "citation_title": "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Conventional recommendation systems (RSs) are typically optimized to enhance performance metrics uniformly across all training samples.\nThis makes it hard for data-driven RSs to cater to a diverse set of users due to the varying properties of these users. The performance disparity among various populations can harm the model's robustness with respect to sub-populations. While recent works have shown promising results in adapting large language models (LLMs) for recommendation to address hard samples, long user queries from millions of users can degrade the performance of LLMs and elevate costs, processing times and inference latency. This challenges the practical applicability of LLMs for recommendations. To address this, we propose a hybrid task allocation framework that utilizes the capabilities of both LLMs and traditional RSs. By adopting a two-phase approach to improve robustness to sub-populations, we promote a strategic assignment of tasks for efficient and responsible adaptation of LLMs. Our strategy works by first identifying the weak and inactive users that receive a suboptimal ranking performance by RSs. Next, we use an in-context learning approach for such users, wherein each user interaction history is contextualized as a distinct ranking task and given to an LLM. We test our hybrid framework by incorporating various recommendation algorithms -- collaborative filtering and learning-to-rank recommendation models -- and two LLMs -- both open and close-sourced. Our results on three real-world datasets show a significant reduction in weak users and improved robustness of RSs to sub-populations $(\\approx12\\%)$ and overall performance without disproportionately escalating costs.",
        "citation_title": "Efficient and Responsible Adaptation of Large Language Models for Robust Top-k Recommendations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study the complexity of fundamental distributed graph problems in the recently popular setting where information about the input graph is available to the nodes before the start of the computation. We focus on the most common such setting, known as the Supported LOCAL model, where the input graph (on which the studied graph problem has to be solved) is guaranteed to be a subgraph of the underlying communication network.\nBuilding on a successful lower bound technique for the LOCAL model called round elimination, we develop a framework for proving complexity lower bounds in the stronger Supported LOCAL model. Our framework reduces the task of proving a (deterministic or randomized) lower bound for a given problem $\\Pi$ to the graph-theoretic task of proving non-existence of a solution to another problem $\\Pi'$ (on a suitable graph) that can be derived from $\\Pi$ in a mechanical manner.\nWe use the developed framework to obtain substantial (and, in the majority of cases, asymptotically tight) Supported LOCAL lower bounds for a variety of fundamental graph problems, including maximal matching, maximal independent set, ruling sets, arbdefective colorings, and generalizations thereof. In a nutshell, for essentially any major lower bound proved in the LOCAL model in recent years, we prove a similar lower bound in the Supported LOCAL model.\nOur framework also gives rise to a new deterministic version of round elimination in the LOCAL model: while, previous to our work, the general round elimination technique required the use of randomness (even for obtaining deterministic lower bounds), our framework allows to obtain deterministic (and therefore via known lifting techniques also randomized) lower bounds in a purely deterministic manner. Previously, such a purely deterministic application of round elimination was only known for the specific problem of sinkless orientation [SOSA'23].",
        "citation_title": "Tight Lower Bounds in the Supported LOCAL Model",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper considers the existence of short synchronizing words in deterministic finite automata (DFAs). In particular, we define a general strategy, which we call the \\emph{cornering strategy}, for generating short synchronizing words in well-structured DFAs. We show that a DFA is synchronizable if and only if this strategy can be applied.\nUsing the cornering strategy, we prove that all DFAs consisting of $n$ points in $\\mathbb{R}^d$ with bidirectional connected edge sets in which each edge $(\\mb x, \\mb y)$ is labeled $\\mb y - \\mb x$ are synchronizable. We also give sufficient conditions for such DFAs to have synchronizing words of length at most $(n-1)^2$ and thereby satisfy \u010cern\u00fd's conjecture. Using similar ideas, we generalise a result of Ananichev and Volkov \\cite{ananichev2004synchronizing} from monotonic automata to a wider class of DFAs admitting well-behaved partial orders. Finally, we consider how the cornering strategy can be applied to the problem of simultaneously synchronizing a DFA $G$ to an initial state $u$ and a DFA $H$ to an initial state $v$. We do not assume that DFAs $G$ and $H$ or states $u$ and $v$ are related beyond sharing the same edge labels.",
        "citation_title": "Cornering Robots to Synchronize a DFA",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We propose WIBA, a novel framework and suite of methods that enable the comprehensive understanding of \"What Is Being Argued\" across contexts. Our approach develops a comprehensive framework that detects: (a) the existence, (b) the topic, and (c) the stance of an argument, correctly accounting for the logical dependence among the three tasks. Our algorithm leverages the fine-tuning and prompt-engineering of Large Language Models. We evaluate our approach and show that it performs well in all the three capabilities. First, we develop and release an Argument Detection model that can classify a piece of text as an argument with an F1 score between 79% and 86% on three different benchmark datasets. Second, we release a language model that can identify the topic being argued in a sentence, be it implicit or explicit, with an average similarity score of 71%, outperforming current naive methods by nearly 40%. Finally, we develop a method for Argument Stance Classification, and evaluate the capability of our approach, showing it achieves a classification F1 score between 71% and 78% across three diverse benchmark datasets. Our evaluation demonstrates that WIBA allows the comprehensive understanding of What Is Being Argued in large corpora across diverse contexts, which is of core interest to many applications in linguistics, communication, and social and computer science. To facilitate accessibility to the advancements outlined in this work, we release WIBA as a free open access platform (wiba.dev).",
        "citation_title": "WIBA: What Is Being Argued? A Comprehensive Approach to Argument Mining",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Digital phased arrays have often been disregarded for millimeter-wave communications since the analog-to-digital converters (ADCs) are power-hungry. In this paper, we provide a different perspective on this matter by demonstrating analytically and numerically how the ADC resolution can be reduced when using digital phased arrays. We perform a theoretical analysis of the quantization noise characteristics for an OFDM signal received and processed by a digital phased array, using Gaussian approximation of the OFDM signal. In particular, we quantify the quantization noise suppression factor analytically and numerically. This factor describes how much the coherent combining reduces the quantization noise as a function of the number of antennas, which allows for reducing the ADC bit resolution. For instance in a 8-16 antenna digital phased array the ADC resolution can be reduced with 1-2 bits compared to the ADC required for an analog phased array.",
        "citation_title": "Analysis of Quantization Noise Suppression Gains in Digital Phased Arrays",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Algorithm design is a vital skill developed in most undergraduate Computer Science (CS) programs, but few research studies focus on pedagogy related to algorithms coursework. To understand the work that has been done in the area, we present a systematic survey and literature review of CS Education studies. We search for research that is both related to algorithm design and evaluated on undergraduate-level students. Across all papers in the ACM Digital Library prior to August 2023, we only find 94 such papers.\nWe first classify these papers by topic, evaluation metric, evaluation methods, and intervention target. Through our classification, we find a broad sparsity of papers which indicates that many open questions remain about teaching algorithm design, with each algorithm topic only being discussed in between 0 and 10 papers. We also note the need for papers using rigorous research methods, as only 38 out of 88 papers presenting quantitative data use statistical tests, and only 15 out of 45 papers presenting qualitative data use a coding scheme. Only 17 papers report controlled trials.\nWe then synthesize the results of the existing literature to give insights into what the corpus reveals about how we should teach algorithms. Much of the literature explores implementing well-established practices, such as active learning or automated assessment, in the algorithms classroom. However, there are algorithms-specific results as well: a number of papers find that students may under-utilize certain algorithmic design techniques, and studies describe a variety of ways to select algorithms problems that increase student engagement and learning.\nThe results we present, along with the publicly available set of papers collected, provide a detailed representation of the current corpus of CS Education work related to algorithm design and can orient further research in the area.",
        "citation_title": "Teaching Algorithm Design: A Literature Review",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Linear representation learning is widely studied due to its conceptual simplicity and empirical utility in tasks such as compression, classification, and feature extraction. Given a set of points $[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n] = \\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and a vector $\\mathbf{y} \\in \\mathbb{R}^d$, the goal is to find coefficients $\\mathbf{w} \\in \\mathbb{R}^n$ so that $\\mathbf{X} \\mathbf{w} \\approx \\mathbf{y}$, subject to some desired structure on $\\mathbf{w}$. In this work we seek $\\mathbf{w}$ that forms a local reconstruction of $\\mathbf{y}$ by solving a regularized least squares regression problem. We obtain local solutions through a locality function that promotes the use of columns of $\\mathbf{X}$ that are close to $\\mathbf{y}$ when used as a regularization term. We prove that, for all levels of regularization and under a mild condition that the columns of $\\mathbf{X}$ have a unique Delaunay triangulation, the optimal coefficients' number of non-zero entries is upper bounded by $d+1$, thereby providing local sparse solutions when $d \\ll n$. Under the same condition we also show that for any $\\mathbf{y}$ contained in the convex hull of $\\mathbf{X}$ there exists a regime of regularization parameter such that the optimal coefficients are supported on the vertices of the Delaunay simplex containing $\\mathbf{y}$. This provides an interpretation of the sparsity as having structure obtained implicitly from the Delaunay triangulation of $\\mathbf{X}$. We demonstrate that our locality regularized problem can be solved in comparable time to other methods that identify the containing Delaunay simplex.",
        "citation_title": "Locality Regularized Reconstruction: Structured Sparsity and Delaunay Triangulations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Decentralized Multi-agent Learning (DML) enables collaborative model training while preserving data privacy. However, inherent heterogeneity in agents' resources (computation, communication, and task size) may lead to substantial variations in training time. This heterogeneity creates a bottleneck, lengthening the overall training time due to straggler effects and potentially wasting spare resources of faster agents. To minimize training time in heterogeneous environments, we present a Communication-Efficient Training Workload Balancing for Decentralized Multi-Agent Learning (ComDML), which balances the workload among agents through a decentralized approach. Leveraging local-loss split training, ComDML enables parallel updates, where slower agents offload part of their workload to faster agents. To minimize the overall training time, ComDML optimizes the workload balancing by jointly considering the communication and computation capacities of agents, which hinges upon integer programming. A dynamic decentralized pairing scheduler is developed to efficiently pair agents and determine optimal offloading amounts. We prove that in ComDML, both slower and faster agents' models converge, for convex and non-convex functions. Furthermore, extensive experimental results on popular datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D. variants, with large models such as ResNet-56 and ResNet-110, demonstrate that ComDML can significantly reduce the overall training time while maintaining model accuracy, compared to state-of-the-art methods. ComDML demonstrates robustness in heterogeneous environments, and privacy measures can be seamlessly integrated for enhanced data protection.",
        "citation_title": "Communication-Efficient Training Workload Balancing for Decentralized Multi-Agent Learning",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In this paper, we present Sim-Grasp, a robust 6-DOF two-finger grasping system that integrates advanced language models for enhanced object manipulation in cluttered environments. We introduce the Sim-Grasp-Dataset, which includes 1,550 objects across 500 scenarios with 7.9 million annotated labels, and develop Sim-GraspNet to generate grasp poses from point clouds. The Sim-Grasp-Polices achieve grasping success rates of 97.14% for single objects and 87.43% and 83.33% for mixed clutter scenarios of Levels 1-2 and Levels 3-4 objects, respectively. By incorporating language models for target identification through text and box prompts, Sim-Grasp enables both object-agnostic and target picking, pushing the boundaries of intelligent robotic systems.",
        "citation_title": "Sim-Grasp: Learning 6-DOF Grasp Policies for Cluttered Environments Using a Synthetic Benchmark",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The prevalence of unwarranted beliefs, spanning pseudoscience, logical fallacies, and conspiracy theories, presents substantial societal hurdles and the risk of disseminating misinformation. Utilizing established psychometric assessments, this study explores the capabilities of large language models (LLMs) vis-a-vis the average human in detecting prevalent logical pitfalls. We undertake a philosophical inquiry, juxtaposing the rationality of humans against that of LLMs. Furthermore, we propose methodologies for harnessing LLMs to counter misconceptions, drawing upon psychological models of persuasion such as cognitive dissonance theory and elaboration likelihood theory. Through this endeavor, we highlight the potential of LLMs as personalized misinformation debunking agents.",
        "citation_title": "Can a Hallucinating Model help in Reducing Human \"Hallucination\"?",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The full realization of smart city technology is dependent on the secure and honest collaboration between IoT applications and edge-computing. In particular, resource constrained IoT devices may rely on fog-computing to alleviate the computing load of IoT tasks. Mutual authentication is needed between IoT and fog to preserve IoT data security, and monetization of fog services is needed to promote the fog service ecosystem. However, there is no guarantee that fog nodes will always respond to IoT requests correctly, either intentionally or accidentally. In the public decentralized IoT-fog environment, it is crucial to enforce integrity among fog nodes. In this paper, we propose a blockchain-based system that 1) streamlines the mutual authentication service monetization between IoT and fog, 2) verifies the integrity of fog nodes via service audits, and 3) discourages malicious activity and promotes honesty among fog nodes through incentives and penalties.",
        "citation_title": "A Blockchain-Based Audit Mechanism for Trust and Integrity in IoT-Fog Environments",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Ensuring the safe operation of legged robots in uncertain, novel environments is crucial to their widespread adoption. Despite recent advances in safety filters that can keep arbitrary task-driven policies from incurring safety failures, existing solutions for legged robot locomotion still rely on simplified dynamics and may fail when the robot is perturbed away from predefined stable gaits. This paper presents a general approach that leverages offline game-theoretic reinforcement learning to synthesize a highly robust safety filter for high-order nonlinear dynamics. This gameplay filter then maintains runtime safety by continually simulating adversarial futures and precluding task-driven actions that would cause it to lose future games (and thereby violate safety). Validated on a 36-dimensional quadruped robot locomotion task, the gameplay safety filter exhibits inherent robustness to the sim-to-real gap without manual tuning or heuristic designs. Physical experiments demonstrate the effectiveness of the gameplay safety filter under perturbations, such as tugging and unmodeled irregular terrains, while simulation studies shed light on how to trade off computation and conservativeness without compromising safety.",
        "citation_title": "Gameplay Filters: Safe Robot Walking through Adversarial Imagination",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study the problem of learning a binary classifier on the vertices of a graph. In particular, we consider classifiers given by monophonic halfspaces, partitions of the vertices that are convex in a certain abstract sense. Monophonic halfspaces, and related notions such as geodesic halfspaces,have recently attracted interest, and several connections have been drawn between their properties(e.g., their VC dimension) and the structure of the underlying graph $G$. We prove several novel results for learning monophonic halfspaces in the supervised, online, and active settings. Our main result is that a monophonic halfspace can be learned with near-optimal passive sample complexity in time polynomial in $n = |V(G)|$. This requires us to devise a polynomial-time algorithm for consistent hypothesis checking, based on several structural insights on monophonic halfspaces and on a reduction to $2$-satisfiability. We prove similar results for the online and active settings. We also show that the concept class can be enumerated with delay $\\operatorname{poly}(n)$, and that empirical risk minimization can be performed in time $2^{\\omega(G)}\\operatorname{poly}(n)$ where $\\omega(G)$ is the clique number of $G$. These results answer open questions from the literature (Gonz\u00e1lez et al., 2020), and show a contrast with geodesic halfspaces, for which some of the said problems are NP-hard (Seiffarth et al., 2023).",
        "citation_title": "Efficient Algorithms for Learning Monophonic Halfspaces in Graphs",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Differences in image quality, lighting conditions, and patient demographics pose challenges to automated glaucoma detection from color fundus photography. Brighteye, a method based on Vision Transformer, is proposed for glaucoma detection and glaucomatous feature classification. Brighteye learns long-range relationships among pixels within large fundus images using a self-attention mechanism. Prior to being input into Brighteye, the optic disc is localized using YOLOv8, and the region of interest (ROI) around the disc center is cropped to ensure alignment with clinical practice. Optic disc detection improves the sensitivity at 95% specificity from 79.20% to 85.70% for glaucoma detection and the Hamming distance from 0.2470 to 0.1250 for glaucomatous feature classification. In the developmental stage of the Justified Referral in AI Glaucoma Screening (JustRAIGS) challenge, the overall outcome secured the fifth position out of 226 entries.",
        "citation_title": "Brighteye: Glaucoma Screening with Color Fundus Photographs based on Vision Transformer",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations. Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification. The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space. Results: ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%. The use of a triplet loss function reduces overfitting in the distance-based classifier. Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes.",
        "citation_title": "Guided Conditional Diffusion Classifier (ConDiff) for Enhanced Prediction of Infection in Diabetic Foot Ulcers",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The belief that AI technology is on the cusp of causing a generalized social crisis became a popular one in 2023. Interestingly, some of these worries were voiced from within the tech sector itself. While there was no doubt an element of hype and exaggeration to some of these accounts, they do reflect the fact that there are troubling ramifications to this technology stack. This conjunction of shared concerns about social, political, and personal futures presaged by current developments in machine learning and data science presents the academic discipline of computing with a rare opportunity for self-examination and reconfiguration. This position paper endeavors to do so in four sections. The first expands on the nature of the AI crisis for computing. The second articulates possible critical responses to this crisis and advocates for a broader analytic focus on power relations. The third section presents a novel characterization of academic computing's epistemological field, one which includes not only the discipline's usual instrumental forms of knowledge but reflexive knowledge as well. This reflexive dimension integrates both the critical and public functions of the discipline as equal intellectual partners and a necessary component of any contemporary academic field. The final section will advocate for a conceptual archetype--the Public Computer Intellectual--as a way of practically imagining the expanded possibilities of academic practice in our discipline, one that provides both self-critique and an outward-facing orientation towards the public good. It will argue that the computer education research community can play a vital role in this regard.",
        "citation_title": "Public Computing Intellectuals in the Age of AI Crisis",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Multiple choice questions (MCQs) are a popular method for evaluating students' knowledge due to their efficiency in administration and grading. Crafting high-quality math MCQs is a labor-intensive process that requires educators to formulate precise stems and plausible distractors. Recent advances in large language models (LLMs) have sparked interest in automating MCQ creation, but challenges persist in ensuring mathematical accuracy and addressing student errors. This paper introduces a prototype tool designed to facilitate collaboration between LLMs and educators for streamlining the math MCQ generation process. We conduct a pilot study involving math educators to investigate how the tool can help them simplify the process of crafting high-quality math MCQs. We found that while LLMs can generate well-formulated question stems, their ability to generate distractors that capture common student errors and misconceptions is limited. Nevertheless, a human-AI collaboration has the potential to enhance the efficiency and effectiveness of MCQ generation.",
        "citation_title": "Math Multiple Choice Question Generation via Human-Large Language Model Collaboration",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The use of steganography to transmit secret data is becoming increasingly common in security products and malware today. Despite being extremely popular, PDF files are not often the focus of steganography research, as most applications utilize digital image, audio, and video files as their cover data. However, the PDF file format is promising for usage in medium-capacity steganography applications. In this paper, we present a novel PDF steganography algorithm based upon least-significant bit insertion into the real-valued operands of PDF stream operators. Where prior research has only considered a small subset of these operators, we take an extensive look at all the possible operators defined in the Adobe PDF standard to evaluate their usability in our steganography algorithm. We also provide a case study which embeds malware into a given cover PDF document.",
        "citation_title": "Hiding Sensitive Information Using PDF Steganography",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present a fast trajectory optimization algorithm for the soft capture of uncooperative tumbling space objects. Our algorithm generates safe, dynamically feasible, and minimum-fuel trajectories for a six-degree-of-freedom servicing spacecraft to achieve soft capture (near-zero relative velocity at contact) between predefined locations on the servicer spacecraft and target body. We solve a convex problem by enforcing a convex relaxation of the field-of-view constraint, followed by a sequential convex program correcting the trajectory for collision avoidance. The optimization problems can be solved with a standard second-order cone programming solver, making the algorithm both fast and practical for implementation in flight software. We demonstrate the performance and robustness of our algorithm in simulation over a range of object tumble rates up to 10\u00b0/s.",
        "citation_title": "A Convex Formulation of the Soft-Capture Problem",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The growing scale and complexity of safety-critical control systems underscore the need to evolve current control architectures aiming for the unparalleled performances achievable through state-of-the-art optimization and machine learning algorithms. However, maintaining closed-loop stability while boosting the performance of nonlinear control systems using data-driven and deep-learning approaches stands as an important unsolved challenge. In this paper, we tackle the performance-boosting problem with closed-loop stability guarantees. Specifically, we establish a synergy between the Internal Model Control (IMC) principle for nonlinear systems and state-of-the-art unconstrained optimization approaches for learning stable dynamics. Our methods enable learning over arbitrarily deep neural network classes of performance-boosting controllers for stable nonlinear systems; crucially, we guarantee Lp closed-loop stability even if optimization is halted prematurely, and even when the ground-truth dynamics are unknown, with vanishing conservatism in the class of stabilizing policies as the model uncertainty is reduced to zero. We discuss the implementation details of the proposed control schemes, including distributed ones, along with the corresponding optimization procedures, demonstrating the potential of freely shaping the cost functions through several numerical experiments.",
        "citation_title": "Learning to Boost the Performance of Stable Nonlinear Systems",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Automated software testing is integral to the software development process, streamlining workflows and ensuring product reliability. Visual testing within this context, especially concerning user interface (UI) and user experience (UX) validation, stands as one of crucial determinants of overall software quality. Nevertheless, conventional methods like pixel-wise comparison and region-based visual change detection fall short in capturing contextual similarities, nuanced alterations, and understanding the spatial relationships between UI elements. In this paper, we introduce a novel graph-based method for visual change detection in software test automation. Leveraging a machine learning model, our method accurately identifies UI controls from software screenshots and constructs a graph representing contextual and spatial relationships between the controls. This information is then used to find correspondence between UI controls within screenshots of different versions of a software. The resulting graph encapsulates the intricate layout of the UI and underlying contextual relations, providing a holistic and context-aware model. This model is finally used to detect and highlight visual regressions in the UI. Comprehensive experiments on different datasets showed that our change detector can accurately detect visual software changes in various simple and complex test scenarios. Moreover, it outperformed pixel-wise comparison and region-based baselines by a large margin in more complex testing scenarios. This work not only contributes to the advancement of visual change detection but also holds practical implications, offering a robust solution for real-world software test automation challenges, enhancing reliability, and ensuring the seamless evolution of software interfaces.",
        "citation_title": "Artificial intelligence for context-aware visual change detection in software test automation",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Vision language models (VLMs) have recently emerged and gained the spotlight for their ability to comprehend the dual modality of image and textual data. VLMs such as LLaVA, ChatGPT-4, and Gemini have recently shown impressive performance on tasks such as natural image captioning, visual question answering (VQA), and spatial reasoning. Additionally, a universal segmentation model by Meta AI, Segment Anything Model (SAM) shows unprecedented performance at isolating objects from unforeseen images. Since medical experts, biologists, and materials scientists routinely examine microscopy or medical images in conjunction with textual information in the form of captions, literature, or reports, and draw conclusions of great importance and merit, it is indubitably essential to test the performance of VLMs and foundation models such as SAM, on these images. In this study, we charge ChatGPT, LLaVA, Gemini, and SAM with classification, segmentation, counting, and VQA tasks on a variety of microscopy images. We observe that ChatGPT and Gemini are impressively able to comprehend the visual features in microscopy images, while SAM is quite capable at isolating artefacts in a general sense. However, the performance is not close to that of a domain expert - the models are readily encumbered by the introduction of impurities, defects, artefact overlaps and diversity present in the images.",
        "citation_title": "Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Discounted algorithms often encounter evaluation errors due to their reliance on short-term estimations, which can impede their efficacy in addressing simple, short-term tasks and impose undesired temporal discounts (\\(\\gamma\\)). Interestingly, these algorithms are often tested without applying a discount, a phenomenon we refer as the \\textit{train-test bias}. In response to these challenges, we propose the Markov Flow Policy, which utilizes a non-negative neural network flow to enable comprehensive forward-view predictions. Through integration into the TD7 codebase and evaluation using the MuJoCo benchmark, we observe significant performance improvements, positioning MFP as a straightforward, practical, and easily implementable solution within the domain of average rewards algorithms.",
        "citation_title": "Markov flow policy -- deep MC",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We are witnessing a revolution in conditional image synthesis with the recent success of large scale text-to-image generation methods. This success also opens up new opportunities in controlling the generation and editing process using multi-modal input. While spatial control using cues such as depth, sketch, and other images has attracted a lot of research, we argue that another equally effective modality is audio since sound and sight are two main components of human perception. Hence, we propose a method to enable audio-conditioning in large scale image diffusion models. Our method first maps features obtained from audio clips to tokens that can be injected into the diffusion model in a fashion similar to text tokens. We introduce additional audio-image cross attention layers which we finetune while freezing the weights of the original layers of the diffusion model. In addition to audio conditioned image generation, our method can also be utilized in conjuction with diffusion based editing methods to enable audio conditioned image editing. We demonstrate our method on a wide range of audio and image datasets. We perform extensive comparisons with recent methods and show favorable performance.",
        "citation_title": "SonicDiffusion: Audio-Driven Image Generation and Editing with Pretrained Diffusion Models",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Scientists conduct large-scale simulations to compute derived quantities-of-interest (QoI) from primary data. Often, QoI are linked to specific features, regions, or time intervals, such that data can be adaptively reduced without compromising the integrity of QoI. For many spatiotemporal applications, these QoI are binary in nature and represent presence or absence of a physical phenomenon. We present a pipelined compression approach that first uses neural-network-based techniques to derive regions where QoI are highly likely to be present. Then, we employ a Guaranteed Autoencoder (GAE) to compress data with differential error bounds. GAE uses QoI information to apply low-error compression to only these regions. This results in overall high compression ratios while still achieving downstream goals of simulation or data collections. Experimental results are presented for climate data generated from the E3SM Simulation model for downstream quantities such as tropical cyclone and atmospheric river detection and tracking. These results show that our approach is superior to comparable methods in the literature.",
        "citation_title": "Machine Learning Techniques for Data Reduction of Climate Applications",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper investigates the differentiable dynamic modeling of mobile manipulators to facilitate efficient motion planning and physical design of actuators, where the actuator design is parameterized by physically meaningful motor geometry parameters. These parameters impact the manipulator's link mass, inertia, center-of-mass, torque constraints, and angular velocity constraints, influencing control authority in motion planning and trajectory tracking control. A motor's maximum torque/speed and how the design parameters affect the dynamics are modeled analytically, facilitating differentiable and analytical dynamic modeling. Additionally, an integrated locomotion and manipulation planning problem is formulated with direct collocation discretization, using the proposed differentiable dynamics and motor parameterization. Such dynamics are required to capture the dynamic coupling between the base and the manipulator. Numerical experiments demonstrate the effectiveness of differentiable dynamics in speeding up optimization and advantages in task completion time and energy consumption over established sequential motion planning approach. Finally, this paper introduces a simultaneous actuator design and motion planning framework, providing numerical results to validate the proposed differentiable modeling approach for co-design problems.",
        "citation_title": "A Differentiable Dynamic Modeling Approach to Integrated Motion Planning and Actuator Physical Design for Mobile Manipulators",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "As a popular distributed learning paradigm, federated learning (FL) over mobile devices fosters numerous applications, while their practical deployment is hindered by participating devices' computing and communication heterogeneity. Some pioneering research efforts proposed to extract subnetworks from the global model, and assign as large a subnetwork as possible to the device for local training based on its full computing and communications capacity. Although such fixed size subnetwork assignment enables FL training over heterogeneous mobile devices, it is unaware of (i) the dynamic changes of devices' communication and computing conditions and (ii) FL training progress and its dynamic requirements of local training contributions, both of which may cause very long FL training delay. Motivated by those dynamics, in this paper, we develop a wireless and heterogeneity aware latency efficient FL (WHALE-FL) approach to accelerate FL training through adaptive subnetwork scheduling. Instead of sticking to the fixed size subnetwork, WHALE-FL introduces a novel subnetwork selection utility function to capture device and FL training dynamics, and guides the mobile device to adaptively select the subnetwork size for local training based on (a) its computing and communication capacity, (b) its dynamic computing and/or communication conditions, and (c) FL training status and its corresponding requirements for local training contributions. Our evaluation shows that, compared with peer designs, WHALE-FL effectively accelerates FL training without sacrificing learning accuracy.",
        "citation_title": "WHALE-FL: Wireless and Heterogeneity Aware Latency Efficient Federated Learning over Mobile Devices via Adaptive Subnetwork Scheduling",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Traditional language models operate autoregressively, i.e., they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that reduce net inference times. Our models $\\textit{dynamically}$ predict multiple tokens based on their confidence in the predicted joint probability distribution. We propose a lightweight technique to train these models, leveraging the weights of traditional autoregressive counterparts. Moreover, we propose novel ways to enhance the estimated joint probability to improve text generation quality, namely co-occurrence weighted masking and adaptive thresholding. We also propose systematic qualitative and quantitative methods to rigorously test the quality of generated text for non-autoregressive generation. One of the models in our suite, DynaMo-7.3B-T3, achieves same-quality generated text as the baseline (Pythia-6.9B) while achieving 2.57$\\times$ speed-up with only 5.87% and 2.67% parameter and training time overheads, respectively.",
        "citation_title": "DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Machine learning applications on extremely low-power devices, commonly referred to as tiny machine learning (TinyML), promises a smarter and more connected world. However, the advancement of current TinyML research is hindered by the limited size and quality of pertinent datasets. To address this challenge, we introduce Wake Vision, a large-scale, diverse dataset tailored for person detection -- the canonical task for TinyML visual sensing. Wake Vision comprises over 6 million images, which is a hundredfold increase compared to the previous standard, and has undergone thorough quality filtering. Using Wake Vision for training results in a 2.41\\% increase in accuracy compared to the established benchmark. Alongside the dataset, we provide a collection of five detailed benchmark sets that assess model performance on specific segments of the test data, such as varying lighting conditions, distances from the camera, and demographic characteristics of subjects. These novel fine-grained benchmarks facilitate the evaluation of model quality in challenging real-world scenarios that are often ignored when focusing solely on overall accuracy. Through an evaluation of a MobileNetV2 TinyML model on the benchmarks, we show that the input resolution plays a more crucial role than the model width in detecting distant subjects and that the impact of quantization on model robustness is minimal, thanks to the dataset quality. These findings underscore the importance of a detailed evaluation to identify essential factors for model development. The dataset, benchmark suite, code, and models are publicly available under the CC-BY 4.0 license, enabling their use for commercial use cases.",
        "citation_title": "Wake Vision: A Large-scale, Diverse Dataset and Benchmark Suite for TinyML Person Detection",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Large language models appear quite creative, often performing on par with the average human on creative tasks. However, research on LLM creativity has focused solely on \\textit{products}, with little attention on the creative \\textit{process}. Process analyses of human creativity often require hand-coded categories or exploit response times, which do not apply to LLMs. We provide an automated method to characterise how humans and LLMs explore semantic spaces on the Alternate Uses Task, and contrast with behaviour in a Verbal Fluency Task. We use sentence embeddings to identify response categories and compute semantic similarities, which we use to generate jump profiles. Our results corroborate earlier work in humans reporting both persistent (deep search in few semantic spaces) and flexible (broad search across multiple semantic spaces) pathways to creativity, where both pathways lead to similar creativity scores. LLMs were found to be biased towards either persistent or flexible paths, that varied across tasks. Though LLMs as a population match human profiles, their relationship with creativity is different, where the more flexible models score higher on creativity. Our dataset and scripts are available on \\href{this https URL}{GitHub}.",
        "citation_title": "Characterising the Creative Process in Humans and Large Language Models",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Photorealistic simulation plays a crucial role in applications such as autonomous driving, where advances in neural radiance fields (NeRFs) may allow better scalability through the automatic creation of digital 3D assets. However, reconstruction quality suffers on street scenes due to largely collinear camera motions and sparser samplings at higher speeds. On the other hand, the application often demands rendering from camera views that deviate from the inputs to accurately simulate behaviors like lane changes. In this paper, we propose several insights that allow a better utilization of Lidar data to improve NeRF quality on street scenes. First, our framework learns a geometric scene representation from Lidar, which is fused with the implicit grid-based representation for radiance decoding, thereby supplying stronger geometric information offered by explicit point cloud. Second, we put forth a robust occlusion-aware depth supervision scheme, which allows utilizing densified Lidar points by accumulation. Third, we generate augmented training views from Lidar points for further improvement. Our insights translate to largely improved novel view synthesis under real driving scenes.",
        "citation_title": "DiL-NeRF: Delving into Lidar for Neural Radiance Field on Street Scenes",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "There are many different forms of design knowledge that guide and shape a designer's ability to act and realize potential realities. Methods and schemas are examples of design knowledge commonly used by design researchers and designers alike. In this pictorial, we explore, engage, and describe the role of schemas as tools that can support design researchers in formulating methods to support design action, with our framing of method design specifically focused on ethical design complexity. We present four ways for method designers to engage with schema: 1) Systems to operationalize complex design constructs such as ethical design complexity through an A.E.I.O.YOU schema; 2) Classifiers to map existing methods and identify the possibility for new methods through descriptive semantic differentials; 3) Tools that enable the creation of methods that relate to one or more elements of the schema through creative departures from research to design; and 4) Interactive channels to playfully engage potential and new opportunities through schema interactivity.",
        "citation_title": "Using Schema to Inform Method Design Practices",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Multi-agent reinforcement learning (MARL) algorithms often struggle to find strategies close to Pareto optimal Nash Equilibrium, owing largely to the lack of efficient exploration. The problem is exacerbated in sparse-reward settings, caused by the larger variance exhibited in policy learning. This paper introduces MESA, a novel meta-exploration method for cooperative multi-agent learning. It learns to explore by first identifying the agents' high-rewarding joint state-action subspace from training tasks and then learning a set of diverse exploration policies to \"cover\" the subspace. These trained exploration policies can be integrated with any off-policy MARL algorithm for test-time tasks. We first showcase MESA's advantage in a multi-step matrix game. Furthermore, experiments show that with learned exploration policies, MESA achieves significantly better performance in sparse-reward tasks in several multi-agent particle environments and multi-agent MuJoCo environments, and exhibits the ability to generalize to more challenging tasks at test time.",
        "citation_title": "MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Over the last decade, similar to other application domains, social media content has been proven very effective in disaster informatics. However, due to the unstructured nature of the data, several challenges are associated with disaster analysis in social media content. To fully explore the potential of social media content in disaster informatics, access to relevant content and the correct geo-location information is very critical. In this paper, we propose a three-step solution to tackling these challenges. Firstly, the proposed solution aims to classify social media posts into relevant and irrelevant posts followed by the automatic extraction of location information from the posts' text through Named Entity Recognition (NER) analysis. Finally, to quickly analyze the topics covered in large volumes of social media posts, we perform topic modeling resulting in a list of top keywords, that highlight the issues discussed in the tweet. For the Relevant Classification of Twitter Posts (RCTP), we proposed a merit-based fusion framework combining the capabilities of four different models namely BERT, RoBERTa, Distil BERT, and ALBERT obtaining the highest F1-score of 0.933 on a benchmark dataset. For the Location Extraction from Twitter Text (LETT), we evaluated four models namely BERT, RoBERTa, Distil BERTA, and Electra in an NER framework obtaining the highest F1-score of 0.960. For topic modeling, we used the BERTopic library to discover the hidden topic patterns in the relevant tweets. The experimental results of all the components of the proposed end-to-end solution are very encouraging and hint at the potential of social media content and NLP in disaster management.",
        "citation_title": "A Named Entity Recognition and Topic Modeling-based Solution for Locating and Better Assessment of Natural Disasters in Social Media",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Solutions to the governing partial differential equations obtained from a discrete numerical scheme can have significant errors, especially near shocks when the discrete representation of the solution cannot fully capture the discontinuity in the solution. A recent approach to shock tracking [1, 2] has been to implicitly align the faces of mesh elements with the shock, yielding accurate solutions on coarse meshes. In engineering applications, the solution field is often used to evaluate a scalar functional of interest, such as lift or drag over an airfoil. While functionals are sensitive to errors in the flow solution, certain regions in the domain are more important for accurate evaluation of the functional than the rest. Using this fact, we formulate a goal-oriented implicit shock tracking approach that captures a segment of the shock that is important for evaluating the functional. Shock tracking is achieved using Lagrange-Newton-Krylov-Schur (LNKS) full space optimizer, with the objective of minimizing the adjoint-weighted residual error indicator. We also present a method to evaluate the sensitivity and the Hessian of the functional error. Using available block preconditioners for LNKS [3, 4] makes the full space approach scalable. The method is applied to test cases of two-dimensional advection and inviscid compressible flows to demonstrate functional-dependent shock tracking. Tracking the entire shock without using artificial dissipation results in the error converging at the orders of $\\mathcal{O}(h^{p+1})$.",
        "citation_title": "Adjoint-based goal-oriented implicit shock tracking using full space mesh optimization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Vision transformers have revolutionized computer vision, but their computational demands present challenges for training and deployment. This paper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel method that leverages data lottery ticket selection and sparsity pruning to accelerate vision transformer training while maintaining accuracy. Our approach focuses on identifying and utilizing the most informative data subsets and eliminating redundant model parameters to optimize the training process. Through extensive experiments, we demonstrate the effectiveness of LOTUS in achieving rapid convergence and high accuracy with significantly reduced computational requirements. This work highlights the potential of combining data selection and sparsity techniques for efficient vision transformer training, opening doors for further research and development in this area.",
        "citation_title": "LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Background and Purpose: Identifying the thromboembolism source in ischemic stroke is crucial for treatment and secondary prevention yet is often undetermined. This study describes a self-supervised deep learning approach in digital pathology of emboli for classifying ischemic stroke clot origin from histopathological images. Methods: The dataset included whole slide images (WSI) from the STRIP AI Kaggle challenge, consisting of retrieved clots from ischemic stroke patients following mechanical thrombectomy. Transformer-based deep learning models were developed using transfer learning and self-supervised pretraining for classifying WSI. Customizations included an attention pooling layer, weighted loss function, and threshold optimization. Various model architectures were tested and compared, and model performances were primarily evaluated using weighted logarithmic loss. Results: The model achieved a logloss score of 0.662 in cross-validation and 0.659 on the test set. Different model backbones were compared, with the swin_large_patch4_window12_384 showed higher performance. Thresholding techniques for clot origin classification were employed to balance false positives and negatives. Conclusion: The study demonstrates the extent of efficacy of transformer-based deep learning models in identifying ischemic stroke clot origins from histopathological images and emphasizes the need for refined modeling techniques specifically adapted to thrombi WSI. Further research is needed to improve model performance, interpretability, validate its effectiveness. Future enhancement could include integrating larger patient cohorts, advanced preprocessing strategies, and exploring ensemble multimodal methods for enhanced diagnostic accuracy.",
        "citation_title": "Transformer-Based Self-Supervised Learning for Histopathological Classification of Ischemic Stroke Clot Origin",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantum Federated Learning (QFL) is an emerging concept that aims to unfold federated learning (FL) over quantum networks, enabling collaborative quantum model training along with local data privacy. We explore the challenges of deploying QFL on cloud platforms, emphasizing quantum intricacies and platform limitations. The proposed data-encoding-driven QFL, with a proof of concept (GitHub Open Source) using genomic data sets on quantum simulators, shows promising results.",
        "citation_title": "Quantum Federated Learning Experiments in the Cloud with Data Encoding",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Prediction models can improve efficiency by automating decisions such as the approval of loan applications. However, they may inherit bias against protected groups from the data they are trained on. This paper adds counterfactual (simulated) ethnic bias to real data on mortgage application decisions, and shows that this bias is replicated by a machine learning model (XGBoost) even when ethnicity is not used as a predictive variable. Next, several other de-biasing methods are compared: averaging over prohibited variables, taking the most favorable prediction over prohibited variables (a novel method), and jointly minimizing errors as well as the association between predictions and prohibited variables. De-biasing can recover some of the original decisions, but the results are sensitive to whether the bias is effected through a proxy.",
        "citation_title": "De-Biasing Models of Biased Decisions: A Comparison of Methods Using Mortgage Application Data",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Description Logics are a formalism used in the knowledge representation, where the knowledge is captured in the form of concepts constructed in a controlled way from a restricted vocabulary. This allows one to test effectively for consistency of and the subsumption between the concepts. Unification of concepts may likewise become a useful tool in analysing the relations between concepts. The unification problem has been solved for the description logics $\\mathcal{FL}_0$ and $\\mathcal{EL}$. These small logics do not provide any means to express negation. Here we show an algorithm solving unification in $\\mathcal{FL}_\\bot$, the logic that extends $\\mathcal{FL}_0$ with the bottom concept. Bottom allows one to express that two concepts are disjoint. Our algorithm runs in exponential time, with respect to the size of the problem.",
        "citation_title": "Unification in the description logic $\\mathcal{FL}_\\bot$",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present EchoScene, an interactive and controllable generative model that generates 3D indoor scenes on scene graphs. EchoScene leverages a dual-branch diffusion model that dynamically adapts to scene graphs. Existing methods struggle to handle scene graphs due to varying numbers of nodes, multiple edge combinations, and manipulator-induced node-edge operations. EchoScene overcomes this by associating each node with a denoising process and enables collaborative information exchange, enhancing controllable and consistent generation aware of global constraints. This is achieved through an information echo scheme in both shape and layout branches. At every denoising step, all processes share their denoising data with an information exchange unit that combines these updates using graph convolution. The scheme ensures that the denoising processes are influenced by a holistic understanding of the scene graph, facilitating the generation of globally coherent scenes. The resulting scenes can be manipulated during inference by editing the input scene graph and sampling the noise in the diffusion model. Extensive experiments validate our approach, which maintains scene controllability and surpasses previous methods in generation fidelity. Moreover, the generated scenes are of high quality and thus directly compatible with off-the-shelf texture generation. Code and trained models are open-sourced.",
        "citation_title": "EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Population protocols are a well-studied model of distributed computation in which a group of anonymous finite-state agents communicates via pairwise interactions. Together they decide whether their initial configuration, that is, the initial distribution of agents in the states, satisfies a property. As an extension in order to express properties of multisets over an infinite data domain, Blondin and Ladouceur (ICALP'23) introduced population protocols with unordered data (PPUD). In PPUD, each agent carries a fixed data value, and the interactions between agents depend on whether their data are equal or not. Blondin and Ladouceur also identified the interesting subclass of immediate observation PPUD (IOPPUD), where in every transition one of the two agents remains passive and does not move, and they characterised its expressive power.\nWe study the decidability and complexity of formally verifying these protocols. The main verification problem for population protocols is well-specification, that is, checking whether the given PPUD computes some function. We show that well-specification is undecidable in general. By contrast, for IOPPUD, we exhibit a large yet natural class of problems, which includes well-specification among other classic problems, and establish that these problems are in EXPSPACE. We also provide a lower complexity bound, namely coNEXPTIME-hardness.",
        "citation_title": "Verification of Population Protocols with Unordered Data",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Traffic congestion has significant impacts on both the economy and the environment. Measures of Effectiveness (MOEs) have long been the standard for evaluating the level of service and operational efficiency of traffic intersections. However, the scarcity of traditional high-resolution loop detector data (ATSPM) presents challenges in accurately measuring MOEs or capturing the intricate temporospatial characteristics inherent in urban intersection traffic. In response to this challenge, we have introduced the Multi-Task Deep Learning Digital Twin (MTDT) as a solution for multifaceted and precise intersection traffic flow simulation. MTDT enables accurate, fine-grained estimation of loop detector waveform time series for each lane of movement, alongside successful estimation of several MOEs for each lane group associated with a traffic phase concurrently and for all approaches of an arbitrary urban intersection. Unlike existing deep learning methodologies, MTDT distinguishes itself through its adaptability to local temporal and spatial features, such as signal timing plans, intersection topology, driving behaviors, and turning movement counts. While maintaining a straightforward design, our model emphasizes the advantages of multi-task learning in traffic modeling. By consolidating the learning process across multiple tasks, MTDT demonstrates reduced overfitting, increased efficiency, and enhanced effectiveness by sharing representations learned by different tasks. Furthermore, our approach facilitates sequential computation and lends itself to complete parallelization through GPU implementation. This not only streamlines the computational process but also enhances scalability and performance.",
        "citation_title": "MTDT: A Multi-Task Deep Learning Digital Twin",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper studies the controller synthesis problem for nonlinear control systems under linear temporal logic (LTL) specifications using zonotope techniques. A local-to-global control strategy is proposed for the desired specification expressed as an LTL formula. First, a novel approach is developed to divide the state space into finite zonotopes and constrained zonotopes, which are called cells and allowed to intersect with the neighbor cells. Second, from the intersection relation, a graph among all cells is generated to verify the realization of the accepting path for the LTL formula. The realization verification determines if there is a need for the control design, and also results in finite local LTL formulas. Third, once the accepting path is realized, a novel abstraction-based method is derived for the controller design. In particular, we only focus on the cells from the realization verification and approximate each cell thanks to properties of zonotopes. Based on local symbolic models and local LTL formulas, an iterative synthesis algorithm is proposed to design all local abstract controllers, whose existence and combination establish the global controller for the LTL formula. Finally, the proposed framework is illustrated via a path planning problem of mobile robots.",
        "citation_title": "Zonotope-based Symbolic Controller Synthesis for Linear Temporal Logic Specifications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
        "citation_title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "When solving systems of banded Toeplitz equations or calculating their inverses, it is necessary to determine the invertibility of the matrices beforehand. In this paper, we equate the invertibility of an $n$-order banded Toeplitz matrix with bandwidth $2k+1$ to that of a small $k*k$ matrix. By utilizing a specially designed algorithm, we compute the invertibility sequence of a class of banded Toeplitz matrices with a time complexity of $5k^2n/2+kn$ and a space complexity of $3k^2$ where $n$ is the size of the largest matrix. This enables efficient preprocessing when solving equation systems and inverses of banded Toeplitz matrices.",
        "citation_title": "Efficient Computation for Invertibility Sequence of Banded Toeplitz Matrices",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Linkage methods are among the most popular algorithms for hierarchical clustering. Despite their relevance the current knowledge regarding the quality of the clustering produced by these methods is limited. Here, we improve the currently available bounds on the maximum diameter of the clustering obtained by complete-link for metric spaces.\nOne of our new bounds, in contrast to the existing ones, allows us to separate complete-link from single-link in terms of approximation for the diameter, which corroborates the common perception that the former is more suitable than the latter when the goal is producing compact clusters.\nWe also show that our techniques can be employed to derive upper bounds on the cohesion of a class of linkage methods that includes the quite popular average-link.",
        "citation_title": "New bounds on the cohesion of complete-link and other linkage methods for agglomeration clustering",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present an approach to designing 3D Iterated Function Systems (IFS) within the Unity Editor and rendered to VR in real-time. Objects are modeled as a hierarchical tree of primitive shapes and operators, editable using a graphical user interface allowing artists to develop psychedelic scenes with little to no coding knowledge, and is easily extensible for more advanced users to add their own primitive shapes and operators.",
        "citation_title": "Virtual Psychedelia",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Step Chemical Reaction Networks (step CRNs) are an augmentation of the Chemical Reaction Network (CRN) model where additional species may be introduced to the system in a sequence of ``steps.'' We study step CRN systems using a weak subset of reaction rules, \\emph{void} rules, in which molecular species can only be deleted. We demonstrate that step CRNs with only void rules of size (2,0) can simulate threshold formulas (TFs) under linear resources. These limited systems can also simulate threshold \\emph{circuits} (TCs) by modifying the volume of the system to be exponential. We then prove a matching exponential lower bound on the required volume for simulating threshold circuits in a step CRN with (2,0)-size rules under a restricted \\emph{gate-wise} simulation, thus showing our construction is optimal for simulating circuits in this way.",
        "citation_title": "Computing Threshold Circuits with Bimolecular Void Reactions in Step Chemical Reaction Networks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Communication is defined as ``Who says what to whom with what effect.'' A message from a communicator generates downstream receiver effects, also known as behavior. Receiver behavior, being a downstream effect of the message, carries rich signals about it. Even after carrying signals about the message, the behavior data is often ignored while training large language models. We show that training LLMs on receiver behavior can actually help improve their content-understanding abilities. Specifically, we show that training LLMs to predict the receiver behavior of likes and comments improves the LLM's performance on a wide variety of downstream content understanding tasks. We show this performance increase over 40 video and image understanding tasks over 23 benchmark datasets across both 0-shot and fine-tuning settings, outperforming many supervised baselines. Moreover, since receiver behavior, such as likes and comments, is collected by default on the internet and does not need any human annotations to be useful, the performance improvement we get after training on this data is essentially free-lunch. We release the receiver behavior cleaned comments and likes of 750k images and videos collected from multiple platforms along with our instruction-tuning data.",
        "citation_title": "LLaVA Finds Free Lunch: Teaching Human Behavior Improves Content Understanding Abilities Of LLMs",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A chaser satellite equipped with robotic arms can capture space debris and manipulate it for use in more advanced missions such as refueling and deorbiting. To facilitate capturing, a caging-based strategy has been proposed to simplify the control system. Caging involves geometrically constraining the motion of the target debris, and is achieved via position control. However, if the target is spinning at a high speed, direct caging may result in unsuccessful constraints or hardware destruction; therefore, the target should be de-tumbled before capture. To address this problem, this study proposes a repeated contact-based method that uses impedance control to mitigate the momentum of the target. In this study, we analyzed the proposed detumbling technique from the perspective of impedance parameters. We investigated their effects through a parametric analysis and demonstrated the successful detumbling and caging sequence of a microsatellite as representative of space debris. The contact forces decreased during the detumbling sequence compared with direct caging. Further, the proposed detumbling and caging sequence was validated through simulations and experiments using a dual-arm air-floating robot in two-dimensional microgravity emulating testbed.",
        "citation_title": "Space Debris Reliable Capturing by a Dual-Arm Orbital Robot: Detumbling and Caging",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Motivated by the ideal peak-to-average-power ratio and radar sensing capability of traditional frequency-coded radar waveforms, this paper considers the frequency shift keying (FSK) based waveform for joint communications and radar (JCR). An analysis of the probability distributions of its ambiguity function (AF) sidelobe levels (SLs) and peak sidelobe level (PSL) is conducted to study the radar sensing capability of random FSK. Numerical results show that the independent frequency modulation introduces uncontrollable AF PSLs. In order to address this problem, the initial phases of waveform sub-pulses are designed by solving a min-max optimisation problem. Numerical results indicate that the optimisation-based phase design can effectively reduce the AF PSL to a level close to well-designed radar waveforms while having no impact on the data rate and the receiver complexity. For large numbers of waveform sub-pulses and modulation orders, the impact on the error probability is also insignificant.",
        "citation_title": "Can FSK Be Optimised for Integrated Sensing and Communications?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper introduces SparseTSF, a novel, extremely lightweight model for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources. At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by decoupling the periodicity and trend in time series data. This technique involves downsampling the original sequences to focus on cross-period trend prediction, effectively extracting periodic features while minimizing the model's complexity and parameter count. Based on this technique, the SparseTSF model uses fewer than 1k parameters to achieve competitive or superior performance compared to state-of-the-art models. Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data. The code is available at: this https URL.",
        "citation_title": "SparseTSF: Modeling Long-term Time Series Forecasting with 1k Parameters",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Empathy requires perspective-taking: empathetic responses require a person to reason about what another has experienced and communicate that understanding in language. However, most NLP approaches to empathy do not explicitly model this alignment process. Here, we introduce a new approach to recognizing alignment in empathetic speech, grounded in Appraisal Theory. We introduce a new dataset of over 9.2K span-level annotations of different types of appraisals of a person's experience and over 3K empathetic alignments between a speaker's and observer's speech. Through computational experiments, we show that these appraisals and alignments can be accurately recognized. In experiments in over 9.2M Reddit conversations, we find that appraisals capture meaningful groupings of behavior but that most responses have minimal alignment. However, we find that mental health professionals engage with substantially more empathetic alignment.",
        "citation_title": "Modeling Empathetic Alignment in Conversation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This study introduces a systematic framework to compare the efficacy of Large Language Models (LLMs) for fine-tuning across various cheminformatics tasks. Employing a uniform training methodology, we assessed three well-known models-RoBERTa, BART, and LLaMA-on their ability to predict molecular properties using the Simplified Molecular Input Line Entry System (SMILES) as a universal molecular representation format. Our comparative analysis involved pre-training 18 configurations of these models, with varying parameter sizes and dataset scales, followed by fine-tuning them on six benchmarking tasks from DeepChem. We maintained consistent training environments across models to ensure reliable comparisons. This approach allowed us to assess the influence of model type, size, and training dataset size on model performance. Specifically, we found that LLaMA-based models generally offered the lowest validation loss, suggesting their superior adaptability across tasks and scales. However, we observed that absolute validation loss is not a definitive indicator of model performance - contradicts previous research - at least for fine-tuning tasks: instead, model size plays a crucial role. Through rigorous replication and validation, involving multiple training and fine-tuning cycles, our study not only delineates the strengths and limitations of each model type but also provides a robust methodology for selecting the most suitable LLM for specific cheminformatics applications. This research underscores the importance of considering model architecture and dataset characteristics in deploying AI for molecular property prediction, paving the way for more informed and effective utilization of AI in drug discovery and related fields.",
        "citation_title": "The Role of Model Architecture and Scale in Predicting Molecular Properties: Insights from Fine-Tuning RoBERTa, BART, and LLaMA",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Restless multi-armed bandits (RMAB) play a central role in modeling sequential decision making problems under an instantaneous activation constraint that at most B arms can be activated at any decision epoch. Each restless arm is endowed with a state that evolves independently according to a Markov decision process regardless of being activated or not. In this paper, we consider the task of learning in episodic RMAB with unknown transition functions and adversarial rewards, which can change arbitrarily across episodes. Further, we consider a challenging but natural bandit feedback setting that only adversarial rewards of activated arms are revealed to the decision maker (DM). The goal of the DM is to maximize its total adversarial rewards during the learning process while the instantaneous activation constraint must be satisfied in each decision epoch. We develop a novel reinforcement learning algorithm with two key contributors: a novel biased adversarial reward estimator to deal with bandit feedback and unknown transitions, and a low-complexity index policy to satisfy the instantaneous activation constraint. We show $\\tilde{\\mathcal{O}}(H\\sqrt{T})$ regret bound for our algorithm, where $T$ is the number of episodes and $H$ is the episode length. To our best knowledge, this is the first algorithm to ensure $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret for adversarial RMAB in our considered challenging settings.",
        "citation_title": "Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Hyperspectral Imaging (HSI) serves as an important technique in remote sensing. However, high dimensionality and data volume typically pose significant computational challenges. Band selection is essential for reducing spectral redundancy in hyperspectral imagery while retaining intrinsic critical information. In this work, we propose a novel hyperspectral band selection model by decomposing the data into a low-rank and smooth component and a sparse one. In particular, we develop a generalized 3D total variation (G3DTV) by applying the $\\ell_1^p$-norm to derivatives to preserve spatial-spectral smoothness. By employing the alternating direction method of multipliers (ADMM), we derive an efficient algorithm, where the tensor low-rankness is implied by the tensor CUR decomposition. We demonstrate the effectiveness of the proposed approach through comparisons with various other state-of-the-art band selection techniques using two benchmark real-world datasets. In addition, we provide practical guidelines for parameter selection in both noise-free and noisy scenarios.",
        "citation_title": "Hyperspectral Band Selection based on Generalized 3DTV and Tensor CUR Decomposition",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry->Texture->Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: this https URL.",
        "citation_title": "X-Oscar: A Progressive Framework for High-quality Text-guided 3D Animatable Avatar Generation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Gradient inversion (GI) attacks present a threat to the privacy of clients in federated learning (FL) by aiming to enable reconstruction of the clients' data from communicated model updates. A number of such techniques attempts to accelerate data recovery by first reconstructing labels of the samples used in local training. However, existing label extraction methods make strong assumptions that typically do not hold in realistic FL settings. In this paper we present a novel label recovery scheme, Recovering Labels from Local Updates (RLU), which provides near-perfect accuracy when attacking untrained (most vulnerable) models. More significantly, RLU achieves high performance even in realistic real-world settings where the clients in an FL system run multiple local epochs, train on heterogeneous data, and deploy various optimizers to minimize different objective functions. Specifically, RLU estimates labels by solving a least-square problem that emerges from the analysis of the correlation between labels of the data points used in a training round and the resulting update of the output layer. The experimental results on several datasets, architectures, and data heterogeneity scenarios demonstrate that the proposed method consistently outperforms existing baselines, and helps improve quality of the reconstructed images in GI attacks in terms of both PSNR and LPIPS.",
        "citation_title": "Recovering Labels from Local Updates in Federated Learning",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Surgical scene simulation plays a crucial role in surgical education and simulator-based robot learning. Traditional approaches for creating these environments with surgical scene involve a labor-intensive process where designers hand-craft tissues models with textures and geometries for soft body simulations. This manual approach is not only time-consuming but also limited in the scalability and realism. In contrast, data-driven simulation offers a compelling alternative. It has the potential to automatically reconstruct 3D surgical scenes from real-world surgical video data, followed by the application of soft body physics. This area, however, is relatively uncharted. In our research, we introduce 3D Gaussian as a learnable representation for surgical scene, which is learned from stereo endoscopic video. To prevent over-fitting and ensure the geometrical correctness of these scenes, we incorporate depth supervision and anisotropy regularization into the Gaussian learning process. Furthermore, we apply the Material Point Method, which is integrated with physical properties, to the 3D Gaussians to achieve realistic scene deformations. Our method was evaluated on our collected in-house and public surgical videos datasets. Results show that it can reconstruct and simulate surgical scenes from endoscopic videos efficiently-taking only a few minutes to reconstruct the surgical scene-and produce both visually and physically plausible deformations at a speed approaching real-time. The results demonstrate great potential of our proposed method to enhance the efficiency and variety of simulations available for surgical education and robot learning.",
        "citation_title": "Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Graph Neural Networks (GNNs) demonstrate excellent performance on graphs, with their core idea about aggregating neighborhood information and learning from labels. However, the prevailing challenges in most graph datasets are twofold of Insufficient High-Quality Labels and Lack of Neighborhoods, resulting in weak GNNs. Existing data augmentation methods designed to address these two issues often tackle only one. They may either require extensive training of generators, rely on overly simplistic strategies, or demand substantial prior knowledge, leading to suboptimal generalization abilities. To simultaneously address both of these two challenges, we propose an elegant method called IntraMix. IntraMix innovatively employs Mixup among low-quality labeled data of the same class, generating high-quality labeled data at minimal cost. Additionally, it establishes neighborhoods for the generated data by connecting them with data from the same class with high confidence, thereby enriching the neighborhoods of graphs. IntraMix efficiently tackles both challenges faced by graphs and challenges the prior notion of the limited effectiveness of Mixup in node classification. IntraMix serves as a universal framework that can be readily applied to all GNNs. Extensive experiments demonstrate the effectiveness of IntraMix across various GNNs and datasets.",
        "citation_title": "IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this study, we introduce Generative Manufacturing Systems (GMS) as a novel approach to effectively manage and coordinate autonomous manufacturing assets, thereby enhancing their responsiveness and flexibility to address a wide array of production objectives and human preferences. Deviating from traditional explicit modeling, GMS employs generative AI, including diffusion models and ChatGPT, for implicit learning from envisioned futures, marking a shift from a model-optimum to a training-sampling decision-making. Through the integration of generative AI, GMS enables complex decision-making through interactive dialogue with humans, allowing manufacturing assets to generate multiple high-quality global decisions that can be iteratively refined based on human feedback. Empirical findings showcase GMS's substantial improvement in system resilience and responsiveness to uncertainties, with decision times reduced from seconds to milliseconds. The study underscores the inherent creativity and diversity in the generated solutions, facilitating human-centric decision-making through seamless and continuous human-machine interactions.",
        "citation_title": "Generative manufacturing systems using diffusion models and ChatGPT",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The growing reliance on digital twins across various industries and domains brings with it semantic interoperability challenges. Ontologies are a well-known strategy for addressing such challenges, though given the complexity of the phenomenon, there are risks of reintroducing the interoperability challenges at the level of ontology representations. In the interest of avoiding such pitfalls, we introduce and defend characterizations of digital twins within the context of the Common Core Ontologies, an extension of the widely-used Basic Formal Ontology. We provide a set of definitions and design patterns relevant to the domain of digital twins, highlighted by illustrative use cases of digital twins and their physical counterparts. In doing so, we provide a foundation on which to build more sophisticated ontological content related and connected to digital twins.",
        "citation_title": "Foundations for Digital Twins",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Radiology report generation aims to automatically generate detailed and coherent descriptive reports alongside radiology images. Previous work mainly focused on refining fine-grained image features or leveraging external knowledge. However, the precise alignment of fine-grained image features with corresponding text descriptions has not been considered. This paper presents a novel method called Fine-grained Image-Text Aligner (FITA) to construct fine-grained alignment for image and text features. It has three novel designs: Image Feature Refiner (IFR), Text Feature Refiner (TFR) and Contrastive Aligner (CA). IFR and TFR aim to learn fine-grained image and text features, respectively. We achieve this by leveraging saliency maps to effectively fuse symptoms with corresponding abnormal visual regions, and by utilizing a meticulously constructed triplet set for training. Finally, CA module aligns fine-grained image and text features using contrastive loss for precise alignment. Results show that our method surpasses existing methods on the widely used benchmark",
        "citation_title": "FITA: Fine-grained Image-Text Aligner for Radiology Report Generation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "As distributed learning applications such as Federated Learning, the Internet of Things (IoT), and Edge Computing grow, it is critical to address the shortcomings of such technologies from a theoretical perspective. As an abstraction, we consider decentralized learning over a network of communicating clients or nodes and tackle two major challenges: data heterogeneity and adversarial robustness. We propose a decentralized minimax optimization method that employs two important modules: local updates and gradient tracking. Minimax optimization is the key tool to enable adversarial training for ensuring robustness. Having local updates is essential in Federated Learning (FL) applications to mitigate the communication bottleneck, and utilizing gradient tracking is essential to proving convergence in the case of data heterogeneity. We analyze the performance of the proposed algorithm, Dec-FedTrack, in the case of nonconvex-strongly concave minimax optimization, and prove that it converges a stationary point. We also conduct numerical experiments to support our theoretical findings.",
        "citation_title": "Robust Decentralized Learning with Local Updates and Gradient Tracking",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still underperforms on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we examine its limitations, demonstrating the presence of speaker-related (gender, age) and model-related (resourcefulness and model size) bias. Despite that, we show that only model-related bias are amplified by quantization, impacting more low-resource languages and smaller models. Searching for a better compression approach, we propose DistilWhisper, an approach that is able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference.",
        "citation_title": "Efficient Compression of Multitask Multilingual Speech Models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "One-on-one tutoring is widely acknowledged as an effective instructional method, conditioned on qualified tutors. However, the high demand for qualified tutors remains a challenge, often necessitating the training of novice tutors (i.e., trainees) to ensure effective tutoring. Research suggests that providing timely explanatory feedback can facilitate the training process for trainees. However, it presents challenges due to the time-consuming nature of assessing trainee performance by human experts. Inspired by the recent advancements of large language models (LLMs), our study employed the GPT-4 model to build an explanatory feedback system. This system identifies trainees' responses in binary form (i.e., correct/incorrect) and automatically provides template-based feedback with responses appropriately rephrased by the GPT-4 model. We conducted our study on 410 responses from trainees across three training lessons: Giving Effective Praise, Reacting to Errors, and Determining What Students Know. Our findings indicate that: 1) using a few-shot approach, the GPT-4 model effectively identifies correct/incorrect trainees' responses from three training lessons with an average F1 score of 0.84 and an AUC score of 0.85; and 2) using the few-shot approach, the GPT-4 model adeptly rephrases incorrect trainees' responses into desired responses, achieving performance comparable to that of human experts.",
        "citation_title": "How Can I Get It Right? Using GPT to Rephrase Incorrect Trainee Responses",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Large language models (LLMs) have shown remarkable potential in various domains, but they often lack the ability to access and reason over domain-specific knowledge and tools. In this paper, we introduced CACTUS (Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that integrates cheminformatics tools to enable advanced reasoning and problem-solving in chemistry and molecular discovery. We evaluate the performance of CACTUS using a diverse set of open-source LLMs, including Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of thousands of chemistry questions. Our results demonstrate that CACTUS significantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b models achieving the highest accuracy regardless of the prompting strategy used. Moreover, we explore the impact of domain-specific prompting and hardware configurations on model performance, highlighting the importance of prompt engineering and the potential for deploying smaller models on consumer-grade hardware without significant loss in accuracy. By combining the cognitive capabilities of open-source LLMs with domain-specific tools, CACTUS can assist researchers in tasks such as molecular property prediction, similarity searching, and drug-likeness assessment. Furthermore, CACTUS represents a significant milestone in the field of cheminformatics, offering an adaptable tool for researchers engaged in chemistry and molecular discovery. By integrating the strengths of open-source LLMs with domain-specific tools, CACTUS has the potential to accelerate scientific advancement and unlock new frontiers in the exploration of novel, effective, and safe therapeutic candidates, catalysts, and materials. Moreover, CACTUS's ability to integrate with automated experimentation platforms and make data-driven decisions in real time opens up new possibilities for autonomous discovery.",
        "citation_title": "CACTUS: Chemistry Agent Connecting Tool-Usage to Science",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Cell inconsistency within a lithium-ion battery system poses a significant challenge in maximizing the system operational time. This study presents an optimization-driven active balancing method to minimize the effects of cell inconsistency on the system operational time while simultaneously satisfying the system output power demand and prolonging the system operational time in energy storage applications. The proposed method utilizes a fractional order model to forecast the terminal voltage dynamics of each cell within a battery system, enhanced with a particle-swarm-optimisation-genetic algorithm for precise parameter identification. It is implemented under two distinct cell-level balancing topologies: independent cell balancing and differential cell balancing. Subsequently, the current distribution for each topology is determined by resolving two optimization control problems constrained by the battery's operational specifications and power demands. The effectiveness of the proposed method is validated by extensive experiments based on the two balancing topologies. The results demonstrate that the proposed method increases the operational time by 3.2%.",
        "citation_title": "Active Cell Balancing for Extended Operational Time of Lithium-Ion Battery Systems in Energy Storage Applications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "PLAID, an efficient implementation of the ColBERT late interaction bi-encoder using pretrained language models for ranking, consistently achieves state-of-the-art performance in monolingual, cross-language, and multilingual retrieval. PLAID differs from ColBERT by assigning terms to clusters and representing those terms as cluster centroids plus compressed residual vectors. While PLAID is effective in batch experiments, its performance degrades in streaming settings where documents arrive over time because representations of new tokens may be poorly modeled by the earlier tokens used to select cluster centroids. PLAID Streaming Hierarchical Indexing that Runs on Terabytes of Temporal Text (PLAID SHIRTTT) addresses this concern using multi-phase incremental indexing based on hierarchical sharding. Experiments on ClueWeb09 and the multilingual NeuCLIR collection demonstrate the effectiveness of this approach both for the largest collection indexed to date by the ColBERT architecture and in the multilingual setting, respectively.",
        "citation_title": "PLAID SHIRTTT for Large-Scale Streaming Dense Retrieval",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recent work in cross-language information retrieval (CLIR), where queries and documents are in different languages, has shown the benefit of the Translate-Distill framework that trains a cross-language neural dual-encoder model using translation and distillation. However, Translate-Distill only supports a single document language. Multilingual information retrieval (MLIR), which ranks a multilingual document collection, is harder to train than CLIR because the model must assign comparable relevance scores to documents in different languages. This work extends Translate-Distill and propose Multilingual Translate-Distill (MTD) for MLIR. We show that ColBERT-X models trained with MTD outperform their counterparts trained ith Multilingual Translate-Train, which is the previous state-of-the-art training approach, by 5% to 25% in nDCG@20 and 15% to 45% in MAP. We also show that the model is robust to the way languages are mixed in training batches. Our implementation is available on GitHub.",
        "citation_title": "Distillation for Multilingual Information Retrieval",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Multilingual information retrieval (MLIR) considers the problem of ranking documents in several languages for a query expressed in a language that may differ from any of those languages. Recent work has observed that approaches such as combining ranked lists representing a single document language each or using multilingual pretrained language models demonstrate a preference for one language over others. This results in systematic unfair treatment of documents in different languages. This work proposes a language fairness metric to evaluate whether documents across different languages are fairly ranked through statistical equivalence testing using the Kruskal-Wallis test. In contrast to most prior work in group fairness, we do not consider any language to be an unprotected group. Thus our proposed measure, PEER (Probability of EqualExpected Rank), is the first fairness metric specifically designed to capture the language fairness of MLIR systems. We demonstrate the behavior of PEER on artificial ranked lists. We also evaluate real MLIR systems on two publicly available benchmarks and show that the PEER scores align with prior analytical findings on MLIR fairness. Our implementation is compatible with ir-measures and is available at this http URL.",
        "citation_title": "Language Fairness in Multilingual Information Retrieval",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A critical hindrance to realize frequency division duplex (FDD) massive multi-input multi-output (MIMO) systems is overhead associated with downlink channel state information at the transmitter (CSIT) acquisition. To address this challenge, we propose a novel framework that achieves robust performances while completely eliminating downlink CSIT training and feedback. Specifically, by exploiting partial frequency invariance of channel parameters between the uplink (UL) and downlink (DL), we adopt the 2D-Newtonized orthogonal matching pursuit (2D-NOMP) algorithm to reconstruct DL CSIT from UL training. Due to inherent discrepancies arising from a carrier frequency difference between two disjoint bands, however, the multi-user interference is inevitable. To overcome this, we propose a precoding method that employs rate-splitting multiple access (RSMA) and also develop an error covariance matrix (ECM) estimation method by using the observed Fisher information matrix (O-FIM). We find that this ECM estimation is crucial for our precoding design in maximizing the sum spectral efficiency (SE). Simulation results show that our method significantly improves the sum SE compared to other state-of-the-art approaches, underscoring the importance of our ECM estimation.",
        "citation_title": "Splitting Messages in the Dark- Rate-Splitting Multiple Access for FDD Massive MIMO Without CSI Feedback",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper introduces TVB-HKSL-News, a new Hong Kong sign language (HKSL) dataset collected from a TV news program over a period of 7 months. The dataset is collected to enrich resources for HKSL and support research in large-vocabulary continuous sign language recognition (SLR) and translation (SLT). It consists of 16.07 hours of sign videos of two signers with a vocabulary of 6,515 glosses (for SLR) and 2,850 Chinese characters or 18K Chinese words (for SLT). One signer has 11.66 hours of sign videos and the other has 4.41 hours. One objective in building the dataset is to support the investigation of how well large-vocabulary continuous sign language recognition/translation can be done for a single signer given a (relatively) large amount of his/her training data, which could potentially lead to the development of new modeling methods. Besides, most parts of the data collection pipeline are automated with little human intervention; we believe that our collection method can be scaled up to collect more sign language data easily for SLT in the future for any sign languages if such sign-interpreted videos are available. We also run a SOTA SLR/SLT model on the dataset and get a baseline SLR word error rate of 34.08% and a baseline SLT BLEU-4 score of 23.58 for benchmarking future research on the dataset.",
        "citation_title": "A Hong Kong Sign Language Corpus Collected from Sign-interpreted TV News",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Designing preference elicitation (PE) methodologies that can quickly ascertain a user's top item preferences in a cold-start setting is a key challenge for building effective and personalized conversational recommendation (ConvRec) systems. While large language models (LLMs) constitute a novel technology that enables fully natural language (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches lack the multi-turn, decision-theoretic reasoning required to effectively balance the NL exploration and exploitation of user preferences towards an arbitrary item set. In contrast, traditional Bayesian optimization PE methods define theoretically optimal PE strategies, but fail to use NL item descriptions or generate NL queries, unrealistically assuming users can express preferences with direct item ratings and comparisons. To overcome the limitations of both approaches, we formulate NL-PE in a Bayesian Optimization (BO) framework that seeks to generate NL queries which actively elicit natural language feedback to reduce uncertainty over item utilities to identify the best recommendation. We demonstrate our framework in a novel NL-PE algorithm, PEBOL, which uses Natural Language Inference (NLI) between user preference utterances and NL item descriptions to maintain preference beliefs and BO strategies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) to guide LLM query generation. We numerically evaluate our methods in controlled experiments, finding that PEBOL achieves up to 131% improvement in MAP@10 after 10 turns of cold start NL-PE dialogue compared to monolithic GPT-3.5, despite relying on a much smaller 400M parameter NLI model for preference inference.",
        "citation_title": "Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Large Language Models (LLMs) have enabled new ways to satisfy information needs. Although great strides have been made in applying them to settings like document ranking and short-form text generation, they still struggle to compose complete, accurate, and verifiable long-form reports. Reports with these qualities are necessary to satisfy the complex, nuanced, or multi-faceted information needs of users. In this perspective paper, we draw together opinions from industry and academia, and from a variety of related research areas, to present our vision for automatic report generation, and -- critically -- a flexible framework by which such reports can be evaluated. In contrast with other summarization tasks, automatic report generation starts with a detailed description of an information need, stating the necessary background, requirements, and scope of the report. Further, the generated reports should be complete, accurate, and verifiable. These qualities, which are desirable -- if not required -- in many analytic report-writing settings, require rethinking how to build and evaluate systems that exhibit these qualities. To foster new efforts in building these systems, we present an evaluation framework that draws on ideas found in various evaluations. To test completeness and accuracy, the framework uses nuggets of information, expressed as questions and answers, that need to be part of any high-quality generated report. Additionally, evaluation of citations that map claims made in the report to their source documents ensures verifiability.",
        "citation_title": "On the Evaluation of Machine-Generated Reports",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The development of Audio Description (AD) has been a pivotal step forward in making video content more accessible and inclusive. Traditionally, AD production has demanded a considerable amount of skilled labor, while existing automated approaches still necessitate extensive training to integrate multimodal inputs and tailor the output from a captioning style to an AD style. In this paper, we introduce an automated AD generation pipeline that harnesses the potent multimodal and instruction-following capacities of GPT-4V(ision). Notably, our methodology employs readily available components, eliminating the need for additional training. It produces ADs that not only comply with established natural language AD production standards but also maintain contextually consistent character information across frames, courtesy of a tracking-based character recognition module. A thorough analysis on the MAD dataset reveals that our approach achieves a performance on par with learning-based methods in automated AD production, as substantiated by a CIDEr score of 20.5.",
        "citation_title": "LLM-AD: Large Language Model based Audio Description System",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Data-Free Meta-Learning (DFML) aims to extract knowledge from a collection of pre-trained models without requiring the original data, presenting practical benefits in contexts constrained by data privacy concerns. Current DFML methods primarily focus on the data recovery from these pre-trained models. However, they suffer from slow recovery speed and overlook gaps inherent in heterogeneous pre-trained models. In response to these challenges, we introduce the Faster and Better Data-Free Meta-Learning (FREE) framework, which contains: (i) a meta-generator for rapidly recovering training tasks from pre-trained models; and (ii) a meta-learner for generalizing to new unseen tasks. Specifically, within the module Faster Inversion via Meta-Generator, each pre-trained model is perceived as a distinct task. The meta-generator can rapidly adapt to a specific task in just five steps, significantly accelerating the data recovery. Furthermore, we propose Better Generalization via Meta-Learner and introduce an implicit gradient alignment algorithm to optimize the meta-learner. This is achieved as aligned gradient directions alleviate potential conflicts among tasks from heterogeneous pre-trained models. Empirical experiments on multiple benchmarks affirm the superiority of our approach, marking a notable speed-up (20$\\times$) and performance enhancement (1.42\\% $\\sim$ 4.78\\%) in comparison to the state-of-the-art.",
        "citation_title": "FREE: Faster and Better Data-Free Meta-Learning",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Neural collapse (NC) is a simple and symmetric phenomenon for deep neural networks (DNNs) at the terminal phase of training, where the last-layer features collapse to their class means and form a simplex equiangular tight frame aligning with the classifier vectors. However, the relationship of the last-layer features to the data and intermediate layers during training remains unexplored. To this end, we characterize the geometry of intermediate layers of ResNet and propose a novel conjecture, progressive feedforward collapse (PFC), claiming the degree of collapse increases during the forward propagation of DNNs. We derive a transparent model for the well-trained ResNet according to that ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase. The metrics of PFC indeed monotonically decrease across depth on various datasets. We propose a new surrogate model, multilayer unconstrained feature model (MUFM), connecting intermediate layers by an optimal transport regularizer. The optimal solution of MUFM is inconsistent with NC but is more concentrated relative to the input data. Overall, this study extends NC to PFC to model the collapse phenomenon of intermediate layers and its dependence on the input data, shedding light on the theoretical understanding of ResNet in classification problems.",
        "citation_title": "Progressive Feedforward Collapse of ResNet Training",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Session-based recommendation (SBR) aims to predict the following item a user will interact with during an ongoing session. Most existing SBR models focus on designing sophisticated neural-based encoders to learn a session representation, capturing the relationship among session items. However, they tend to focus on the last item, neglecting diverse user intents that may exist within a session. This limitation leads to significant performance drops, especially for longer sessions. To address this issue, we propose a novel SBR model, called Multi-intent-aware Session-based Recommendation Model (MiaSRec). It adopts frequency embedding vectors indicating the item frequency in session to enhance the information about repeated items. MiaSRec represents various user intents by deriving multiple session representations centered on each item and dynamically selecting the important ones. Extensive experimental results show that MiaSRec outperforms existing state-of-the-art SBR models on six datasets, particularly those with longer average session length, achieving up to 6.27% and 24.56% gains for MRR@20 and Recall@20. Our code is available at this https URL.",
        "citation_title": "Multi-intent-aware Session-based Recommendation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Learning expressive stochastic policies instead of deterministic ones has been proposed to achieve better stability, sample complexity, and robustness. Notably, in Maximum Entropy Reinforcement Learning (MaxEnt RL), the policy is modeled as an expressive Energy-Based Model (EBM) over the Q-values. However, this formulation requires the estimation of the entropy of such EBMs, which is an open problem. To address this, previous MaxEnt RL methods either implicitly estimate the entropy, resulting in high computational complexity and variance (SQL), or follow a variational inference procedure that fits simplified actor distributions (e.g., Gaussian) for tractability (SAC). We propose Stein Soft Actor-Critic (S$^2$AC), a MaxEnt RL algorithm that learns expressive policies without compromising efficiency. Specifically, S$^2$AC uses parameterized Stein Variational Gradient Descent (SVGD) as the underlying policy. We derive a closed-form expression of the entropy of such policies. Our formula is computationally efficient and only depends on first-order derivatives and vector products. Empirical results show that S$^2$AC yields more optimal solutions to the MaxEnt objective than SQL and SAC in the multi-goal environment, and outperforms SAC and SQL on the MuJoCo benchmark. Our code is available at: this https URL",
        "citation_title": "S$^2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Despite the remarkable success of Large Language Models (LLMs) in text understanding and generation, their potential for text clustering tasks remains underexplored. We observed that powerful closed-source LLMs provide good quality clusterings of entity sets but are not scalable due to the massive compute power required and the associated costs. Thus, we propose CACTUS (Context-Aware ClusTering with aUgmented triplet losS), a systematic approach that leverages open-source LLMs for efficient and effective supervised clustering of entity subsets, particularly focusing on text-based entities. Existing text clustering methods fail to effectively capture the context provided by the entity subset. Moreover, though there are several language modeling based approaches for clustering, very few are designed for the task of supervised clustering. This paper introduces a novel approach towards clustering entity subsets using LLMs by capturing context via a scalable inter-entity attention mechanism. We propose a novel augmented triplet loss function tailored for supervised clustering, which addresses the inherent challenges of directly applying the triplet loss to this problem. Furthermore, we introduce a self-supervised clustering task based on text augmentation techniques to improve the generalization of our model. For evaluation, we collect ground truth clusterings from a closed-source LLM and transfer this knowledge to an open-source LLM under the supervised clustering framework, allowing a faster and cheaper open-source model to perform the same task. Experiments on various e-commerce query and product clustering datasets demonstrate that our proposed approach significantly outperforms existing unsupervised and supervised baselines under various external clustering evaluation metrics.",
        "citation_title": "Context-Aware Clustering using Large Language Models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Building height is an important indicator for scientific research and practical application. However, building height products with a high spatial resolution (10m) are still very scarce. To meet the needs of high-resolution building height estimation models, this study established a set of spatial-spectral-temporal feature databases, combining SAR data provided by Sentinel-1, optical data provided by Sentinel-2, and shape data provided by building footprints. The statistical indicators on the time scale are extracted to form a rich database of 160 features. This study combined with permutation feature importance, Shapley Additive Explanations, and Random Forest variable importance, and the final stable features are obtained through an expert scoring system. This study took 12 large, medium, and small cities in the United States as the training data. It used moving windows to aggregate the pixels to solve the impact of SAR image displacement and building shadows. This study built a building height model based on a random forest model and compared three model ensemble methods of bagging, boosting, and stacking. To evaluate the accuracy of the prediction results, this study collected Lidar data in the test area, and the evaluation results showed that its R-Square reached 0.78, which can prove that the building height can be obtained effectively. The fast production of high-resolution building height data can support large-scale scientific research and application in many fields.",
        "citation_title": "Estimate the building height at a 10-meter resolution based on Sentinel data",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the rapidly evolving landscape of computing disciplines, substantial efforts are being dedicated to unraveling the sociotechnical implications of generative AI (Gen AI). While existing research has manifested in various forms, there remains a notable gap concerning the direct engagement of knowledge workers in academia with Gen AI. We interviewed 18 knowledge workers, including faculty and students, to investigate the social and technical dimensions of Gen AI from their perspective. Our participants raised concerns about the opacity of the data used to train Gen AI. This lack of transparency makes it difficult to identify and address inaccurate, biased, and potentially harmful, information generated by these models. Knowledge workers also expressed worries about Gen AI undermining trust in the relationship between instructor and student and discussed potential solutions, such as pedagogy readiness, to mitigate them. Additionally, participants recognized Gen AI's potential to democratize knowledge by accelerating the learning process and act as an accessible research assistant. However, there were also concerns about potential social and power imbalances stemming from unequal access to such technologies. Our study offers insights into the concerns and hopes of knowledge workers about the ethical use of Gen AI in educational settings and beyond, with implications for navigating this new landscape.",
        "citation_title": "Not a Swiss Army Knife: Academics' Perceptions of Trade-Offs Around Generative Artificial Intelligence Use",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper investigates gaps in access to and the cost of housing credit by race and ethnicity using the near universe of U.S. mortgage applications. Our data contain borrower creditworthiness variables that have historically been absent from industry-wide application data and that are likely to affect application approval and loan pricing. We find large unconditional disparities in approval and pricing between racial and ethnic groups. After conditioning on key elements of observable borrower creditworthiness, these disparities are smaller but remain economically meaningful. Sensitivity analysis indicates that omitted factors as predictive of approval/pricing and race/ethnicity as credit score can explain some of the pricing disparities but cannot explain the approval disparities. Taken together, our results suggest that credit score, income, and down payment requirements significantly contribute to disparities in mortgage access and affordability but that other systemic barriers are also responsible for a large share of disparate outcomes in the mortgage market.",
        "citation_title": "Racial and Ethnic Disparities in Mortgage Lending: New Evidence from Expanded HMDA Data",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper enhances our comprehension of the Distributional Synthetic Control (DSC) proposed by Gunsilius (2023), focusing on its asymptotic properties. We first establish the DSC estimator's asymptotic optimality. The essence of this optimality lies in the treatment effect estimator given by DSC achieves the lowest possible squared prediction error among all potential treatment effect estimators that depend on an average of quantiles of control units. We also establish the convergence of the DSC weights when some requirements are met, as well as the convergence rate. A significant aspect of our research is that we find DSC synthesis forms an optimal weighted average, particularly in situations where it is impractical to perfectly fit the treated unit's quantiles through the weighted average of the control units' quantiles. To corroborate our theoretical insights, we provide empirical evidence derived from simulations.",
        "citation_title": "Asymptotic Properties of the Distributional Synthetic Controls",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "By employing causal discovery method, the Fast Causal Inference (FCI) model to analyze data from the 2022 \"Financial Literacy Survey,\" we explore the causal relationships between financial literacy and financial activities, specifically investment participation and retirement planning. Our findings indicate that increasing financial literacy may not directly boost engagement in financial investments or retirement planning in Japan, which underscores the necessity for alternative strategies to motivate financial activities among Japanese households. This research offers valuable insights for policymakers focused on improving financial well-being by advancing the use of causal discovery algorithms in understanding financial behaviors.",
        "citation_title": "Does Financial Literacy Impact Investment Participation and Retirement Planning in Japan?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The study uses bibliometric as well as content analysis to determine the current situation regarding the application of technology adoption models (i.e., the Technology Acceptance Model, Unified Theory of Acceptance and Use of Technology, and Innovation Diffusion Theory) to the smartphone market that also includes smart wearables. Hereby the author would like to determine the connection between smartphone usage and adoption models and enrich literature by defining state-of-the-art tendencies and approaches. To achieve the goal, the author applied a two-stage approach: in the first stage, 213 articles were analyzed using Citation and Bibliographic coupling tools in VOSviewer (1.6.20). The papers were selected from the Scopus database and the search of the papers was conducted in the fields of Economics, Business, and Computer technologies. In the second stage, the author conducted a brief literature review of the most influential papers. The results illustrate the situation regarding the implementation of different models in the case of smartphone adoption. Content analyses of the most influential papers were applied to explain and enrich the results of bibliometric analyses as well as determine research gaps and future research development.",
        "citation_title": "Modelling user behavior towards smartphones and wearable technologies: A bibliometric study and brief literature review",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The management of common-pool resources is a complex challenge due to the risk of overexploitation and the tragedy of the commons. A novel framework has been introduced to address this issue, focusing on the coevolutionary relationship between human behavior and common-pool resources within a human-environment system. However, the impact of the Allee effect on the coevolution and its resource sustainability is still unexplored. The Allee effect, a biological phenomenon characterized by a correlation between resource availability and growth rate, is a fundamental attribute of numerous natural resources. In this paper, we introduce two coevolutionary models of resource and strategy under replicator dynamics and knowledge feedback by applying the Allee effect to the common-pool resources within human-environment system. These models encapsulate various facets of resource dynamics and the players' behavior, such as resource growth function, the extraction rates, and the strategy update rules. We find that the Allee effect can induce bi-stability and critical transition, leading to either sustainable or unsustainable outcomes depending on the initial condition and parameter configuration. We demonstrate that knowledge feedback enhances the resilience and sustainability of the coevolving system, and these results advances the understanding of human-environment system and management of common-pool resources.",
        "citation_title": "The role of the Allee effect in common-pool resource and its sustainability",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Polarization is a well-documented phenomenon across a wide range of social issues. However, prevailing theories often compartmentalize the examination of herding behavior and opinion convergence within different contexts. In this study, we delve into the micro-foundations of how individuals strategically select reference groups, offering insight into a dynamic process where both individual opinions and the network evolve simultaneously. We base our model on two parameters: people's direct benefit from connections and their adaptability in adjusting their opinions. Our research highlights which conditions impede the network from achieving complete connectivity, resulting in enduring polarization. Notably, our model also reveals that polarization can transiently emerge during the transition towards consensus. We explore the connection between these scenarios and a critical network metric: the initial diameter, under specific conditions related to the initial distribution of opinions.",
        "citation_title": "Dynamic opinion updating with endogenous networks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may choose to (not) comply with a treatment recommendation, whenever it is made available to them, based on unobserved confounding factors. We provide non-parametric identification, estimation, and inference for Dynamic Local Average Treatment Effects, which are expected values of multi-period treatment contrasts among appropriately defined complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify local average effects of contrasts that correspond to offering treatment at any single time step. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify local average treatment effects of treating in multiple time periods.",
        "citation_title": "Dynamic Local Average Treatment Effects",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Prediction models can improve efficiency by automating decisions such as the approval of loan applications. However, they may inherit bias against protected groups from the data they are trained on. This paper adds counterfactual (simulated) ethnic bias to real data on mortgage application decisions, and shows that this bias is replicated by a machine learning model (XGBoost) even when ethnicity is not used as a predictive variable. Next, several other de-biasing methods are compared: averaging over prohibited variables, taking the most favorable prediction over prohibited variables (a novel method), and jointly minimizing errors as well as the association between predictions and prohibited variables. De-biasing can recover some of the original decisions, but the results are sensitive to whether the bias is effected through a proxy.",
        "citation_title": "De-Biasing Models of Biased Decisions: A Comparison of Methods Using Mortgage Application Data",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Adaptive experiments such as multi-arm bandits adapt the treatment-allocation policy and/or the decision to stop the experiment to the data observed so far. This has the potential to improve outcomes for study participants within the experiment, to improve the chance of identifying best treatments after the experiment, and to avoid wasting data. Seen as an experiment (rather than just a continually optimizing system) it is still desirable to draw statistical inferences with frequentist guarantees. The concentration inequalities and union bounds that generally underlie adaptive experimentation algorithms can yield overly conservative inferences, but at the same time the asymptotic normality we would usually appeal to in non-adaptive settings can be imperiled by adaptivity. In this article we aim to explain why, how, and when adaptivity is in fact an issue for inference and, when it is, understand the various ways to fix it: reweighting to stabilize variances and recover asymptotic normality, always-valid inference based on joint normality of an asymptotic limiting sequence, and characterizing and inverting the non-normal distributions induced by adaptivity.",
        "citation_title": "Demistifying Inference after Adaptive Experiments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Algorithms frequently assist, rather than replace, human decision-makers. However, the design and analysis of algorithms often focus on predicting outcomes and do not explicitly model their effect on human decisions. This discrepancy between the design and role of algorithmic assistants becomes of particular concern in light of empirical evidence that suggests that algorithmic assistants again and again fail to improve human decisions. In this article, we formalize the design of recommendation algorithms that assist human decision-makers without making restrictive ex-ante assumptions about how recommendations affect decisions. We formulate an algorithmic-design problem that leverages the potential-outcomes framework from causal inference to model the effect of recommendations on a human decision-maker's binary treatment choice. Within this model, we introduce a monotonicity assumption that leads to an intuitive classification of human responses to the algorithm. Under this monotonicity assumption, we can express the human's response to algorithmic recommendations in terms of their compliance with the algorithm and the decision they would take if the algorithm sends no recommendation. We showcase the utility of our framework using an online experiment that simulates a hiring task. We argue that our approach explains the relative performance of different recommendation algorithms in the experiment, and can help design solutions that realize human-AI complementarity.",
        "citation_title": "Designing Algorithmic Recommendations to Achieve Human-AI Complementarity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper proposes a novel identification strategy relying on quasi-instrumental variables (quasi-IVs). A quasi-IV is a relevant but possibly invalid IV because it is not exogenous or not excluded. We show that a variety of models with discrete or continuous endogenous treatment which are usually identified with an IV - quantile models with rank invariance, additive models with homogenous treatment effects, and local average treatment effect models - can be identified under the joint relevance of two complementary quasi-IVs instead. To achieve identification, we complement one excluded but possibly endogenous quasi-IV (e.g., \"relevant proxies\" such as lagged treatment choice) with one exogenous (conditional on the excluded quasi-IV) but possibly included quasi-IV (e.g., random assignment or exogenous market shocks). Our approach also holds if any of the two quasi-IVs turns out to be a valid IV. In practice, being able to address endogeneity with complementary quasi-IVs instead of IVs is convenient since there are many applications where quasi-IVs are more readily available. Difference-in-differences is a notable example: time is an exogenous quasi-IV while the group assignment acts as a complementary excluded quasi-IV.",
        "citation_title": "Identification with possibly invalid IVs",
        "date_delivered": "[Submitted on 8 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Let $X$ be an arbitrary set. Then a topology $t$ on $X$ is said to be completely useful if every upper semicontinuous linear (total) preorder $\\precsim$ on $X$ can be represented by an upper semicontinuous real-valued order preserving function. In this paper, appealing, simple and new characterizations of completely useful topologies will be proved, therefore clarifying the structure of such topologies.",
        "citation_title": "New characterizations of completely useful topologies in mathematical utility theory",
        "date_delivered": "[Submitted on 28 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We demonstrate and discuss the testability of the common trend assumption imposed in Difference-in-Differences (DiD) estimation in panel data when not relying on multiple pre-treatment periods for running placebo tests. Our testing approach involves two steps: (i) constructing a control group of non-treated units whose pre-treatment outcome distribution matches that of treated units, and (ii) verifying if this control group and the original non-treated group share the same time trend in average outcomes. Testing is motivated by the fact that in several (but not all) panel data models, a common trend violation across treatment groups implies and is implied by a common trend violation across pre-treatment outcomes. For this reason, the test verifies a sufficient, but (depending on the model) not necessary condition for DiD-based identification. We investigate the finite sample performance of a testing procedure that is based on double machine learning, which permits controlling for covariates in a data-driven manner, in a simulation study and also apply it to labor market data from the National Supported Work Demonstration.",
        "citation_title": "On the testability of common trends in panel data without placebo periods",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Detecting and characterising vehicles is one of the purposes of embedded systems used in intelligent environments. An analysis of a vehicle characteristics can reveal inappropriate or dangerous behaviour. This detection makes it possible to sanction or notify emergency services to take early and practical actions. Vehicle detection and characterisation systems employ complex sensors such as video cameras, especially in urban environments. These sensors provide high precision and performance, although the price and computational requirements are proportional to their accuracy. These sensors offer high accuracy, but the price and computational requirements are directly proportional to their performance. This article introduces a system based on modular devices that is economical and has a low computational cost. These devices use ultrasonic sensors to detect the speed and length of vehicles. The measurement accuracy is improved through the collaboration of the device modules. The experiments were performed using multiple modules oriented to different angles. This module is coupled with another specifically designed to detect distance using previous modules speed and length data. The collaboration between different modules reduces the speed relative error ranges from 1 to 5, depending on the angle configuration used in the modules.",
        "citation_title": "Low-cost modular devices for on-road vehicle detection and characterisation",
        "date_delivered": "[Submitted on 26 Jan 2024]"
    },
    {
        "abstract": "This paper studies the wireless scheduling design to coordinate the transmissions of (local) model parameters of federated learning (FL) for a swarm of unmanned aerial vehicles (UAVs). The overall goal of the proposed design is to realize the FL training and aggregation processes with a central aggregator exploiting the sensory data collected by the UAVs but it considers the multi-hop wireless network formed by the UAVs. Such transmissions of model parameters over the UAV-based wireless network potentially cause large transmission delays and overhead. Our proposed framework smartly aggregates local model parameters trained by the UAVs while efficiently transmitting the underlying parameters to the central aggregator in each FL global round. We theoretically show that the proposed scheme achieves minimal delay and communication overhead. Extensive numerical experiments demonstrate the superiority of the proposed scheme compared to other baselines.",
        "citation_title": "Delay and Overhead Efficient Transmission Scheduling for Federated Learning in UAV Swarms",
        "date_delivered": "[Submitted on 22 Feb 2024]"
    },
    {
        "abstract": "Synthetic longitudinal brain MRI simulates brain aging and would enable more efficient research on neurodevelopmental and neurodegenerative conditions. Synthetically generated, age-adjusted brain images could serve as valuable alternatives to costly longitudinal imaging acquisitions, serve as internal controls for studies looking at the effects of environmental or therapeutic modifiers on brain development, and allow data augmentation for diverse populations. In this paper, we present a diffusion-based approach called SynthBrainGrow for synthetic brain aging with a two-year step. To validate the feasibility of using synthetically-generated data on downstream tasks, we compared structural volumetrics of two-year-aged brains against synthetically-aged brain MRI. Results show that SynthBrainGrow can accurately capture substructure volumetrics and simulate structural changes such as ventricle enlargement and cortical thinning. Our approach provides a novel way to generate longitudinal brain datasets from cross-sectional data to enable augmented training and benchmarking of computational tools for analyzing lifespan trajectories. This work signifies an important advance in generative modeling to synthesize realistic longitudinal data with limited lifelong MRI scans. The code is available at XXX.",
        "citation_title": "SynthBrainGrow: Synthetic Diffusion Brain Aging for Longitudinal MRI Data Generation in Young People",
        "date_delivered": "[Submitted on 22 Feb 2024]"
    },
    {
        "abstract": "Purpose Medical imaging diagnosis faces challenges, including low-resolution images due to machine artifacts and patient movement. This paper presents the Frequency-Guided U-Net (GFNet), a novel approach for medical image segmentation that addresses challenges associated with low-resolution images and inefficient feature extraction. Approach In response to challenges related to computational cost and complexity in feature extraction, our approach introduces the Attention Filter Gate. Departing from traditional spatial domain learning, our model operates in the frequency domain using FFT. A strategically placed weighted learnable matrix filters feature, reducing computational costs. FFT is integrated between up-sampling and down-sampling, mitigating issues of throughput, latency, FLOP, and enhancing feature extraction. Results Experimental outcomes shed light on model performance. The Attention Filter Gate, a pivotal component of GFNet, achieves competitive segmentation accuracy (Mean Dice: 0.8366, Mean IoU: 0.7962). Comparatively, the Attention Gate model surpasses others, with a Mean Dice of 0.9107 and a Mean IoU of 0.8685. The widely-used U-Net baseline demonstrates satisfactory performance (Mean Dice: 0.8680, Mean IoU: 0.8268). Conclusion his work introduces GFNet as an efficient and accurate method for medical image segmentation. By leveraging the frequency domain and attention filter gates, GFNet addresses key challenges of information loss, computational cost, and feature extraction limitations. This novel approach offers potential advancements for computer-aided diagnosis and other healthcare applications. Keywords: Medical Segmentation, Neural Networks,",
        "citation_title": "Frequency-Guided U-Net: Leveraging Attention Filter Gates and Fast Fourier Transformation for Enhanced Medical Image Segmentation",
        "date_delivered": "[Submitted on 25 Feb 2024]"
    },
    {
        "abstract": "Human Activity Recognition (HAR) is a well-studied field with research dating back to the 1980s. Over time, HAR technologies have evolved significantly from manual feature extraction, rule-based algorithms, and simple machine learning models to powerful deep learning models, from one sensor type to a diverse array of sensing modalities. The scope has also expanded from recognising a limited set of activities to encompassing a larger variety of both simple and complex activities. However, there still exist many challenges that hinder advancement in complex activity recognition using modern deep learning methods. In this paper, we comprehensively systematise factors leading to inaccuracy in complex HAR, such as data variety and model capacity. Among many sensor types, we give more attention to wearable and camera due to their prevalence. Through this Systematisation of Knowledge (SoK) paper, readers can gain a solid understanding of the development history and existing challenges of HAR, different categorisations of activities, obstacles in deep learning-based complex HAR that impact accuracy, and potential research directions.",
        "citation_title": "SoK: Behind the Accuracy of Complex Human Activity Recognition Using Deep Learning",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "Agile beam management is key for providing seamless millimeter wave (mm-wave) connectivity given the site-specific spatio-temporal variations of the mm-wave channel. Leveraging non radio frequency (RF) sensor inputs for environment awareness, e.g. via machine learning (ML) techniques, can greatly enhance RF-based beam steering. To overcome the lack of diverse publicly available multi-modal mm-wave datasets for the design and evaluation of such novel beam steering approaches, we demonstrate our software-defined radio multi-band mm-wave measurement platform which integrates multi-modal sensors towards environment-aware beam management.",
        "citation_title": "Multi-Band mm-Wave Measurement Platform Towards Environment-Aware Beam Management",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "Effectively learning the temporal dynamics in electroencephalogram (EEG) signals is challenging yet essential for decoding brain activities using brain-computer interfaces (BCIs). Although Transformers are popular for their long-term sequential learning ability in the BCI field, most methods combining Transformers with convolutional neural networks (CNNs) fail to capture the coarse-to-fine temporal dynamics of EEG signals. To overcome this limitation, we introduce EEG-Deformer, which incorporates two main novel components into a CNN-Transformer: (1) a Hierarchical Coarse-to-Fine Transformer (HCT) block that integrates a Fine-grained Temporal Learning (FTL) branch into Transformers, effectively discerning coarse-to-fine temporal patterns; and (2) a Dense Information Purification (DIP) module, which utilizes multi-level, purified temporal information to enhance decoding accuracy. Comprehensive experiments on three representative cognitive tasks consistently verify the generalizability of our proposed EEG-Deformer, demonstrating that it either outperforms existing state-of-the-art methods or is comparable to them. Visualization results show that EEG-Deformer learns from neurophysiologically meaningful brain regions for the corresponding cognitive tasks. The source code can be found at this https URL.",
        "citation_title": "EEG-Deformer: A Dense Convolutional Transformer for Brain-computer Interfaces",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "A frequency-calibrated SCINet (FC-SCINet) equalizer is proposed for down-stream 100G PON with 28.7 dB path loss. At 5 km, FC-SCINet improves the BER by 88.87% compared to FFE and a 3-layer DNN with 10.57% lower complexity.",
        "citation_title": "A Novel Machine Learning-based Equalizer for a Downstream 100G PAM-4 PON",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "Brain-computer interface systems and the recording of brain activity has garnered significant attention across a diverse spectrum of applications. EEG signals have emerged as a modality for recording neural electrical activity. Among the methodologies designed for feature extraction from EEG data, the method of RCSP has proven to be an approach, particularly in the context of MI tasks. RCSP exhibits efficacy in the discrimination and classification of EEG signals. In optimizing the performance of this method, our research extends to a comparative analysis with conventional CSP techniques, as well as optimized methodologies designed for similar applications. Notably, we employ the meta-heuristic multi-objective Strength Pareto Evolutionary Algorithm II (SPEA-II) as a pivotal component of our research paradigm. This is a state-of-the-art approach in the selection of an subset of channels from a multichannel EEG signal with MI tasks. Our main objective is to formulate an optimum channel selection strategy aimed at identifying the most pertinent subset of channels from the multi-dimensional electroencephalogram (EEG) signals. One of the primary objectives inherent to channel selection in the EEG signal analysis pertains to the reduction of the channel count, an approach that enhances user comfort when utilizing gel-based EEG electrodes. Additionally, within this research, we took benefit of ensemble learning models as a component of our decision-making. This technique serves to mitigate the challenges associated with overfitting, especially when confronted with an extensive array of potentially redundant EEG channels and data noise. Our findings not only affirm the performance of RCSP in MI-based BCI systems, but also underscore the significance of channel selection strategies and ensemble learning techniques in optimizing the performance of EEG signal classification.",
        "citation_title": "Optimizing Brain-Computer Interface Performance: Advancing EEG Signals Channel Selection through Regularized CSP and SPEA II Multi-Objective Optimization",
        "date_delivered": "[Submitted on 26 Apr 2024]"
    },
    {
        "abstract": "Brain-Computer Interfaces (BCIs) rely on accurately decoding electroencephalography (EEG) motor imagery (MI) signals for effective device control. Graph Neural Networks (GNNs) outperform Convolutional Neural Networks (CNNs) in this regard, by leveraging the spatial relationships between EEG electrodes through adjacency matrices. The EEG_GLT-Net framework, featuring the state-of-the-art EEG_GLT adjacency matrix method, has notably enhanced EEG MI signal classification, evidenced by an average accuracy of 83.95% across 20 subjects on the PhysioNet dataset. This significantly exceeds the 76.10% accuracy rate achieved using the Pearson Correlation Coefficient (PCC) method within the same framework.\nIn this research, we advance the field by applying a Reinforcement Learning (RL) approach to the classification of EEG MI signals. Our innovative method empowers the RL agent, enabling not only the classification of EEG MI data points with higher accuracy, but effective identification of EEG MI data points that are less distinct. We present the EEG_RL-Net, an enhancement of the EEG_GLT-Net framework, which incorporates the trained EEG GCN Block from EEG_GLT-Net at an adjacency matrix density of 13.39% alongside the RL-centric Dueling Deep Q Network (Dueling DQN) block. The EEG_RL-Net model showcases exceptional classification performance, achieving an unprecedented average accuracy of 96.40% across 20 subjects within 25 milliseconds. This model illustrates the transformative effect of the RL in EEG MI time point classification.",
        "citation_title": "EEG_RL-Net: Enhancing EEG MI Classification through Reinforcement Learning-Optimised Graph Neural Networks",
        "date_delivered": "[Submitted on 26 Apr 2024]"
    },
    {
        "abstract": "Common artefacts such as baseline drift, rescaling, and noise critically limit the performance of machine learningbased automated ECG analysis and interpretation. This study proposes Derived Peak (DP) encoding, a non-parametric method that generates signed spikes corresponding to zero crossings of the signals first and second-order time derivatives. Notably, DP encoding is invariant to shift and scaling artefacts, and its implementation is further simplified by the absence of userdefined parameters. DP encoding was used to encode the 12-lead ECG data from the PTB-XL dataset (n=18,869 participants) and was fed to 1D-ResNet-18 models trained to identify myocardial infarction, conductive deficits and ST-segment abnormalities. Robustness to artefacts was assessed by corrupting ECG data with sinusoidal baseline drift, shift, rescaling and noise, before encoding. The addition of these artefacts resulted in a significant drop in accuracy for seven other methods from prior art, while DP encoding maintained a baseline AUC of 0.88 under drift, shift and rescaling. DP achieved superior performance to unencoded inputs in the presence of shift (AUC under 1mV shift: 0.91 vs 0.62), and rescaling artefacts (AUC 0.91 vs 0.79). Thus, DP encoding is a simple method by which robustness to common ECG artefacts may be improved for automated ECG analysis and interpretation.",
        "citation_title": "Baseline Drift Tolerant Signal Encoding for ECG Classification with Deep Learning",
        "date_delivered": "[Submitted on 26 Apr 2024]"
    },
    {
        "abstract": "This research paper explores ways to apply Federated Learning (FL) and Differential Privacy (DP) techniques to population-scale Electrocardiogram (ECG) data. The study learns a multi-label ECG classification model using FL and DP based on 1,565,849 ECG tracings from 7 hospitals in Alberta, Canada. The FL approach allowed collaborative model training without sharing raw data between hospitals while building robust ECG classification models for diagnosing various cardiac conditions. These accurate ECG classification models can facilitate the diagnoses while preserving patient confidentiality using FL and DP techniques. Our results show that the performance achieved using our implementation of the FL approach is comparable to that of the pooled approach, where the model is trained over the aggregating data from all hospitals. Furthermore, our findings suggest that hospitals with limited ECGs for training can benefit from adopting the FL model compared to single-site training. In addition, this study showcases the trade-off between model performance and data privacy by employing DP during model training. Our code is available at this https URL.",
        "citation_title": "Federated Learning and Differential Privacy Techniques on Multi-hospital Population-scale Electrocardiogram Data",
        "date_delivered": "[Submitted on 26 Apr 2024]"
    },
    {
        "abstract": "The conversion of brain activity into text using electroencephalography (EEG) has gained significant traction in recent years. Many researchers are working to develop new models to decode EEG signals into text form. Although this area has shown promising developments, it still faces numerous challenges that necessitate further improvement. It's important to outline this area's recent developments and future research directions. In this review article, we thoroughly summarize the progress in EEG-to-text conversion. Firstly, we talk about how EEG-to-text technology has grown and what problems we still face. Secondly, we discuss existing techniques used in this field. This includes methods for collecting EEG data, the steps to process these signals, and the development of systems capable of translating these signals into coherent text. We conclude with potential future research directions, emphasizing the need for enhanced accuracy, reduced system constraints, and the exploration of novel applications across varied sectors. By addressing these aspects, this review aims to contribute to developing more accessible and effective Brain-Computer Interface (BCI) technology for a broader user base.",
        "citation_title": "Unveiling Thoughts: A Review of Advancements in EEG Brain Signal Decoding into Text",
        "date_delivered": "[Submitted on 26 Apr 2024]"
    },
    {
        "abstract": "In vibration-based condition monitoring, optimal filter design improves fault detection by enhancing weak fault signatures within vibration signals. This process involves optimising a derived objective function from a defined objective. The objectives are often based on proxy health indicators to determine the filter's parameters. However, these indicators can be compromised by irrelevant extraneous signal components and fluctuating operational conditions, affecting the filter's efficacy. Fault detection primarily uses the fault component's prominence in the squared envelope spectrum, quantified by a squared envelope spectrum-based signal-to-noise ratio. New optimal filter objective functions are derived from the proposed generalised envelope spectrum-based signal-to-noise objective for machines operating under variable speed conditions. Instead of optimising proxy health indicators, the optimal filter coefficients of the formulation directly maximise the squared envelope spectrum-based signal-to-noise ratio over targeted frequency bands using standard gradient-based optimisers. Four derived objective functions from the proposed objective effectively outperform five prominent methods in tests on three experimental datasets.",
        "citation_title": "Generalised envelope spectrum-based signal-to-noise objectives: Formulation, optimisation and application for gear fault detection under time-varying speed conditions",
        "date_delivered": "[Submitted on 26 Apr 2024]"
    },
    {
        "abstract": "Unmanned aerial vehicles (UAVs) are widely applied in multiple fields, which emphasizes the challenge of obtaining UAV flight information to ensure the airspace safety. UAVs equipped with automatic dependent surveillance-broadcast (ADS-B) devices are capable of sending flight information to nearby aircrafts and ground stations (GSs). However, the saturation of limited frequency bands of ADS-B leads to interferences among UAVs and impairs the monitoring performance of GS to civil planes. To address this issue, the integration of the 5th generation mobile communication technology (5G) with ADS-B is proposed for UAV operations in this paper. Specifically, a hierarchical structure is proposed, in which the high-altitude central UAV is equipped with ADS-B and the low-altitude central UAV utilizes 5G modules to transmit flight information. Meanwhile, based on the mobile edge computing technique, the flight information of sub-UAVs is offloaded to the central UAV for further processing, and then transmitted to GS. We present the deterministic model and stochastic geometry based model to build the air-to-ground channel and air-to-air channel, respectively. The effectiveness of the proposed monitoring system is verified via simulations and experiments. This research contributes to improving the airspace safety and advancing the air traffic flow management.",
        "citation_title": "Joint ADS-B in 5G for Hierarchical Aerial Networks: Performance Analysis and Optimization",
        "date_delivered": "[Submitted on 29 Apr 2024]"
    },
    {
        "abstract": "Cross-center data heterogeneity and annotation unreliability significantly challenge the intelligent diagnosis of diseases using brain signals. A notable example is the EEG-based diagnosis of neurodegenerative diseases, which features subtler abnormal neural dynamics typically observed in small-group settings. To advance this area, in this work, we introduce a transferable framework employing Manifold Attention and Confidence Stratification (MACS) to diagnose neurodegenerative disorders based on EEG signals sourced from four centers with unreliable annotations. The MACS framework's effectiveness stems from these features: 1) The Augmentor generates various EEG-represented brain variants to enrich the data space; 2) The Switcher enhances the feature space for trusted samples and reduces overfitting on incorrectly labeled samples; 3) The Encoder uses the Riemannian manifold and Euclidean metrics to capture spatiotemporal variations and dynamic synchronization in EEG; 4) The Projector, equipped with dual heads, monitors consistency across multiple brain variants and ensures diagnostic accuracy; 5) The Stratifier adaptively stratifies learned samples by confidence levels throughout the training process; 6) Forward and backpropagation in MACS are constrained by confidence stratification to stabilize the learning system amid unreliable annotations. Our subject-independent experiments, conducted on both neurocognitive and movement disorders using cross-center corpora, have demonstrated superior performance compared to existing related algorithms. This work not only improves EEG-based diagnostics for cross-center and small-setting brain diseases but also offers insights into extending MACS techniques to other data analyses, tackling data heterogeneity and annotation unreliability in multimedia and multimodal content understanding.",
        "citation_title": "EEG-MACS: Manifold Attention and Confidence Stratification for EEG-based Cross-Center Brain Disease Diagnosis under Unreliable Annotations",
        "date_delivered": "[Submitted on 29 Apr 2024]"
    },
    {
        "abstract": "Signal detection and modulation classification are two crucial tasks in various wireless communication systems. Different from prior works that investigate them independently, this paper studies the joint signal detection and automatic modulation classification (AMC) by considering a realistic and complex scenario, in which multiple signals with different modulation schemes coexist at different carrier frequencies. We first generate a coexisting RADIOML dataset (CRML23) to facilitate the joint design. Different from the publicly available AMC dataset ignoring the signal detection step and containing only one signal, our synthetic dataset covers the more realistic multiple-signal coexisting scenario. Then, we present a joint framework for detection and classification (JDM) for such a multiple-signal coexisting environment, which consists of two modules for signal detection and AMC, respectively. In particular, these two modules are interconnected using a designated data structure called \"proposal\". Finally, we conduct extensive simulations over the newly developed dataset, which demonstrate the effectiveness of our designs. Our code and dataset are now available as open-source (this https URL).",
        "citation_title": "Joint Signal Detection and Automatic Modulation Classification via Deep Learning",
        "date_delivered": "[Submitted on 29 Apr 2024]"
    },
    {
        "abstract": "Parkinson's disease is a widespread neurodegenerative condition necessitating early diagnosis for effective intervention. This paper introduces an innovative method for diagnosing Parkinson's disease through the analysis of human EEG signals, employing a Support Vector Machine (SVM) classification model. this research presents novel contributions to enhance diagnostic accuracy and reliability. Our approach incorporates a comprehensive review of EEG signal analysis techniques and machine learning methods. Drawing from recent studies, we have engineered an advanced SVM-based model optimized for Parkinson's disease diagnosis. Utilizing cutting-edge feature engineering, extensive hyperparameter tuning, and kernel selection, our method achieves not only heightened diagnostic accuracy but also emphasizes model interpretability, catering to both clinicians and researchers. Moreover, ethical concerns in healthcare machine learning, such as data privacy and biases, are conscientiously addressed. We assess our method's performance through experiments on a diverse dataset comprising EEG recordings from Parkinson's disease patients and healthy controls, demonstrating significantly improved diagnostic accuracy compared to conventional techniques. In conclusion, this paper introduces an innovative SVM-based approach for diagnosing Parkinson's disease from human EEG signals. Building upon the IEEE framework and previous research, its novelty lies in the capacity to enhance diagnostic accuracy while upholding interpretability and ethical considerations for practical healthcare applications. These advances promise to revolutionize early Parkinson's disease detection and management, ultimately contributing to enhanced patient outcomes and quality of life.",
        "citation_title": "Diagnosis of Parkinson's Disease Using EEG Signals and Machine Learning Techniques: A Comprehensive Study",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Safe control for dynamical systems is critical, yet the presence of unknown dynamics poses significant challenges. In this paper, we present a learning-based control approach for tracking control of a class of high-order systems, operating under the constraint of partially observable states. The uncertainties inherent within the systems are modeled by kernel ridge regression, leveraging the proposed strategic data acquisition approach with limited state measurements. To achieve accurate trajectory tracking, a state observer that seamlessly integrates with the control law is devised. The analysis of the guaranteed control performance is conducted using Lyapunov theory due to the deterministic prediction error bound of kernel ridge regression, ensuring the adaptability of the approach in safety-critical scenarios. To demonstrate the effectiveness of our proposed approach, numerical simulations are performed, underscoring its contributions to the advancement of control strategies.",
        "citation_title": "Kernel-based Learning for Safe Control of Discrete-Time Unknown Systems under Incomplete Observations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Recent advancements in nanopore sequencing technology, particularly the R10 nanopore from Oxford Nanopore Technology, have necessitated the development of improved data processing methods to utilize their potential for more than 9-mer resolution fully. The processing of the ion currents predominantly utilizes neural network-based methods known for their high basecalling accuracy but face developmental bottlenecks at higher resolutions. In light of this, we introduce the Helicase Hidden Markov Model (HHMM), a novel framework designed to incorporate the dynamics of the helicase motor protein alongside the nucleotide sequence during nanopore sequencing. This model supports the analysis of millions of distinct states, enhancing our understanding of raw ion currents and their alignment with nucleotide sequences. Our findings demonstrate the utility of HHMM not only as a potent visualization tool but also as an effective base for developing advanced basecalling algorithms. This approach offers a promising avenue for leveraging the full capabilities of emerging high-resolution nanopore sequencing technologies.",
        "citation_title": "Modelling the nanopore sequencing process with Helicase HMMs",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The growing scale and complexity of safety-critical control systems underscore the need to evolve current control architectures aiming for the unparalleled performances achievable through state-of-the-art optimization and machine learning algorithms. However, maintaining closed-loop stability while boosting the performance of nonlinear control systems using data-driven and deep-learning approaches stands as an important unsolved challenge. In this paper, we tackle the performance-boosting problem with closed-loop stability guarantees. Specifically, we establish a synergy between the Internal Model Control (IMC) principle for nonlinear systems and state-of-the-art unconstrained optimization approaches for learning stable dynamics. Our methods enable learning over arbitrarily deep neural network classes of performance-boosting controllers for stable nonlinear systems; crucially, we guarantee Lp closed-loop stability even if optimization is halted prematurely, and even when the ground-truth dynamics are unknown, with vanishing conservatism in the class of stabilizing policies as the model uncertainty is reduced to zero. We discuss the implementation details of the proposed control schemes, including distributed ones, along with the corresponding optimization procedures, demonstrating the potential of freely shaping the cost functions through several numerical experiments.",
        "citation_title": "Learning to Boost the Performance of Stable Nonlinear Systems",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Interplanetary links (IPL) serve as crucial enablers for space exploration, facilitating secure and adaptable space missions. An integrated IPL with inter-satellite communication (IP-ISL) establishes a unified deep space network, expanding coverage and reducing atmospheric losses. The challenges, including irregularities in charged density, hardware impairments, and hidden celestial body brightness are analyzed with a reflectarray-based IP-ISL between Earth and Moon orbiters. It is observed that $10^{-8}$ order severe hardware impairments with intense solar plasma density drops an ideal system's spectral efficiency (SE) from $\\sim\\!38~\\textrm{(bit/s)/Hz}$ down to $0~\\textrm{(bit/s)/Hz}$. An ideal full angle of arrival fluctuation recovery with full steering range achieves $\\sim\\!20~\\textrm{(bit/s)/Hz}$ gain and a limited beamsteering with a numerical reflectarray design achieves at least $\\sim\\!1~\\textrm{(bit/s)/Hz}$ gain in severe hardware impairment cases.",
        "citation_title": "On the Role of Reflectarrays for Interplanetary Links",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper studies the controller synthesis problem for nonlinear control systems under linear temporal logic (LTL) specifications using zonotope techniques. A local-to-global control strategy is proposed for the desired specification expressed as an LTL formula. First, a novel approach is developed to divide the state space into finite zonotopes and constrained zonotopes, which are called cells and allowed to intersect with the neighbor cells. Second, from the intersection relation, a graph among all cells is generated to verify the realization of the accepting path for the LTL formula. The realization verification determines if there is a need for the control design, and also results in finite local LTL formulas. Third, once the accepting path is realized, a novel abstraction-based method is derived for the controller design. In particular, we only focus on the cells from the realization verification and approximate each cell thanks to properties of zonotopes. Based on local symbolic models and local LTL formulas, an iterative synthesis algorithm is proposed to design all local abstract controllers, whose existence and combination establish the global controller for the LTL formula. Finally, the proposed framework is illustrated via a path planning problem of mobile robots.",
        "citation_title": "Zonotope-based Symbolic Controller Synthesis for Linear Temporal Logic Specifications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Limited diversity in standardized benchmarks for evaluating audio representation learning (ARL) methods may hinder systematic comparison of current methods' capabilities. We present ARCH, a comprehensive benchmark for evaluating ARL methods on diverse audio classification domains, covering acoustic events, music, and speech. ARCH comprises 12 datasets, that allow us to thoroughly assess pre-trained SSL models of different sizes. ARCH streamlines benchmarking of ARL techniques through its unified access to a wide range of domains and its ability to readily incorporate new datasets and models. To address the current lack of open-source, pre-trained models for non-speech audio, we also release new pre-trained models that demonstrate strong performance on non-speech datasets. We argue that the presented wide-ranging evaluation provides valuable insights into state-of-the-art ARL methods, and is useful to pinpoint promising research directions.",
        "citation_title": "Benchmarking Representations for Speech, Music, and Acoustic Events",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Cell inconsistency within a lithium-ion battery system poses a significant challenge in maximizing the system operational time. This study presents an optimization-driven active balancing method to minimize the effects of cell inconsistency on the system operational time while simultaneously satisfying the system output power demand and prolonging the system operational time in energy storage applications. The proposed method utilizes a fractional order model to forecast the terminal voltage dynamics of each cell within a battery system, enhanced with a particle-swarm-optimisation-genetic algorithm for precise parameter identification. It is implemented under two distinct cell-level balancing topologies: independent cell balancing and differential cell balancing. Subsequently, the current distribution for each topology is determined by resolving two optimization control problems constrained by the battery's operational specifications and power demands. The effectiveness of the proposed method is validated by extensive experiments based on the two balancing topologies. The results demonstrate that the proposed method increases the operational time by 3.2%.",
        "citation_title": "Active Cell Balancing for Extended Operational Time of Lithium-Ion Battery Systems in Energy Storage Applications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper offers a formal framework for the rare collision risk estimation of autonomous vehicles (AVs) with multi-agent situation awareness, affected by different sources of noise in a complex dynamic environment. In our proposed setting, the situation awareness is considered for one of the ego vehicles by aggregating a range of diverse information gathered from other vehicles into a vector. We model AVs equipped with the situation awareness as general stochastic hybrid systems (GSHS) and assess the probability of collision in a lane-change scenario where two self-driving vehicles simultaneously intend to switch lanes into a shared one, while utilizing the time-to-collision measure for decision-making as required. Due to the substantial data requirements of simulation-based methods for the rare collision risk estimation, we leverage a multi-level importance splitting technique, known as interacting particle system-based estimation with fixed assignment splitting (IPS-FAS). This approach allows us to estimate the probability of a rare event by employing a group of interacting particles. Specifically, each particle embodies a system trajectory and engages with others through resampling and branching, focusing computational resources on trajectories with the highest probability of encountering the rare event. The effectiveness of our proposed approach is demonstrated through an extensive simulation of a lane-change scenario.",
        "citation_title": "Rare Collision Risk Estimation of Autonomous Vehicles with Multi-Agent Situation Awareness",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this article, the effect a moving target has on the signal-to-interference-plus-noise-ratio (SINR) for high time-bandwidth noise radars is investigated. To compensate for cell migration we apply a computationally efficient stretch processing algorithm that is tailored for batched processing and suitable for implementation onto a real-time radar processor. The performance of the algorithm is studied using experimental data. In the experiment, pseudorandom noise, with a bandwidth of 100 MHz, is generated and transmitted in real-time. An unmanned aerial vehicle (UAV), flown at a speed of 11 m/s, is acting as a target. For an integration time of 1 s, the algorithm is shown to yield an increase in SINR of roughly 13 dB, compared to no compensation. It is also shown that coherent integration times of 2.5 s can be achieved.",
        "citation_title": "Experimental Evaluation of Moving Target Compensation in High Time-Bandwidth Noise Radar",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "For a class of uncertain systems, a non-overshooting sliding mode control is presented to make them globally exponentially stable and without overshoot. Even when the unknown stochastic disturbance exists, and the time-variant reference trajectory is required, the strict non-overshooting stabilization is still achieved. The control law design is based on a desired second-order sliding mode (2-sliding mode), which successively includes two bounded-gain subsystems. Non-overshooting stability requires that the system gains depend on the initial values of system variables. In order to obtain the global non-overshooting stability, the first subsystem with non-overshooting reachability compresses the initial values of the second subsystem to a given bounded range. By partitioning these initial values, the bounded system gains are determined to satisfy the robust non-overshooting stability. In order to reject the chattering in the controller output, a tanh-function-based sliding mode is developed for the design of smoothed non-overshooting controller. The proposed method is applied to a UAV trajectory tracking when the disturbances and uncertainties exist. The control laws are designed to implement the non-overshooting stabilization in position and attitude. Finally, the effectiveness of the proposed method is demonstrated by the flying tests.",
        "citation_title": "Non-overshooting sliding mode for UAV control",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "At Diamond Light Source, the UK's national synchrotron facility, electron beam disturbances are attenuated by the fast orbit feedback (FOFB), which controls a cross-directional (CD) system with hundreds of inputs and outputs. Due to the inability to measure the disturbance spectrum in real-time, the closed-loop sensitivity of the FOFB cannot be evaluated, making it difficult to compare FOFB algorithms and detect faults. Existing methods rely on comparing open-loop with closed-loop measurements, but they are prone to instabilities and actuator saturation because of the system's strong directionality. Here, we introduce a reference signal to estimate the complementary sensitivity in closed loop. By decoupling the system into sets of single-input, single-output (SISO) systems, we design the reference mode-by-mode to accommodate the system's strong directionality. This allows SISO system identification to be used, making our approach suitable for large-scale systems. Additionally, we derive lower bounds on reference amplitudes to achieve a predefined estimation error bound in the presence of disturbances and measurement noise. Our approach not only enables performance estimation of ill-conditioned CD systems in closed-loop but also provides a signal for fault detection. Its potential applications extend to other CD systems, such as papermaking, steel rolling, or battery manufacturing processes.",
        "citation_title": "Closed-Loop Sensitivity Identification for Cross-Directional Systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Satellite constellation systems are becoming more attractive to provide communication services worldwide, especially in areas without network connectivity. While optimizing satellite gateway placement is crucial for operators to minimize deployment and operating costs, reducing the number of gateways may require more inter-satellite link hops to reach the ground network, thereby increasing latency. Therefore, it is of significant importance to develop a framework that optimizes gateway placement, dynamic routing, and flow management in inter-satellite links to enhance network performance. To this end, we model an optimization problem as a mixed-integer problem with a cost function combining the number of gateways, flow allocation, and traffic latency, allowing satellite operators to set priorities based on their policies. Our simulation results indicate that the proposed approach effectively reduces the number of active gateways by selecting their most appropriate locations while balancing the trade-off between the number of gateways and traffic latency. Furthermore, we demonstrate the impact of different weights in the cost function on performance through comparative analysis.",
        "citation_title": "Optimizing Satellite Network Infrastructure: A Joint Approach to Gateway Placement and Routing",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In outlier hypothesis testing, one aims to detect outlying sequences among a given set of sequences, where most sequences are generated i.i.d. from a nominal distribution while outlying sequences (outliers) are generated i.i.d. from a different anomalous distribution. Most existing studies focus on discrete-valued sequences, where each data sample takes values in a finite set. To account for practical scenarios where data sequences usually take real values, we study outlier hypothesis testing for continuous sequences when both the nominal and anomalous distributions are \\emph{unknown}. Specifically, we propose distribution free tests and prove that the probabilities of misclassification error, false reject and false alarm decay exponentially fast for three different test designs: fixed-length test, sequential test, and two-phase test. In a fixed-length test, one fixes the sample size of each observed sequence; in a sequential test, one takes a sample sequentially from each sequence per unit time until a reliable decision can be made; in a two-phase test, one adapts the sample size from two different fixed values. Remarkably, the two-phase test achieves a good balance between test design complexity and theoretical performance. We first consider the case of at most one outlier, and then generalize our results to the case with multiple outliers where the number of outliers is unknown.",
        "citation_title": "Exponentially Consistent Outlier Hypothesis Testing for Continuous Sequences",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper considers the beamforming optimization for sensing a point-like scatterer using a bistatic multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) radar, which could be part of a joint communication and sensing system. The goal is to minimize the Cram\u00e9r-Rao bound on the target position's estimation error, where the radar already knows an approximate position that is taken into account in the optimization. The optimization allows for beamforming with more than one beam per subcarrier. Optimal solutions for the beamforming are discussed for known and unknown channel gain. Numerical results show that beamforming with at most one beam per subcarrier is optimal for certain parameters, but for other parameters, optimal solutions need two beams on some subcarriers. In addition, the degree of freedom in selecting which end of the bistatic radar should transmit and receive is considered.",
        "citation_title": "Optimal Beamforming for Bistatic MIMO Sensing",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This letter proposes a few-shot physics-guided spatial temporal graph convolutional network (FPG-STGCN) to fast solve unit commitment (UC). Firstly, STGCN is tailored to parameterize UC. Then, few-shot physics-guided learning scheme is proposed. It exploits few typical UC solutions yielded via commercial optimizer to escape from local minimum, and leverages the augmented Lagrangian method for constraint satisfaction. To further enable both feasibility and continuous relaxation for integers in learning process, straight-through estimator for Tanh-Sign composition is proposed to fully differentiate the mixed integer solution space. Case study on the IEEE benchmark justifies that, our method bests mainstream learning ways on UC feasibility, and surpasses traditional solver on efficiency.",
        "citation_title": "Learning-to-solve unit commitment based on few-shot physics-guided spatial-temporal graph convolution network",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we investigate the localization of targets in the presence of multiple scattering. We focus on the often omitted scenario in which measurement data is affected by multiple scattering, and a simpler model is employed in the estimation. We study the impact of such model mismatch by means of the Misspecified Cram\u00e9r-Rao Bound (MCRB). In numerical simulations inspired by tomographic inspection in ultrasound nondestructive testing, the MCRB is shown to correctly describe the estimation variance of localization parameters under misspecification of the wave propagation model. We provide extensive discussion on the utility of the MCRB in the practical task of verifying whether a chosen misspecified model is suitable for localization based on the properties of the maximum likelihood estimator and the nuanced distinction between bias and parameter space differences. Finally, we highlight that careful interpretation is needed whenever employing the classical CRB in the presence of mismatch through numerical examples based on the Born approximation and other simplified propagation models stemming from it.",
        "citation_title": "Misspecification of Multiple Scattering in Scalar Wave Fields and its Impact in Ultrasound Tomography",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper introduces a landing guidance strategy for reusable launch vehicles (RLVs) using a model predictive approach based on sequential convex programming (SCP). The proposed approach devises two distinct optimal control problems (OCPs): planning a fuel-optimal landing trajectory that accommodates practical path constraints specific to RLVs, and determining real-time optimal tracking commands. This dual optimization strategy allows for reduced computational load through adjustable prediction horizon lengths in the tracking task, achieving near closed-loop performance. Enhancements in model fidelity for the tracking task are achieved through an alternative rotational dynamics representation, enabling a more stable numerical solution of the OCP and accounting for vehicle transient dynamics. Furthermore, modifications of aerodynamic force in both planning and tracking phases are proposed, tailored for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding computational complexity. Extensive 6-DOF simulation experiments validate the effectiveness and improved guidance performance of the proposed algorithm.",
        "citation_title": "Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch Vehicles",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Fronthaul quantization causes a significant distortion in cell-free massive MIMO networks. Due to the limited capacity of fronthaul links, information exchange among access points (APs) must be quantized significantly. Furthermore, the complexity of the multiplication operation in the base-band processing unit increases with the number of bits of the operands. Thus, quantizing the APs' signal vector reduces the complexity of signal estimation in the base-band processing unit. Most recent works consider the direct quantization of the received signal vectors at each AP without any pre-processing. However, the signal vectors received at different APs are correlated mutually (inter-AP correlation) and also have correlated dimensions (intra-AP correlation). Hence, cooperative quantization of APs fronthaul can help to efficiently use the quantization bits at each AP and further reduce the distortion imposed on the quantized vector at the APs. This paper considers a daisy chain fronthaul and three different processing sequences at each AP. We show that 1) de-correlating the received signal vector at each AP from the corresponding vectors of the previous APs (inter-AP de-correlation) and 2) de-correlating the dimensions of the received signal vector at each AP (intra-AP de-correlation) before quantization helps to use the quantization bits at each AP more efficiently than directly quantizing the received signal vector without any pre-processing and consequently, improves the bit error rate (BER) and normalized mean square error (NMSE) of users signal estimation.",
        "citation_title": "Joint Sequential Fronthaul Quantization and Hardware Complexity Reduction in Uplink Cell-Free Massive MIMO Networks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We address a joint trajectory planning, user association, resource allocation, and power control problem to maximize proportional fairness in the aerial IoT network, considering practical end-to-end quality-of-service (QoS) and communication schedules. Though the problem is rather ancient, apart from the fact that the previous approaches have never considered user- and time-specific QoS, we point out a prevalent mistake in coordinate optimization approaches adopted by the majority of the literature. Coordinate optimization approaches, which repetitively optimize radio resources for a fixed trajectory and vice versa, generally converge to local optima when all variables are differentiable. However, these methods often stagnate at a non-stationary point, significantly degrading the network utility in mixed-integer problems such as joint trajectory and radio resource optimization. We detour this problem by converting the formulated problem into the Markov decision process (MDP). Exploiting the beneficial characteristics of the MDP, we design a non-iterative framework that cooperatively optimizes trajectory and radio resources without initial trajectory choice. The proposed framework can incorporate various trajectory planning algorithms such as the genetic algorithm, tree search, and reinforcement learning. Extensive comparisons with diverse baselines verify that the proposed framework significantly outperforms the state-of-the-art method, nearly achieving the global optimum. Our implementation code is available at this https URL.",
        "citation_title": "Non-iterative Optimization of Trajectory and Radio Resource for Aerial Network",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "New spectrum allocations in the 4--8 GHz FR1(C) and 7--24 GHz FR3 mid-band frequency spectrum are being considered for 5G/6G cellular deployments. This paper presents results from the world's first comprehensive indoor hotspot (InH) propagation measurement campaign at 6.75 GHz and 16.95 GHz in the NYU WIRELESS Research Center using a 1 GHz wideband channel sounder system over distances from 11 to 97 m in line-of-sight (LOS) and non-LOS (NLOS). Analysis of directional and omnidirectional path loss (PL) using the close-in free space 1 m reference distance model shows a familiar waveguiding effect in LOS with an omnidirectional path loss exponent (PLE) of 1.40 at 6.75 GHz and 1.32 at 16.95 GHz. Compared to mmWave frequencies, the directional NLOS PLEs are lower at FR3 and FR1(C), while omnidirectional NLOS PLEs are similar, suggesting better propagation distances at lower frequencies for links with omnidirectional antennas at both ends of the links, but also, importantly, showing that higher gain antennas will offer better coverage at higher frequencies when antenna apertures are kept same over all frequencies. Comparison of the omnidirectional and directional RMS delay spread (DS) at FR1(C) and FR3 with mmWave frequencies indicates a clear decrease with increasing frequency. The mean spatial lobe and omnidirectional RMS angular spread (AS) is found to be wider at 6.75 GHz compared to 16.95 GHz indicating more multipath components are found in the azimuthal spatial domain at lower frequencies.",
        "citation_title": "Propagation measurements and channel models in Indoor Environment at 6.75 GHz FR1(C) and 16.95 GHz FR3 Upper-mid band Spectrum for 5G and 6G",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The 4--8 GHz FR1(C) and 7--24 GHz upper mid-band FR3 spectrum are promising new 6G spectrum allocations being considered by the International Telecommunications Union (ITU) and major governments around the world. There is an urgent need to understand the propagation behavior and radio coverage, outage, and material penetration for the global mobile wireless industry in both indoor and outdoor environments in these emerging frequency bands. This work presents measurements and models that describe the penetration loss in co-polarized and cross-polarized antenna configurations, exhibited by common materials found inside buildings and on building perimeters, including concrete, low-emissivity glass, wood, doors, drywall, and whiteboard at 6.75 GHz and 16.95 GHz. Measurement results show consistent lower penetration loss at 6.75 GHz compared to 16.95 GHz for all ten materials measured for co and cross-polarized antennas at incidence. For instance, the low-emissivity glass wall presents 33.7 dB loss at 6.75 GHz, while presenting 42.3 dB loss at 16.95 GHz. Penetration loss at these frequencies is contrasted with measurements at sub-6 GHz, mmWave and sub-THz frequencies along with 3GPP material penetration loss models. The results provide critical knowledge for future 5G and 6G cellular system deployments as well as refinements for the 3GPP material penetration models.",
        "citation_title": "Wideband Penetration Loss through Building Materials and Partitions at 6.75 GHz in FR1(C) and 16.95 GHz in the FR3 Upper Mid-band spectrum",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Electricity markets are experiencing a rapid increase in energy storage unit participation. Unlike conventional generation resources, quantifying the competitive operation and identifying if a storage unit is exercising market power is challenging, particularly in the context of multi-interval bidding strategies. We present a framework to differentiate strategic capacity withholding behaviors attributed to market power from inherent competitive bidding in storage unit strategies. Our framework evaluates the profitability of strategic storage unit participation, analyzing bidding behaviors as both price takers and price makers using a self-scheduling model, and investigates how they leverage market inefficiencies. Specifically, we propose a price sensitivity model derived from the linear supply function equilibrium model to examine the price-anticipating bidding strategy, effectively capturing the influence of market power. We introduce a sufficient ex-post analysis for market operators to identify potential exploitative behaviors by monitoring instances of withholding within the bidding profiles, ensuring market resilience and competitiveness. We discuss and verify applicability of the proposed framework to realistic settings. Our analysis substantiates commonly observed economic bidding behaviors of storage units. Furthermore, it demonstrates that significant price volatility offers considerable profit opportunities not only for participants possessing market power but also for typical strategic profit seekers.",
        "citation_title": "Market Power and Withholding Behavior of Energy Storage Units",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Computer-aided segmentation methods can assist medical personnel in improving diagnostic outcomes. While recent advancements like UNet and its variants have shown promise, they face a critical challenge: balancing accuracy with computational efficiency. Shallow encoder architectures in UNets often struggle to capture crucial spatial features, leading in inaccurate and sparse segmentation. To address this limitation, we propose a novel \\underline{P}rogressive \\underline{A}ttention based \\underline{M}obile \\underline{UNet} (\\underline{PAM-UNet}) architecture. The inverted residual (IR) blocks in PAM-UNet help maintain a lightweight framework, while layerwise \\textit{Progressive Luong Attention} ($\\mathcal{PLA}$) promotes precise segmentation by directing attention toward regions of interest during synthesis. Our approach prioritizes both accuracy and speed, achieving a commendable balance with a mean IoU of 74.65 and a dice score of 82.87, while requiring only 1.32 floating-point operations per second (FLOPS) on the Liver Tumor Segmentation Benchmark (LiTS) 2017 dataset. These results highlight the importance of developing efficient segmentation models to accelerate the adoption of AI in clinical practice.",
        "citation_title": "PAM-UNet: Shifting Attention on Region of Interest in Medical Images",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Autonomous vehicle control is growing in availability for new vehicles and there is a potential need to retrofit older vehicles with this capability. Additionally, automotive cybersecurity has become a significant concern in recent years due to documented attacks on vehicles. As a result, researchers have been exploring reverse engineering techniques to automate vehicle control and improve vehicle security and threat analysis. In prior work, a vehicle's accelerator and brake pedal controller area network (CAN) channels were identified using reverse engineering techniques without prior knowledge of the vehicle. However, the correlation results for deceleration were lower than those for acceleration, which may be able to be improved by incorporating data from an additional telemetry device. In this paper, a method that uses IMU and GPS data to reverse-engineer a vehicle's steering wheel position CAN channels, without prior knowledge of the vehicle, is presented. Using GPS data is shown to greatly improve correlation values for deceleration, particularly for the brake pedal CAN channels. This work demonstrates the efficacy of using these data sources for automotive CAN reverse engineering. This has potential uses in automotive vehicle control and for improving vehicle security and threat analysis.",
        "citation_title": "Analysis of the Efficacy of the Use of Inertial Measurement and Global Positioning System Data to Reverse Engineer Automotive CAN Bus Steering Signals",
        "date_delivered": "[Submitted on 27 Mar 2024]"
    },
    {
        "abstract": "Does Knowledge Distillation (KD) really work? Conventional wisdom viewed it as a knowledge transfer procedure where a perfect mimicry of the student to its teacher is desired. However, paradoxical studies indicate that closely replicating the teacher's behavior does not consistently improve student generalization, posing questions on its possible causes. Confronted with this gap, we hypothesize that diverse attentions in teachers contribute to better student generalization at the expense of reduced fidelity in ensemble KD setups. By increasing data augmentation strengths, our key findings reveal a decrease in the Intersection over Union (IoU) of attentions between teacher models, leading to reduced student overfitting and decreased fidelity. We propose this low-fidelity phenomenon as an underlying characteristic rather than a pathology when training KD. This suggests that stronger data augmentation fosters a broader perspective provided by the divergent teacher ensemble and lower student-teacher mutual information, benefiting generalization performance. These insights clarify the mechanism on low-fidelity phenomenon in KD. Thus, we offer new perspectives on optimizing student model performance, by emphasizing increased diversity in teacher attentions and reduced mimicry behavior between teachers and student.",
        "citation_title": "Why does Knowledge Distillation Work? Rethink its Attention and Fidelity Mechanism",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Digital phased arrays have often been disregarded for millimeter-wave communications since the analog-to-digital converters (ADCs) are power-hungry. In this paper, we provide a different perspective on this matter by demonstrating analytically and numerically how the ADC resolution can be reduced when using digital phased arrays. We perform a theoretical analysis of the quantization noise characteristics for an OFDM signal received and processed by a digital phased array, using Gaussian approximation of the OFDM signal. In particular, we quantify the quantization noise suppression factor analytically and numerically. This factor describes how much the coherent combining reduces the quantization noise as a function of the number of antennas, which allows for reducing the ADC bit resolution. For instance in a 8-16 antenna digital phased array the ADC resolution can be reduced with 1-2 bits compared to the ADC required for an analog phased array.",
        "citation_title": "Analysis of Quantization Noise Suppression Gains in Digital Phased Arrays",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Linear representation learning is widely studied due to its conceptual simplicity and empirical utility in tasks such as compression, classification, and feature extraction. Given a set of points $[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n] = \\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and a vector $\\mathbf{y} \\in \\mathbb{R}^d$, the goal is to find coefficients $\\mathbf{w} \\in \\mathbb{R}^n$ so that $\\mathbf{X} \\mathbf{w} \\approx \\mathbf{y}$, subject to some desired structure on $\\mathbf{w}$. In this work we seek $\\mathbf{w}$ that forms a local reconstruction of $\\mathbf{y}$ by solving a regularized least squares regression problem. We obtain local solutions through a locality function that promotes the use of columns of $\\mathbf{X}$ that are close to $\\mathbf{y}$ when used as a regularization term. We prove that, for all levels of regularization and under a mild condition that the columns of $\\mathbf{X}$ have a unique Delaunay triangulation, the optimal coefficients' number of non-zero entries is upper bounded by $d+1$, thereby providing local sparse solutions when $d \\ll n$. Under the same condition we also show that for any $\\mathbf{y}$ contained in the convex hull of $\\mathbf{X}$ there exists a regime of regularization parameter such that the optimal coefficients are supported on the vertices of the Delaunay simplex containing $\\mathbf{y}$. This provides an interpretation of the sparsity as having structure obtained implicitly from the Delaunay triangulation of $\\mathbf{X}$. We demonstrate that our locality regularized problem can be solved in comparable time to other methods that identify the containing Delaunay simplex.",
        "citation_title": "Locality Regularized Reconstruction: Structured Sparsity and Delaunay Triangulations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In the problem of quickest change detection (QCD), a change occurs at some unknown time in the distribution of a sequence of independent observations. This work studies a QCD problem where the change is either a bad change, which we aim to detect, or a confusing change, which is not of our interest. Our objective is to detect a bad change as quickly as possible while avoiding raising a false alarm for pre-change or a confusing change. We identify a specific set of pre-change, bad change, and confusing change distributions that pose challenges beyond the capabilities of standard Cumulative Sum (CuSum) procedures. Proposing novel CuSum-based detection procedures, S-CuSum and J-CuSum, leveraging two CuSum statistics, we offer solutions applicable across all kinds of pre-change, bad change, and confusing change distributions. For both S-CuSum and J-CuSum, we provide analytical performance guarantees and validate them by numerical results. Furthermore, both procedures are computationally efficient as they only require simple recursive updates.",
        "citation_title": "Quickest Change Detection with Confusing Change",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present a fast trajectory optimization algorithm for the soft capture of uncooperative tumbling space objects. Our algorithm generates safe, dynamically feasible, and minimum-fuel trajectories for a six-degree-of-freedom servicing spacecraft to achieve soft capture (near-zero relative velocity at contact) between predefined locations on the servicer spacecraft and target body. We solve a convex problem by enforcing a convex relaxation of the field-of-view constraint, followed by a sequential convex program correcting the trajectory for collision avoidance. The optimization problems can be solved with a standard second-order cone programming solver, making the algorithm both fast and practical for implementation in flight software. We demonstrate the performance and robustness of our algorithm in simulation over a range of object tumble rates up to 10\u00b0/s.",
        "citation_title": "A Convex Formulation of the Soft-Capture Problem",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper investigates the differentiable dynamic modeling of mobile manipulators to facilitate efficient motion planning and physical design of actuators, where the actuator design is parameterized by physically meaningful motor geometry parameters. These parameters impact the manipulator's link mass, inertia, center-of-mass, torque constraints, and angular velocity constraints, influencing control authority in motion planning and trajectory tracking control. A motor's maximum torque/speed and how the design parameters affect the dynamics are modeled analytically, facilitating differentiable and analytical dynamic modeling. Additionally, an integrated locomotion and manipulation planning problem is formulated with direct collocation discretization, using the proposed differentiable dynamics and motor parameterization. Such dynamics are required to capture the dynamic coupling between the base and the manipulator. Numerical experiments demonstrate the effectiveness of differentiable dynamics in speeding up optimization and advantages in task completion time and energy consumption over established sequential motion planning approach. Finally, this paper introduces a simultaneous actuator design and motion planning framework, providing numerical results to validate the proposed differentiable modeling approach for co-design problems.",
        "citation_title": "A Differentiable Dynamic Modeling Approach to Integrated Motion Planning and Actuator Physical Design for Mobile Manipulators",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "As a popular distributed learning paradigm, federated learning (FL) over mobile devices fosters numerous applications, while their practical deployment is hindered by participating devices' computing and communication heterogeneity. Some pioneering research efforts proposed to extract subnetworks from the global model, and assign as large a subnetwork as possible to the device for local training based on its full computing and communications capacity. Although such fixed size subnetwork assignment enables FL training over heterogeneous mobile devices, it is unaware of (i) the dynamic changes of devices' communication and computing conditions and (ii) FL training progress and its dynamic requirements of local training contributions, both of which may cause very long FL training delay. Motivated by those dynamics, in this paper, we develop a wireless and heterogeneity aware latency efficient FL (WHALE-FL) approach to accelerate FL training through adaptive subnetwork scheduling. Instead of sticking to the fixed size subnetwork, WHALE-FL introduces a novel subnetwork selection utility function to capture device and FL training dynamics, and guides the mobile device to adaptively select the subnetwork size for local training based on (a) its computing and communication capacity, (b) its dynamic computing and/or communication conditions, and (c) FL training status and its corresponding requirements for local training contributions. Our evaluation shows that, compared with peer designs, WHALE-FL effectively accelerates FL training without sacrificing learning accuracy.",
        "citation_title": "WHALE-FL: Wireless and Heterogeneity Aware Latency Efficient Federated Learning over Mobile Devices via Adaptive Subnetwork Scheduling",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study the problem of stabilizing infinite-dimensional systems with input and output quantization. The closed-loop system we consider is subject to packet loss in the sensor-to-controller channels, whose duration is assumed to be averagely bounded. Given a bound on the initial state, we propose design methods for dynamic quantizers with zoom parameters. We show that the closed-loop state staring in a given region exponentially converges to zero if the bounds of quantization errors and packet-loss duration satisfy suitable conditions. Since the norms of the operators representing the system dynamics are used in the proposed quantization schemes, we also present methods for approximately computing the operator norms.",
        "citation_title": "Stabilization of infinite-dimensional systems under quantization and packet loss",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
        "citation_title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Motivated by the ideal peak-to-average-power ratio and radar sensing capability of traditional frequency-coded radar waveforms, this paper considers the frequency shift keying (FSK) based waveform for joint communications and radar (JCR). An analysis of the probability distributions of its ambiguity function (AF) sidelobe levels (SLs) and peak sidelobe level (PSL) is conducted to study the radar sensing capability of random FSK. Numerical results show that the independent frequency modulation introduces uncontrollable AF PSLs. In order to address this problem, the initial phases of waveform sub-pulses are designed by solving a min-max optimisation problem. Numerical results indicate that the optimisation-based phase design can effectively reduce the AF PSL to a level close to well-designed radar waveforms while having no impact on the data rate and the receiver complexity. For large numbers of waveform sub-pulses and modulation orders, the impact on the error probability is also insignificant.",
        "citation_title": "Can FSK Be Optimised for Integrated Sensing and Communications?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study how high charging rate demands from electric vehicles (EVs) in a power distribution grid may collectively cause its dynamic instability, and, accordingly, how a price incentivization strategy can be used to steer customers to settle for lesser charging rate demands so that these instabilities can be avoided. We pose the problem as a joint optimization and optimal control formulation. The optimization determines the optimal charging setpoints for EVs to minimize the $\\mathcal{H}_2$-norm of the transfer function of the grid model, while the optimal control simultaneously develops a linear quadratic regulator (LQR) based state-feedback control signal for the battery-currents of those EVs to jointly minimize the risk of grid instability. A subsequent algorithm is developed to determine how much customers may be willing to sacrifice their intended charging rate demands in return for financial incentives. Results are derived for both unidirectional and bidirectional charging, and validated using numerical simulations of multiple EV charging stations in the IEEE 33-bus power distribution model.",
        "citation_title": "Co-Optimization of EV Charging Control and Incentivization for Enhanced Power System Stability",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this study, we introduce Generative Manufacturing Systems (GMS) as a novel approach to effectively manage and coordinate autonomous manufacturing assets, thereby enhancing their responsiveness and flexibility to address a wide array of production objectives and human preferences. Deviating from traditional explicit modeling, GMS employs generative AI, including diffusion models and ChatGPT, for implicit learning from envisioned futures, marking a shift from a model-optimum to a training-sampling decision-making. Through the integration of generative AI, GMS enables complex decision-making through interactive dialogue with humans, allowing manufacturing assets to generate multiple high-quality global decisions that can be iteratively refined based on human feedback. Empirical findings showcase GMS's substantial improvement in system resilience and responsiveness to uncertainties, with decision times reduced from seconds to milliseconds. The study underscores the inherent creativity and diversity in the generated solutions, facilitating human-centric decision-making through seamless and continuous human-machine interactions.",
        "citation_title": "Generative manufacturing systems using diffusion models and ChatGPT",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still underperforms on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we examine its limitations, demonstrating the presence of speaker-related (gender, age) and model-related (resourcefulness and model size) bias. Despite that, we show that only model-related bias are amplified by quantization, impacting more low-resource languages and smaller models. Searching for a better compression approach, we propose DistilWhisper, an approach that is able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference.",
        "citation_title": "Efficient Compression of Multitask Multilingual Speech Models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A critical hindrance to realize frequency division duplex (FDD) massive multi-input multi-output (MIMO) systems is overhead associated with downlink channel state information at the transmitter (CSIT) acquisition. To address this challenge, we propose a novel framework that achieves robust performances while completely eliminating downlink CSIT training and feedback. Specifically, by exploiting partial frequency invariance of channel parameters between the uplink (UL) and downlink (DL), we adopt the 2D-Newtonized orthogonal matching pursuit (2D-NOMP) algorithm to reconstruct DL CSIT from UL training. Due to inherent discrepancies arising from a carrier frequency difference between two disjoint bands, however, the multi-user interference is inevitable. To overcome this, we propose a precoding method that employs rate-splitting multiple access (RSMA) and also develop an error covariance matrix (ECM) estimation method by using the observed Fisher information matrix (O-FIM). We find that this ECM estimation is crucial for our precoding design in maximizing the sum spectral efficiency (SE). Simulation results show that our method significantly improves the sum SE compared to other state-of-the-art approaches, underscoring the importance of our ECM estimation.",
        "citation_title": "Splitting Messages in the Dark- Rate-Splitting Multiple Access for FDD Massive MIMO Without CSI Feedback",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Rapid advancement of antenna technology catalyses the popularization of extremely large-scale multiple-input multiple-output (XL-MIMO) antenna arrays, which pose unique challenges for localization with the inescapable near-field effect. In this paper, we propose an efficient near-field localization algorithm by leveraging a sectored uniform circular array (sUCA). In particular, we first customize a backprojection algorithm in the polar coordinate for sUCA-enabled near-field localization, which facilitates the target detection procedure. We then analyze the resolutions in both angular and distance domains via deriving the interval of zero-crossing points, and further unravel the minimum required number of antennas to eliminate grating lobes. The proposed localization method is finally implemented using fast Fourier transform (FFT) to reduce computational complexity. Simulation results verify the resolution analysis and demonstrate that the proposed method remarkably outperforms conventional localization algorithms in terms of localization accuracy. Moreover, the low-complexity FFT implementation achieves an average runtime that is hundreds of times faster when large numbers of antenna elements are employed.",
        "citation_title": "Low-Complexity Near-Field Localization with XL-MIMO Sectored Uniform Circular Arrays",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recent transformer-based ASR models have achieved word-error rates (WER) below 4%, surpassing human annotator accuracy, yet they demand extensive server resources, contributing to significant carbon footprints. The traditional server-based architecture of ASR also presents privacy concerns, alongside reliability and latency issues due to network dependencies. In contrast, on-device (edge) ASR enhances privacy, boosts performance, and promotes sustainability by effectively balancing energy use and accuracy for specific applications. This study examines the effects of quantization, memory demands, and energy consumption on the performance of various ASR model inference on the NVIDIA Jetson Orin Nano. By analyzing WER and transcription speed across models using FP32, FP16, and INT8 quantization on clean and noisy datasets, we highlight the crucial trade-offs between accuracy, speeds, quantization, energy efficiency, and memory needs. We found that changing precision from fp32 to fp16 halves the energy consumption for audio transcription across different models, with minimal performance degradation. A larger model size and number of parameters neither guarantees better resilience to noise, nor predicts the energy consumption for a given transcription load. These, along with several other findings offer novel insights for optimizing ASR systems within energy- and memory-limited environments, crucial for the development of efficient on-device ASR solutions. The code and input data needed to reproduce the results in this article are open sourced are available on [this https URL].",
        "citation_title": "Deep Learning Models in Speech Recognition: Measuring GPU Energy Consumption, Impact of Noise and Model Quantization for Edge Deployment",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The rapidly growing diversity of concurrent applications from both different users and same devices calls for application-specific Quality of Experience (QoE) enhancement of future wireless communications. Achieving this goal relies on application-specific packet scheduling, as it is vital for achieving tailored QoE enhancement by realizing the application-specific Quality of Service (QoS) requirements and for optimal perceived QoE values. However, the intertwining diversified QoE perception mechanisms, fairness among concurrent applications, and the impact of network dynamics inevitably complicate tailored packet scheduling. To achieve concurrent application-specific QoE enhancement, the problem of multi-user multi-application packet scheduling in downlink 6G radio access network (RAN) is first formulated as a Markov decision process (MDP) problem in this paper. For solving this problem, a deep deterministic policy gradient (DDPG)-based solution is proposed. However, due to the high dimensionalities of both the state and action spaces, the trained DDPG agents might generate decisions causing unnecessary resource waste. Hence, a knowledge embedding method is proposed to adjust the decisions of the DDPG agents according to human insights. Extensive experiments are conducted, which demonstrate the superiority of DDPG-based packet schedulers over baseline algorithms and the effectiveness of the proposed knowledge embedding technique.",
        "citation_title": "Multi-User Multi-Application Packet Scheduling for Application-Specific QoE Enhancement Based on Knowledge-Embedded DDPG in 6G RAN",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recent advancements in deep learning have demonstrated remarkable performance comparable to human capabilities across various supervised computer vision tasks. However, the prevalent assumption of having an extensive pool of training data encompassing all classes prior to model training often diverges from real-world scenarios, where limited data availability for novel classes is the norm. The challenge emerges in seamlessly integrating new classes with few samples into the training data, demanding the model to adeptly accommodate these additions without compromising its performance on base classes. To address this exigency, the research community has introduced several solutions under the realm of few-shot class incremental learning (FSCIL).\nIn this study, we introduce an innovative FSCIL framework that utilizes language regularizer and subspace regularizer. During base training, the language regularizer helps incorporate semantic information extracted from a Vision-Language model. The subspace regularizer helps in facilitating the model's acquisition of nuanced connections between image and text semantics inherent to base classes during incremental training. Our proposed framework not only empowers the model to embrace novel classes with limited data, but also ensures the preservation of performance on base classes. To substantiate the efficacy of our approach, we conduct comprehensive experiments on three distinct FSCIL benchmarks, where our framework attains state-of-the-art performance.",
        "citation_title": "Few Shot Class Incremental Learning using Vision-Language models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In complex and unknown processes, global models are initially generated over the entire experimental space, but they often fail to provide accurate predictions in local areas. Recognizing this limitation, this study addresses the need for models that effectively represent both global and local experimental spaces. It introduces a novel machine learning (ML) approach: Polynomial Chaos Expanded Gaussian Process (PCEGP), leveraging polynomial chaos expansion (PCE) to calculate input-dependent hyperparameters of the Gaussian process (GP). This approach provides a mathematically interpretable method that incorporates non-stationary covariance functions and heteroscedastic noise estimation to generate locally adapted models. The model performance is compared to different algorithms in benchmark tests for regression tasks. The results demonstrate low prediction errors of the PCEGP in these benchmark applications, highlighting model performance that is often competitive with or superior to previous methods. A key advantage of the presented model is the transparency and traceability in the calculation of hyperparameters and model predictions.",
        "citation_title": "Polynomial Chaos Expanded Gaussian Process",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Simulating soil reflectance spectra is invaluable for soil-plant radiative modeling and training machine learning models, yet it is difficult as the intricate relationships between soil structure and its constituents. To address this, a fully data-driven soil optics generative model (SOGM) for simulation of soil reflectance spectra based on soil property inputs was developed. The model is trained on an extensive dataset comprising nearly 180,000 soil spectra-property pairs from 17 datasets. It generates soil reflectance spectra from text-based inputs describing soil properties and their values rather than only numerical values and labels in binary vector format. The generative model can simulate output spectra based on an incomplete set of input properties. SOGM is based on the denoising diffusion probabilistic model (DDPM). Two additional sub-models were also built to complement the SOGM: a spectral padding model that can fill in the gaps for spectra shorter than the full visible-near-infrared range (VIS-NIR; 400 to 2499 nm), and a wet soil spectra model that can estimate the effects of water content on soil reflectance spectra given the dry spectrum predicted by the SOGM. The SOGM was up-scaled by coupling with the Helios 3D plant modeling software, which allowed for generation of synthetic aerial images of simulated soil and plant scenes. It can also be easily integrated with soil-plant radiation model used for remote sensin research like PROSAIL. The testing results of the SOGM on new datasets that not included in model training proved that the model can generate reasonable soil reflectance spectra based on available property inputs. The presented models are openly accessible on: this https URL.",
        "citation_title": "A text-based, generative deep learning model for soil reflectance spectrum simulation in the VIS-NIR (400-2499 nm) bands",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider a wireless network with multiple single-antenna repeaters that amplify and instantaneously re-transmit the signals they receive to improve the channel rank and system coverage. Due to the positive feedback formed by inter-repeater interference, stability could become a critical issue. We investigate the problem of determining the maximum amplification gain that the repeaters can use without breaking the system stability. Specifically, we obtain a bound by using the Gershgorin disc theorem, which reveals that the maximum amplification gain is restricted by the sum of channel amplitude gains. We show by case studies the usefulness of the so-obtained bound and provide insights on how the repeaters should be deployed.",
        "citation_title": "Stability Analysis of Interacting Wireless Repeaters",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "3D Swin Transformer (3D-ST) known for its hierarchical attention and window-based processing, excels in capturing intricate spatial relationships within images. Spatial-spectral Transformer (SST), meanwhile, specializes in modeling long-range dependencies through self-attention mechanisms. Therefore, this paper introduces a novel method: an attentional fusion of these two transformers to significantly enhance the classification performance of Hyperspectral Images (HSIs). What sets this approach apart is its emphasis on the integration of attentional mechanisms from both architectures. This integration not only refines the modeling of spatial and spectral information but also contributes to achieving more precise and accurate classification results. The experimentation and evaluation of benchmark HSI datasets underscore the importance of employing disjoint training, validation, and test samples. The results demonstrate the effectiveness of the fusion approach, showcasing its superiority over traditional methods and individual transformers. Incorporating disjoint samples enhances the robustness and reliability of the proposed methodology, emphasizing its potential for advancing hyperspectral image classification.",
        "citation_title": "Transformers Fusion across Disjoint Samples for Hyperspectral Image Classification",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper investigates a Stacked Intelligent Metasurfaces (SIM)-assisted Integrated Sensing and Communications (ISAC) system. An extended target model is considered, where the BS aims to estimate the complete target response matrix relative to the SIM. Under the constraints of minimum Signal-to-Interference-plus-Noise Ratio (SINR) for the communication users (CUs) and maximum transmit power, we jointly optimize the transmit beamforming at the base station (BS) and the end-to-end transmission matrix of the SIM, to minimize the Cram\u00e9r-Rao Bound (CRB) for target estimation. Effective algorithms such as the alternating optimization (AO) and semidefinite relaxation (SDR) are employed to solve the non-convex SINR-constrained CRB minimization problem. Finally, we design and build an experimental platform for SIM, and evaluate the performance of the proposed algorithms for communication and sensing tasks.",
        "citation_title": "Multi-user ISAC through Stacked Intelligent Metasurfaces: New Algorithms and Experiments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Spatial understanding from vision is crucial for robots operating in unstructured environments. In the real world, spatial understanding is often an ill-posed problem. There are a number of powerful classical methods that accurately regress relative pose, however, these approaches often lack the ability to leverage data-derived priors to resolve ambiguities. In multi-robot systems, these challenges are exacerbated by the need for accurate and frequent position estimates of cooperating agents. To this end, we propose CoViS-Net, a cooperative, multi-robot, visual spatial foundation model that learns spatial priors from data. Unlike prior work evaluated primarily on offline datasets, we design our model specifically for online evaluation and real-world deployment on cooperative robots. Our model is completely decentralized, platform agnostic, executable in real-time using onboard compute, and does not require existing network infrastructure. In this work, we focus on relative pose estimation and local Bird's Eye View (BEV) prediction tasks. Unlike classical approaches, we show that our model can accurately predict relative poses without requiring camera overlap, and predict BEVs of regions not visible to the ego-agent. We demonstrate our model on a multi-robot formation control task outside the confines of the laboratory.",
        "citation_title": "CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A major obstacle to the development of effective monocular depth estimation algorithms is the difficulty in obtaining high-quality depth data that corresponds to collected RGB images. Collecting this data is time-consuming and costly, and even data collected by modern sensors has limited range or resolution, and is subject to inconsistencies and noise. To combat this, we propose a method of data generation in simulation using 3D synthetic environments and CycleGAN domain transfer. We compare this method of data generation to the popular NYUDepth V2 dataset by training a depth estimation model based on the DenseDepth structure using different training sets of real and simulated data. We evaluate the performance of the models on newly collected images and LiDAR depth data from a Husky robot to verify the generalizability of the approach and show that GAN-transformed data can serve as an effective alternative to real-world data, particularly in depth estimation.",
        "citation_title": "Domain-Transferred Synthetic Data Generation for Improving Monocular Depth Estimation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Initial alignment is one of the key technologies in strapdown inertial navigation system (SINS) to provide initial state information for vehicle attitude and navigation. For some situations, such as the attitude heading reference system, the position is not necessarily required or even available, then the self-alignment that does not rely on any external aid becomes very necessary. This study presents a new self-alignment method under swaying conditions, which can determine the latitude and attitude simultaneously by utilizing all observation vectors without solving the Wahba problem, and it is different from the existing methods. By constructing the dyadic tensor of each observation and reference vector itself, all equations related to observation and reference vectors are accumulated into one equation, where the latitude variable is extracted and solved according to the same eigenvalues of similar matrices on both sides of the equation, meanwhile the attitude is obtained by eigenvalue decomposition. Simulation and experiment tests verify the effectiveness of the proposed methods, and the alignment result is better than TRIAD in convergence speed and stability and comparable with OBA method in alignment accuracy with or without latitude. It is useful for guiding the design of initial alignment in autonomous vehicle applications.",
        "citation_title": "A New Self-Alignment Method without Solving Wahba Problem for SINS in Autonomous Vehicles",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recognizing and understanding implicit driving cues across diverse cultures is imperative for fostering safe and efficient global transportation systems, particularly when training new immigrants holding driving licenses from culturally disparate countries. Additionally, it is essential to consider cross-cultural differences in the development of Automated Driving features tailored to different countries. Previous piloting studies have compared and analyzed cross-cultural differences in selected implicit driving cues, but they typically examine only limited countries. However, a comprehensive worldwide comparison and analysis are lacking. This study conducts a thorough review of existing literature, online blogs, and expert insights from diverse countries to investigate cross-cultural disparities in driving behaviors, specifically focusing on implicit cues such as non-verbal communication (e.g., hand gestures, signal lighting, honking), norms, and social expectations. Through comparative analysis, variations in driving cues are illuminated across different cultural contexts. Based on the findings and identified gaps, a research roadmap is proposed for future research to further explore and address these differences, aiming to enhance intercultural communication, improve road safety, and increase transportation efficiency on a global scale. This paper presents the pioneering work towards a comprehensive understanding of the implicit driving cues across cultures. Moreover, this understanding will inform the development of automated driving systems tailored to different countries considering cross-cultural differences.",
        "citation_title": "Towards Understanding Worldwide Cross-cultural Differences in Implicit Driving Cues: Review, Comparative Analysis, and Research Roadmap",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Self-supervised learning for image denoising problems in the presence of denaturation for noisy data is a crucial approach in machine learning. However, theoretical understanding of the performance of the approach that uses denatured data is lacking. To provide better understanding of the approach, in this paper, we analyze a self-supervised denoising algorithm that uses denatured data in depth through theoretical analysis and numerical experiments. Through the theoretical analysis, we discuss that the algorithm finds desired solutions to the optimization problem with the population risk, while the guarantee for the empirical risk depends on the hardness of the denoising task in terms of denaturation levels. We also conduct several experiments to investigate the performance of an extended algorithm in practice. The results indicate that the algorithm training with denatured images works, and the empirical performance aligns with the theoretical results. These results suggest several insights for further improvement of self-supervised image denoising that uses denatured data in future directions.",
        "citation_title": "Investigating Self-Supervised Image Denoising with Denaturation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper is devoted to the estimation of the Lipschitz constant of neural networks using semidefinite programming. For this purpose, we interpret neural networks as time-varying dynamical systems, where the $k$-th layer corresponds to the dynamics at time $k$. A key novelty with respect to prior work is that we use this interpretation to exploit the series interconnection structure of neural networks with a dynamic programming recursion. Nonlinearities, such as activation functions and nonlinear pooling layers, are handled with integral quadratic constraints. If the neural network contains signal processing layers (convolutional or state space model layers), we realize them as 1-D/2-D/N-D systems and exploit this structure as well. We distinguish ourselves from related work on Lipschitz constant estimation by more extensive structure exploitation (scalability) and a generalization to a large class of common neural network architectures. To show the versatility and computational advantages of our method, we apply it to different neural network architectures trained on MNIST and CIFAR-10.",
        "citation_title": "Lipschitz constant estimation for general neural network architectures using control tools",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "An intelligent omni-surface (IOS) assisted holographic multiple-input and multiple-output architecture is conceived for $360^\\circ$ full-space coverage at a low energy consumption. The theoretical ergodic rate lower bound of our non-orthogonal multiple access (NOMA) scheme is derived based on the moment matching approximation method, while considering the signal distortion at transceivers imposed by hardware impairments (HWIs). Furthermore, the asymptotically ergodic rate lower bound is derived both for an infinite number of IOS elements and for continuous aperture surfaces. Both the theoretical analysis and the simulation results show that the achievable rate of the NOMA scheme is higher than that of its orthogonal multiple access counterpart. Furthermore, owing to the HWIs at the transceivers, the achievable rate saturates at high signal-to-noise ratio region, instead of reaching its theoretical maximum.",
        "citation_title": "Achievable Rate Analysis of Intelligent Omni-Surface Assisted NOMA Holographic MIMO Systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Reconfigurable holographic surfaces (RHSs) constitute a promising technique of supporting energy-efficient communications. In this paper, we formulate the energy efficiency maximization problem of the switch-controlled RHS-aided beamforming architecture by alternately optimizing the holographic beamformer at the RHS, the digital beamformer, the total transmit power and the power sharing ratio of each user. Specifically, to deal with this challenging non-convex optimization problem, we decouple it into three sub-problems. Firstly, the coefficients of RHS elements responsible for the holographic beamformer are optimized to maximize the sum of the eigen-channel gains of all users by our proposed low-complexity eigen-decomposition (ED) method. Then, the digital beamformer is designed by the singular value decomposition (SVD) method to support multi-user information transfer. Finally, the total transmit power and the power sharing ratio are alternately optimized, while considering the effect of transceiver hardware impairments (HWI). We theoretically derive the spectral efficiency and energy efficiency performance upper bound for the RHS-based beamforming architectures in the presence of HWIs. Our simulation results show that the switch-controlled RHS-aided beamforming architecture achieves higher energy efficiency than the conventional fully digital beamformer and the hybrid beamformer based on phase shift arrays (PSA). Moreover, considering the effect of HWI in the beamforming design can bring about further energy efficiency enhancements.",
        "citation_title": "Energy-Efficient Reconfigurable Holographic Surfaces Operating in the Presence of Realistic Hardware Impairments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In contrast to the conventional reconfigurable intelligent surfaces (RIS), intelligent omni-surfaces (IOS) are capable of full-space coverage of smart radio environments by simultaneously transmitting and reflecting the incident signals. In this paper, we investigate the ergodic spectral efficiency of IOS-aided systems for transmission over random channel links, while considering both realistic imperfect channel state information (CSI) and transceiver hardware impairments (HWIs). Firstly, we formulate the linear minimum mean square error estimator of the equivalent channel spanning from the user equipments (UEs) to the access point (AP), where the transceiver HWIs are also considered. Then, we apply a two-timescale protocol for designing the beamformer of the IOS-aided system. Specifically, for the active AP beamformer, the minimum mean square error combining method is employed, which relies on the estimated equivalent channels, on the statistical information of the channel estimation error, on the inter-user interference as well as on the HWIs at the AP and UEs. By contrast, the passive IOS beamformer is designed based on the statistical CSI for maximizing the upper bound of the ergodic spectral efficiency. The theoretical analysis and simulation results show that the transceiver HWIs have a significant effect on the ergodic spectral efficiency, especially in the high transmit power region. Furthermore, we show that the HWIs at the AP can be effectively compensated by deploying more AP antennas.",
        "citation_title": "Ergodic Spectral Efficiency Analysis of Intelligent Omni-Surface Aided Systems Suffering From Imperfect CSI and Hardware Impairments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Transformer-based entropy models have gained prominence in recent years due to their superior ability to capture long-range dependencies in probability distribution estimation compared to convolution-based methods. However, previous transformer-based entropy models suffer from a sluggish coding process due to pixel-wise autoregression or duplicated computation during inference. In this paper, we propose a novel transformer-based entropy model called GroupedMixer, which enjoys both faster coding speed and better compression performance than previous transformer-based methods. Specifically, our approach builds upon group-wise autoregression by first partitioning the latent variables into groups along spatial-channel dimensions, and then entropy coding the groups with the proposed transformer-based entropy model. The global causal self-attention is decomposed into more efficient group-wise interactions, implemented using inner-group and cross-group token-mixers. The inner-group token-mixer incorporates contextual elements within a group while the cross-group token-mixer interacts with previously decoded groups. Alternate arrangement of two token-mixers enables global contextual reference. To further expedite the network inference, we introduce context cache optimization to GroupedMixer, which caches attention activation values in cross-group token-mixers and avoids complex and duplicated computation. Experimental results demonstrate that the proposed GroupedMixer yields the state-of-the-art rate-distortion performance with fast compression speed.",
        "citation_title": "GroupedMixer: An Entropy Model with Group-wise Token-Mixers for Learned Image Compression",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Understanding pedestrian dynamics is crucial for appropriately designing pedestrian spaces. The pedestrian fundamental diagram (FD), which describes the relationship between pedestrian flow and density within a given space, characterizes these dynamics. Pedestrian FDs are significantly influenced by the flow type, such as uni-directional, bi-directional, and crossing flows. However, to the authors' knowledge, generalized pedestrian FDs that are applicable to various flow types have not been proposed. This may be due to the difficulty of using statistical methods to characterize the flow types. The flow types significantly depend on the angles of pedestrian movement; however, these angles cannot be processed by standard statistics due to their periodicity. In this study, we propose a comprehensive model for pedestrian FDs that can describe the pedestrian dynamics for various flow types by applying Directional Statistics. First, we develop a novel statistic describing the pedestrian flow type solely from pedestrian trajectory data using Directional Statistics. Then, we formulate a comprehensive pedestrian FD model that can be applied to various flow types by incorporating the proposed statistics into a traditional pedestrian FD model. The proposed model was validated using actual pedestrian trajectory data. The results confirmed that the model effectively represents the essential nature of pedestrian dynamics, such as the capacity reduction due to conflict of crossing flows and the capacity improvement due to the lane formation in bi-directional flows.",
        "citation_title": "Modeling pedestrian fundamental diagram based on Directional Statistics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Analog codes add redundancy by expanding the dimension using real/complex-valued operations. Frame theory provides a mathematical basis for constructing such codes, with diverse applications in non-orthogonal code-division multiple access (NOMA-CDMA), distributed computation, multiple description source coding, space-time coding (STC), and more. The channel model corresponding to these applications is a combination of noise and erasures. Recent analyses showed a useful connection between spectral random-matrix theory and large equiangular tight frames (ETFs) under random uniform erasures. In this work we generalize this model to a channel where the erasures come in blocks. This particularly fits NOMA-CDMA with multiple transmit antennas for each user and STC with known spatial grouping. We present a method to adjust ETF codes to suit block erasures, and find minimum intra-block-correlation frames which outperform ETFs in this setting.",
        "citation_title": "Frame Codes for the Block-Erasure Channel",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Membership Inference (MI) poses a substantial privacy threat to the training data of Automatic Speech Recognition (ASR) systems, while also offering an opportunity to audit these models with regard to user data. This paper explores the effectiveness of loss-based features in combination with Gaussian and adversarial perturbations to perform MI in ASR models. To the best of our knowledge, this approach has not yet been investigated. We compare our proposed features with commonly used error-based features and find that the proposed features greatly enhance performance for sample-level MI. For speaker-level MI, these features improve results, though by a smaller margin, as error-based features already obtained a high performance for this task. Our findings emphasise the importance of considering different feature sets and levels of access to target models for effective MI in ASR systems, providing valuable insights for auditing such models.",
        "citation_title": "Improving Membership Inference in ASR Model Auditing with Perturbed Loss Features",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we propose a new wireless sensing system equipped with the movable-antenna (MA) array, which can flexibly adjust the positions of antenna elements for improving the sensing performance over conventional antenna arrays with fixed-position antennas (FPAs). First, we show that the angle estimation performance in wireless sensing is fundamentally determined by the array geometry, where the Cramer-Rao bound (CRB) of the mean square error (MSE) for angle of arrival (AoA) estimation is derived as a function of the antennas' positions for both one-dimensional (1D) and two-dimensional (2D) MA arrays. Then, for the case of 1D MA array, we obtain a globally optimal solution for the MAs' positions in closed form to minimize the CRB of AoA estimation MSE. While in the case of 2D MA array, we aim to achieve the minimum of maximum (min-max) CRBs of estimation MSE for the two AoAs with respect to the horizontal and vertical axes, respectively. In particular, for the special case of circular antenna movement region, an optimal solution for the MAs' positions is derived under certain numbers of MAs and circle radii. Thereby, both the lower- and upper-bounds of the min-max CRB are obtained for the antenna movement region with arbitrary shapes. Moreover, we develop an efficient alternating optimization algorithm to obtain a locally optimal solution for MAs' positions by iteratively optimizing one between their horizontal and vertical coordinates with the other being fixed. Numerical results demonstrate that our proposed 1D/2D MA arrays can significantly decrease the CRB of AoA estimation MSE as well as the actual MSE compared to conventional uniform linear arrays (ULAs)/uniform planar arrays (UPAs) with different values of uniform inter-antenna spacing.",
        "citation_title": "Movable Antenna Enhanced Wireless Sensing Via Antenna Position Optimization",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Video-based remote photoplethysmography (rPPG) has emerged as a promising technology for non-contact vital sign monitoring, especially under controlled conditions. However, the accurate measurement of vital signs in real-world scenarios faces several challenges, including artifacts induced by videocodecs, low-light noise, degradation, low dynamic range, occlusions, and hardware and network constraints. In this article, we systematically investigate comprehensive investigate these issues, measuring their detrimental effects on the quality of rPPG measurements. Additionally, we propose practical strategies for mitigating these challenges to improve the dependability and resilience of video-based rPPG systems. We detail methods for effective biosignal recovery in the presence of network limitations and present denoising and inpainting techniques aimed at preserving video frame integrity. Through extensive evaluations and direct comparisons, we demonstrate the effectiveness of the approaches in enhancing rPPG measurements under challenging environments, contributing to the development of more reliable and effective remote vital sign monitoring technologies.",
        "citation_title": "Evaluation of Video-Based rPPG in Challenging Environments: Artifact Mitigation and Network Resilience",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
        "citation_title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "As human-machine interaction continues to evolve, the capacity for environmental perception is becoming increasingly crucial. Integrating the two most common types of sensory data, images, and point clouds, can enhance detection accuracy. However, currently, no model exists that can simultaneously detect an object's position in both point clouds and images and ascertain their corresponding relationship. This information is invaluable for human-machine interactions, offering new possibilities for their enhancement. In light of this, this paper introduces an end-to-end Consistency Object Detection (COD) algorithm framework that requires only a single forward inference to simultaneously obtain an object's position in both point clouds and images and establish their correlation. Furthermore, to assess the accuracy of the object correlation between point clouds and images, this paper proposes a new evaluation metric, Consistency Precision (CP). To verify the effectiveness of the proposed framework, an extensive set of experiments has been conducted on the KITTI and DAIR-V2X datasets. The study also explored how the proposed consistency detection method performs on images when the calibration parameters between images and point clouds are disturbed, compared to existing post-processing methods. The experimental results demonstrate that the proposed method exhibits excellent detection performance and robustness, achieving end-to-end consistency detection. The source code will be made publicly available at this https URL.",
        "citation_title": "Towards Consistent Object Detection via LiDAR-Camera Synergy",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we consider a setting where heterogeneous agents with connectivity are performing inference using unlabeled streaming data. Observed data are only partially informative about the target variable of interest. In order to overcome the uncertainty, agents cooperate with each other by exchanging their local inferences with and through a fusion center. To evaluate how each agent influences the overall decision, we adopt a causal framework in order to distinguish the actual influence of agents from mere correlations within the decision-making process. Various scenarios reflecting different agent participation patterns and fusion center policies are investigated. We derive expressions to quantify the causal impact of each agent on the joint decision, which could be beneficial for anticipating and addressing atypical scenarios, such as adversarial attacks or system malfunctions. We validate our theoretical results with numerical simulations and a real-world application of multi-camera crowd counting.",
        "citation_title": "Causal Influence in Federated Edge Inference",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we consider the design of data-driven predictive controllers for nonlinear systems from input-output data via linear-in-control input Koopman lifted models. Instead of identifying and simulating a Koopman model to predict future outputs, we design a subspace predictive controller in the Koopman space. This allows us to learn the observables minimizing the multi-step output prediction error of the Koopman subspace predictor, preventing the propagation of prediction errors. To avoid losing feasibility of our predictive control scheme due to prediction errors, we compute a terminal cost and terminal set in the Koopman space and we obtain recursive feasibility guarantees through an interpolated initial state. As a third contribution, we introduce a novel regularization cost yielding input-to-state stability guarantees with respect to the prediction error for the resulting closed-loop system. The performance of the developed Koopman data-driven predictive control methodology is illustrated on a nonlinear benchmark example from the literature.",
        "citation_title": "Koopman Data-Driven Predictive Control with Robust Stability and Recursive Feasibility Guarantees",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper explores the use of Hybrid CTC/Attention encoder-decoder models trained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech recognition (ASR) and dialect identification (DID). Results are compared to the current best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN). An optimal InterCTC setting is initially established using a Conformer encoder. This setting is then used to train a model with an E-branchformer encoder and the performance of both architectures are compared. A multi-task fine-tuning approach is adopted for language model (LM) shallow fusion. The experiments yielded an improvement in DID accuracy of 10.8% relative to a baseline ECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task approach emerges as a promising strategy for Irish low-resource ASR and DID.",
        "citation_title": "Low-resource speech recognition and dialect identification of Irish in a multi-task framework",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper introduces Waste Factor (W) and Waste Figure (WF) to assess power efficiency in any multiple-input multiple-output (MIMO) or single-input multiple-output (SIMO) or multiple-input single-output (MISO) cascaded communication system. This paper builds upon the new theory of Waste Factor, which systematically models added wasted power in any cascade for parallel systems such as MISO, SIMO, and MIMO systems, which are prevalent in current wireless networks. Here, we also show the advantage of W compared to conventional metrics for quantifying and analyzing energy efficiency. This work explores the utility of W in assessing energy efficiency in communication channels, within Radio Access Networks (RANs).",
        "citation_title": "Using Waste Factor to Optimize Energy Efficiency in Multiple-Input Single-Output (MISO) and Multiple-Input Multiple-Output (MIMO) Systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Practical Bayesian learning often requires (1) online inference, (2) dynamic models, and (3) ensembling over multiple different models. Recent advances have shown how to use random feature approximations to achieve scalable, online ensembling of Gaussian processes with desirable theoretical properties and fruitful applications. One key to these methods' success is the inclusion of a random walk on the model parameters, which makes models dynamic. We show that these methods can be generalized easily to any basis expansion model and that using alternative basis expansions, such as Hilbert space Gaussian processes, often results in better performance. To simplify the process of choosing a specific basis expansion, our method's generality also allows the ensembling of several entirely different models, for example, a Gaussian process and polynomial regression. Finally, we propose a novel method to ensemble static and dynamic models together.",
        "citation_title": "Dynamic Online Ensembles of Basis Expansions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Controlling contact forces during interactions is critical for locomotion and manipulation tasks. While sim-to-real reinforcement learning (RL) has succeeded in many contact-rich problems, current RL methods achieve forceful interactions implicitly without explicitly regulating forces. We propose a method for training RL policies for direct force control without requiring access to force sensing. We showcase our method on a whole-body control platform of a quadruped robot with an arm. Such force control enables us to perform gravity compensation and impedance control, unlocking compliant whole-body manipulation. The learned whole-body controller with variable compliance makes it intuitive for humans to teleoperate the robot by only commanding the manipulator, and the robot's body adjusts automatically to achieve the desired position and force. Consequently, a human teleoperator can easily demonstrate a wide variety of loco-manipulation tasks. To the best of our knowledge, we provide the first deployment of learned whole-body force control in legged manipulators, paving the way for more versatile and adaptable legged robots.",
        "citation_title": "Learning Force Control for Legged Manipulation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Feedback-evolving games is a framework that models the co-evolution between payoff functions and an environmental state. It serves as a useful tool to analyze many social dilemmas such as natural resource consumption, behaviors in epidemics, and the evolution of biological populations. However, it has primarily focused on the dynamics of a single population of agents. In this paper, we consider the impact of two populations of agents that share a common environmental resource. We focus on a scenario where individuals in one population are governed by an environmentally \"responsible\" incentive policy, and individuals in the other population are environmentally \"irresponsible\". An analysis on the asymptotic stability of the coupled system is provided, and conditions for which the resource collapses are identified. We then derive consumption rates for the irresponsible population that optimally exploit the environmental resource, and analyze how incentives should be allocated to the responsible population that most effectively promote the environment via a sensitivity analysis.",
        "citation_title": "Two competing populations with a common environmental resource",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Quantum parameter estimation theory is an important component of quantum information theory and provides the statistical foundation that underpins important topics such as quantum system identification and quantum waveform estimation. When there is more than one parameter the ultimate precision in the mean square error given by the quantum Cram\u00e9r-Rao bound is not necessarily achievable. For non-full rank quantum states, it was not known when this bound can be saturated (achieved) when only a single copy of the quantum state encoding the unknown parameters is available. This single-copy scenario is important because of its experimental/practical tractability. Recently, necessary and sufficient conditions for saturability of the quantum Cram\u00e9r-Rao bound in the multiparameter single-copy scenario have been established in terms of i) the commutativity of a set of projected symmetric logarithmic derivatives and ii) the existence of a unitary solution to a system of coupled nonlinear partial differential equations. New sufficient conditions were also obtained that only depend on properties of the symmetric logarithmic derivatives. In this paper, key structural properties of optimal measurements that saturate the quantum Cram\u00e9r-Rao bound are illuminated. These properties are exploited to i) show that the sufficient conditions are in fact necessary and sufficient for an optimal measurement to be projective, ii) give an alternative proof of previously established necessary conditions, and iii) describe general POVMs, not necessarily projective, that saturate the multiparameter QCRB. Examples are given where a unitary solution to the system of nonlinear partial differential equations can be explicitly calculated when the required conditions are fulfilled.",
        "citation_title": "Saturation of the Multiparameter Quantum Cram\u00e9r-Rao Bound at the Single-Copy Level with Projective Measurements",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Adaptive Cruise Control ACC can change the speed of the ego vehicle to maintain a safe distance from the following vehicle automatically. The primary purpose of this research is to use cutting-edge computing approaches to locate and track vehicles in real time under various conditions to achieve a safe ACC. The paper examines the extension of ACC employing depth cameras and radar sensors within Autonomous Vehicles AVs to respond in real time by changing weather conditions using the Car Learning to Act CARLA simulation platform at noon. The ego vehicle controller's decision to accelerate or decelerate depends on the speed of the leading ahead vehicle and the safe distance from that vehicle. Simulation results show that a Proportional Integral Derivative PID control of autonomous vehicles using a depth camera and radar sensors reduces the speed of the leading vehicle and the ego vehicle when it rains. In addition, longer travel time was observed for both vehicles in rainy conditions than in dry conditions. Also, PID control prevents the leading vehicle from rear collisions",
        "citation_title": "Evaluation and Optimization of Adaptive Cruise Control in Autonomous Vehicles using the CARLA Simulator: A Study on Performance under Wet and Dry Weather Conditions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Rate split multiple access (RSMA) has been proven as an effective communication scheme for 5G and beyond, especially in vehicular scenarios. However, RSMA requires complicated iterative algorithms for proper resource allocation, which cannot fulfill the stringent latency requirement in resource constrained vehicles. Although data driven approaches can alleviate this issue, they suffer from poor generalizability and scarce training data. In this paper, we propose a fractional programming (FP) based deep unfolding (DU) approach to address resource allocation problem for a weighted sum rate optimization in RSMA. By carefully designing the penalty function, we couple the variable update with projected gradient descent algorithm (PGD). Following the structure of PGD, we embed few learnable parameters in each layer of the DU network. Through extensive simulation, we have shown that the proposed model-based neural networks has similar performance as optimal results given by traditional algorithm but with much lower computational complexity, less training data, and higher resilience to test set data and out-of-distribution (OOD) data.",
        "citation_title": "Model-based Deep Learning for Rate Split Multiple Access in Vehicular Communications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The transformer structure employed in large language models (LLMs), as a specialized category of deep neural networks (DNNs) featuring attention mechanisms, stands out for their ability to identify and highlight the most relevant aspects of input data. Such a capability is particularly beneficial in addressing a variety of communication challenges, notably in the realm of semantic communication where proper encoding of the relevant data is critical especially in systems with limited bandwidth. In this work, we employ vision transformers specifically for the purpose of compression and compact representation of the input image, with the goal of preserving semantic information throughout the transmission process. Through the use of the attention mechanism inherent in transformers, we create an attention mask. This mask effectively prioritizes critical segments of images for transmission, ensuring that the reconstruction phase focuses on key objects highlighted by the mask. Our methodology significantly improves the quality of semantic communication and optimizes bandwidth usage by encoding different parts of the data in accordance with their semantic information content, thus enhancing overall efficiency. We evaluate the effectiveness of our proposed framework using the TinyImageNet dataset, focusing on both reconstruction quality and accuracy. Our evaluation results demonstrate that our framework successfully preserves semantic information, even when only a fraction of the encoded data is transmitted, according to the intended compression rates.",
        "citation_title": "Transformer-Aided Semantic Communications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper studies the impact of quantization in integrate-and-fire time encoding machine (IF-TEM) sampler used for bandlimited (BL) and finite-rate-of-innovation (FRI) signals. An upper bound is derived for the mean squared error (MSE) of IF-TEM sampler and is compared against that of classical analog-to-digital converters (ADCs) with uniform sampling and quantization. The interplay between a signal's energy, bandwidth, and peak amplitude is used to identify how the MSE of IF-TEM sampler with quantization is influenced by these parameters. More precisely, the quantization step size of the IF-TEM sampler can be reduced when the maximum frequency of a bandlimited signal or the number of pulses of an FRI signal is increased. Leveraging this insight, specific parameter settings are identified for which the quantized IF-TEM sampler achieves an MSE bound that is roughly 8 dB lower than that of a classical ADC with the same number of bits. Experimental results validate the theoretical conclusions.",
        "citation_title": "Time Encoding Quantization of Bandlimited and Finite-Rate-of-Innovation Signals",
        "date_delivered": "[Submitted on 5 Oct 2021 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We study the 2-D super-resolution multi-reference alignment (SR-MRA) problem: estimating an image from its down-sampled, circularly-translated, and noisy copies. The SR-MRA problem serves as a mathematical abstraction of the structure determination problem for biological molecules. Since the SR-MRA problem is ill-posed without prior knowledge, accurate image estimation relies on designing priors that well-describe the statistics of the images of interest. In this work, we build on recent advances in image processing, and harness the power of denoisers as priors of images. In particular, we suggest to use denoisers as projections, and design two computational frameworks to estimate the image: projected expectation-maximization and projected method of moments. We provide an efficient GPU implementation, and demonstrate the effectiveness of these algorithms by extensive numerical experiments on a wide range of parameters and images.",
        "citation_title": "Denoiser-based projections for 2-D super-resolution multi-reference alignment",
        "date_delivered": "[Submitted on 10 Apr 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Designing distributed algorithms for multi-agent problems is vital for many emerging application domains, and game-theoretic approaches are emerging as a useful paradigm to design such algorithms. However, much of the emphasis of the game-theoretic approach is on the study of equilibrium behavior, whereas transient behavior is often less explored. Therefore, in this paper we study the transient efficiency guarantees of best response processes in the context of resource-allocation games, which are used to model a variety of engineering applications. Specifically, the main focus of the paper is on designing utility functions of agents to induce optimal short-term system-level behavior under a best-response process. Interestingly, the resulting transient performance guarantees are relatively close to the optimal asymptotic performance guarantees. Furthermore, we characterize a trade-off that results when optimizing for both asymptotic and transient efficiency through various utility designs.",
        "citation_title": "Optimal Utility Design of Greedy Algorithms in Resource Allocation Games",
        "date_delivered": "[Submitted on 21 Apr 2022 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Multi-rotor UAVs suffer from a restricted range and flight duration due to limited battery capacity. Autonomous landing on a 2D moving platform offers the possibility to replenish batteries and offload data, thus increasing the utility of the vehicle. Classical approaches rely on accurate, complex and difficult-to-derive models of the vehicle and the environment. Reinforcement learning (RL) provides an attractive alternative due to its ability to learn a suitable control policy exclusively from data during a training procedure. However, current methods require several hours to train, have limited success rates and depend on hyperparameters that need to be tuned by trial-and-error. We address all these issues in this work. First, we decompose the landing procedure into a sequence of simpler, but similar learning tasks. This is enabled by applying two instances of the same RL based controller trained for 1D motion for controlling the multi-rotor's movement in both the longitudinal and the lateral directions. Second, we introduce a powerful state space discretization technique that is based on i) kinematic modeling of the moving platform to derive information about the state space topology and ii) structuring the training as a sequential curriculum using transfer learning. Third, we leverage the kinematics model of the moving platform to also derive interpretable hyperparameters for the training process that ensure sufficient maneuverability of the multi-rotor vehicle. The training is performed using the tabular RL method Double Q-Learning. Through extensive simulations we show that the presented method significantly increases the rate of successful landings, while requiring less training time compared to other deep RL approaches. Finally, we deploy and demonstrate our algorithm on real hardware. For all evaluation scenarios we provide statistics on the agent's performance.",
        "citation_title": "Reinforcement Learning based Autonomous Multi-Rotor Landing on Moving Platforms",
        "date_delivered": "[Submitted on 25 Feb 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Advances in the automotive industry and the ever-increasing demand for Connected and Autonomous Vehicles (CAVs) are pushing for a new epoch of networked wireless systems. Vehicular communications, or Vehicle-to-Everything (V2X), are expected to be among the main actors of the future beyond 5G and 6G networks. However, the challenging application requirements, the fast variability of the vehicular environment, and the harsh propagation conditions of high frequencies call for sophisticated control mechanisms to ensure the success of such a disruptive technology. While traditional Radio Access Networks (RAN) lack the flexibility to support the required control primitives, the emergent concept of Open RAN (O-RAN) appears as an ideal enabler of V2X communication orchestration. However, effectively integrating the two ecosystems is still an open issue. This paper discusses possible integration strategies, highlighting the challenges and opportunities of leveraging O-RAN to enable real-time V2X control. Additionally, we enrich our discussion with potential research directions stemming from the current state-of-the-art, and we provide preliminary simulation results that validate the effectiveness of the proposed integration.",
        "citation_title": "Open RAN-empowered V2X Architecture: Challenges, Opportunities, and Research Directions",
        "date_delivered": "[Submitted on 13 Mar 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We consider the problem of finite-time identification of linear dynamical systems from $T$ samples of a single trajectory. Recent results have predominantly focused on the setup where no structural assumption is made on the system matrix $A^* \\in \\mathbb{R}^{n \\times n}$, and have consequently analyzed the ordinary least squares (OLS) estimator in detail. We assume prior structural information on $A^*$ is available, which can be captured in the form of a convex set $\\mathcal{K}$ containing $A^*$. For the solution of the ensuing constrained least squares estimator, we derive non-asymptotic error bounds in the Frobenius norm that depend on the local size of $\\mathcal{K}$ at $A^*$. To illustrate the usefulness of these results, we instantiate them for four examples, namely when (i) $A^*$ is sparse and $\\mathcal{K}$ is a suitably scaled $\\ell_1$ ball; (ii) $\\mathcal{K}$ is a subspace; (iii) $\\mathcal{K}$ consists of matrices each of which is formed by sampling a bivariate convex function on a uniform $n \\times n$ grid (convex regression); (iv) $\\mathcal{K}$ consists of matrices each row of which is formed by uniform sampling (with step size $1/T$) of a univariate Lipschitz function. In all these situations, we show that $A^*$ can be reliably estimated for values of $T$ much smaller than what is needed for the unconstrained setting.",
        "citation_title": "Learning linear dynamical systems under convex constraints",
        "date_delivered": "[Submitted on 27 Mar 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Diffusion models have recently demonstrated an impressive ability to address inverse problems in an unsupervised manner. While existing methods primarily focus on modifying the posterior sampling process, the potential of the forward process remains largely unexplored. In this work, we propose Shortcut Sampling for Diffusion(SSD), a novel approach for solving inverse problems in a zero-shot manner. Instead of initiating from random noise, the core concept of SSD is to find a specific transitional state that bridges the measurement image y and the restored image x. By utilizing the shortcut path of \"input - transitional state - output\", SSD can achieve precise restoration with fewer steps. To derive the transitional state during the forward process, we introduce Distortion Adaptive Inversion. Moreover, we apply back projection as additional consistency constraints during the generation process. Experimentally, we demonstrate SSD's effectiveness on multiple representative IR tasks. Our method achieves competitive results with only 30 NFEs compared to state-of-the-art zero-shot methods(100 NFEs) and outperforms them with 100 NFEs in certain tasks. Code is available at this https URL",
        "citation_title": "Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling",
        "date_delivered": "[Submitted on 26 May 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The purpose of this paper is to study the dynamics of a coherent feedback network where two two-level atoms are coupled with a semi-infinite waveguide. In this set-up, the two-level atoms can work as the photon source, and the photons can be emitted into the waveguide via the nonchiral or chiral couplings between the atom and the waveguide, according to whether the coupling strengths between the atoms and different directional propagating modes in the waveguide are identical or not. For the photon emitted by one of the two atoms, it can be reflected by the terminal mirror, or interact with the other atom, and then the photon can re-interact with the former atom. When the two atoms are both initially excited, finally there can be two-photon, one-photon or zero-photon states in the waveguide via the spontaneous emission and feedback interactions, and this is influenced by the locations of the atoms and the chirality of the coupling between the atom and the waveguide. Similarly, if only one of the two atoms is initially excited, there can be zero or one photon in the waveguide. Thus we can control the number of the photons in the waveguide and the atomic states by tuning the feedback loop length and the chiral couplings between the atom and waveguide. The photonic state in the waveguide is analyzed in the frequency domain and the spatial domain, and the transient process of photon emissions can be better understood based on the comprehensive analysis in these two domains.",
        "citation_title": "Quantum feedback control of a two-atom network closed by a semi-infinite waveguide",
        "date_delivered": "[Submitted on 10 Jun 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Widefield microscopy is widely used for non-invasive imaging of biological structures at subcellular resolution. When applied to complex specimen, its image quality is degraded by sample-induced optical aberration. Adaptive optics can correct wavefront distortion and restore diffraction-limited resolution but require wavefront sensing and corrective devices, increasing system complexity and cost. Here, we describe a self-supervised machine learning algorithm, CoCoA, that performs joint wavefront estimation and three-dimensional structural information extraction from a single input 3D image stack without the need for external training dataset. We implemented CoCoA for widefield imaging of mouse brain tissues and validated its performance with direct-wavefront-sensing-based adaptive optics. Importantly, we systematically explored and quantitatively characterized the limiting factors of CoCoA's performance. Using CoCoA, we demonstrated the first in vivo widefield mouse brain imaging using machine-learning-based adaptive optics. Incorporating coordinate-based neural representations and a forward physics model, the self-supervised scheme of CoCoA should be applicable to microscopy modalities in general.",
        "citation_title": "Coordinate-based neural representations for computational adaptive optics in widefield microscopy",
        "date_delivered": "[Submitted on 7 Jul 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Future wireless networks, in particular, 5G and beyond, are anticipated to deploy dense Low Earth Orbit (LEO) satellites to provide global coverage and broadband connectivity. However, the limited frequency band and the coexistence of multiple constellations bring new challenges for interference management. In this paper, we propose a robust multilayer interference management scheme for spectrum sharing in heterogeneous satellite networks with statistical channel state information (CSI) at the transmitter (CSIT) and receivers (CSIR). In the proposed scheme, Rate-Splitting Multiple Access (RSMA), as a general and powerful framework for interference management and multiple access strategies, is implemented distributedly at GEO and LEO satellites, coined Distributed-RSMA (D-RSMA). By doing so, D-RSMA aims to mitigate the interference and boost the user fairness of the overall multilayer satellite system. Specifically, we study the problem of jointly optimizing the GEO/LEO precoders and message splits to maximize the minimum rate among User Terminals (UTs) subject to a transmit power constraint at all satellites. A robust algorithm is proposed to solve the original non-convex optimization problem. Numerical results demonstrate the effectiveness and robustness towards network load and CSI uncertainty of our proposed D-RSMA scheme. Benefiting from the interference management capability, D-RSMA provides significant max-min fairness performance gains compared to several benchmark schemes.",
        "citation_title": "Distributed Rate-Splitting Multiple Access for Multilayer Satellite Communications",
        "date_delivered": "[Submitted on 14 Jul 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Many deblurring and blur kernel estimation methods use a maximum a posteriori (MAP) approach or deep learning-based classification techniques to sharpen an image and/or predict the blur kernel. We propose a regression approach using convolutional neural networks (CNNs) to predict parameters of linear motion blur kernels, the length and orientation of the blur. We analyze the relationship between length and angle of linear motion blur that can be represented as digital filter kernels. A large dataset of blurred images is generated using a suite of blur kernels and used to train a regression CNN for prediction of length and angle of the motion blur. The coefficients of determination for estimation of length and angle are found to be greater than or equal to 0.89, even under the presence of significant additive Gaussian noise, up to a variance of 10\\% (SNR of 10 dB). Using our estimated kernel in a non-blind image deblurring method, the sum of squared differences error ratio demonstrates higher cumulative histogram values than comparison methods, with most test images yielding an error ratio of less than or equal to 1.25.",
        "citation_title": "Estimation of motion blur kernel parameters using regression convolutional neural networks",
        "date_delivered": "[Submitted on 2 Aug 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Electrical Impedance Tomography (EIT) is a widely employed imaging technique in industrial inspection, geophysical prospecting, and medical imaging. However, the inherent nonlinearity and ill-posedness of EIT image reconstruction present challenges for classical regularization techniques, such as the critical selection of regularization terms and the lack of prior knowledge. Deep generative models (DGMs) have been shown to play a crucial role in learning implicit regularizers and prior knowledge. This study aims to investigate the potential of three DGMs-variational autoencoder networks, normalizing flow, and score-based diffusion model-to learn implicit regularizers in learning-based EIT imaging. We first introduce background information on EIT imaging and its inverse problem formulation. Next, we propose three algorithms for performing EIT inverse problems based on corresponding DGMs. Finally, we present numerical and visual experiments, which reveal that (1) no single method consistently outperforms the others across all settings, and (2) when reconstructing an object with 2 anomalies using a well-trained model based on a training dataset containing 4 anomalies, the conditional normalizing flow model (CNF) exhibits the best generalization in low-level noise, while the conditional score-based diffusion model (CSD*) demonstrates the best generalization in high-level noise settings. We hope our preliminary efforts will encourage other researchers to assess their DGMs in EIT and other nonlinear inverse problems.",
        "citation_title": "A Comparative Study of Variational Autoencoders, Normalizing Flows, and Score-based Diffusion Models for Electrical Impedance Tomography",
        "date_delivered": "[Submitted on 24 Oct 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We study the behavior of deterministic methods for solving inverse problems in imaging. These methods are commonly designed to achieve two goals: (1) attaining high perceptual quality, and (2) generating reconstructions that are consistent with the measurements. We provide a rigorous proof that the better a predictor satisfies these two requirements, the larger its Lipschitz constant must be, regardless of the nature of the degradation involved. In particular, to approach perfect perceptual quality and perfect consistency, the Lipschitz constant of the model must grow to infinity. This implies that such methods are necessarily more susceptible to adversarial attacks. We demonstrate our theory on single image super-resolution algorithms, addressing both noisy and noiseless settings. We also show how this undesired behavior can be leveraged to explore the posterior distribution, thereby allowing the deterministic model to imitate stochastic methods.",
        "citation_title": "The Perception-Robustness Tradeoff in Deterministic Image Restoration",
        "date_delivered": "[Submitted on 14 Nov 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In order to provide robust, reliable, and accurate position and velocity control of motor drives, friction compensation has emerged as a key difficulty. Non-characterised friction could give rise to large position errors and vibrations which could be intensified by stick-slip motion and limit cycles. This paper presents an application of two data-driven nonlinear model identification techniques to discover the governing equations of motor dynamics that also characterise friction. Namely, the extraction of low-power data from time-delayed coordinates of motor velocity and sparse regression on nonlinear terms was applied to data acquired from a Brushless DC (BLDC) motor, to identify the underlying dynamics. The latter can be considered an extension of the conventional linear motor model commonly used in many model-based controllers. The identified nonlinear model was then contrasted with a nonlinear model that included the LuGre friction model and a linear model without friction. A nonlinear grey box model estimation method was used to calculate the optimum friction parameters for the LuGre model. The resulting nonlinear motor model with friction characteristics was then validated using a feedback friction compensation algorithm. The novel model showed more than 90% accuracy in predicting the motor states in all considered input excitation signals. In addition, the model-based friction compensation scheme showed a relative increase in performance when compared with a system without friction compensation.",
        "citation_title": "Motor State Prediction and Friction Compensation for Brushless DC Motor Drives Using Data-Driven Techniques",
        "date_delivered": "[Submitted on 28 Nov 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "Cybersecurity in building energy management is crucial for protecting infrastructure, ensuring data integrity, and preventing unauthorized access or manipulation. This paper investigates the energy efficiency and cybersecurity of building energy management systems (BMS) against false data injection (FDI) attacks using proportional-integral (PI) and model predictive control (MPC) methods. Focusing on a commercial building model with five rooms, vulnerability of PI-based BMS and nonlinear MPC-based BMS against FDIs on sensors and actuators is studied. The study aims to assess the effectiveness of these control strategies in maintaining system performance and lifespan, highlighting the potential of MPC in enhancing system resilience against cyber threats. Our case studies demonstrate that even a short term FDIA can cause a 12% reduction in lifetime of a heat-pump under an MPC controller, and cause a near thirty-fold overuse of flow valves under a PI controller.",
        "citation_title": "Vulnerability of Building Energy Management against Targeted False Data Injection Attacks:Model Predictive Control vs. Proportional Integral",
        "date_delivered": "[Submitted on 6 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Future Naval Microgrids (MGs) will include hybrid energy storage systems (ESS), including battery and supercapacitors to respond to emerging constant power loads (CPLs) and fluctuating pulsed power loads (PPLs). Voltage regulation of naval microgrids and power sharing among these resources become critical for success of a mission. This paper presents a novel control strategy using nonlinear model predictive controller embedded with a complex droop control architecture for voltage restoration and power sharing in medium voltage DC (MVDC) Naval MGs. The complex droop control ensures allocating supercapacitors (SCs) for high-frequency loads (i.e., PPLs), while battery energy storage system (BESS) and auxiliary generators share the steady-state load (i.e., CPL). Compared to state-of-the-art control of the naval ship MGs that relies on linear models, the proposed method incorporates the nonlinear behavior of the MGs in the closed-loop control framework via nonlinear model predictive control (NMPC). A reduced order representation of the MVDC dynamic is employed as the prediction model, augmented with a multi-objective, constraints-based, optimal control formulation. The results demonstrate the effectiveness of the proposed control framework for voltage restoration and power sharing of resources in naval MGs.",
        "citation_title": "Voltage Restoration in MVDC Shipboard Microgrids with Economic Nonlinear Model Predictive Control",
        "date_delivered": "[Submitted on 6 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "This paper presents a novel control strategy for medium voltage DC (MVDC) naval shipboard microgrids (MGs), employing a nonlinear model predictive controller (NMPC) enhanced with stabilizing features and an intricate droop control architecture. This combination quickly regulates the output voltage and adeptly allocates supercapacitors for pulsed power loads (PPLs), while the battery energy storage system (BESS) and auxiliary generators handle the steady state loads. A key feature of this study is the formulation of terminal cost and constraints, providing recursive feasibility and closed-loop stability in the Lyapunov sense, that offers a more robust and effective approach to naval power and energy management. By comparing the proposed Lyapunov-based NMPC with conventional PI controller under fluctuating PPLs, the control robustness is validated.",
        "citation_title": "Nonlinear Model Predictive Control for Navy Microgrids with Stabilizing Terminal Ingredients",
        "date_delivered": "[Submitted on 6 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Resilience is emerging as an evolving notion, reflecting a system's ability to endure and adapt to sudden and catastrophic changes and disruptions. This paper spotlights the significance of the quantitative resilience indices of medium-voltage DC (MVDC) distribution technology in marine vessels, notably naval ships. Given the intricate electrical requirements of modern naval ships, the need for a robust power supply underlines the imperative of resilient DC microgrids. Addressing this, our study introduces a novel quantitative metric for operational resilience of DC microgrids based on the measured voltage of main DC bus. This metric not only fuses real-time tracking, compatibility, and computational efficiency, but also adeptly monitors multiple event phases based on time-domain analysis of dc bus voltage dynamics. The intricacies of the dc bus voltage, including overshoots and undershoots, are meticulously accounted for in the algorithm design. With respect to existing research that typically focuses on offline resilience assessments, the proposed index provides valuable real-time information for microgrid operators and identifies whether microgrid resilience is deteriorating over time.",
        "citation_title": "Time-Domain Operational Metrics for Real-time Resilience Assessment in DC Microgrids",
        "date_delivered": "[Submitted on 6 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Medium-voltage direct-current (MVDC) ship-board microgrids (SMGs) are the state-of-the-art architecture for onboard power distribution in navy. These systems are considered to be highly dynamic due to high penetration of power electronic converters and volatile load patterns such as pulsed-power load (PPL) and propulsion motors demand variation. Obtaining the dynamic model of an MVDC SMG is a challenging task due to the confidentiality of system components models and uncertainty in the dynamic models through time. In this paper, a dynamic identification framework based on a temporal convolutional neural network (TCN) is developed to learn the system dynamics from measurement data. Different kinds of testing scenarios are implemented, and the testing results show that this approach achieves an exceptional performance and high generalization ability, thus holding substantial promise for development of advanced data-driven control strategies and stability prediction of the system.",
        "citation_title": "Learning the Dynamics of Future Marine Microgrids Using Temporal Convolutional Neural Network",
        "date_delivered": "[Submitted on 6 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "This paper explores the novel concept of damping controller coordination, which aims to minimize the Total Action metric by identifying an optimal switching combination (on/off) of these controllers. The metric is rooted in power system physics, capturing oscillation energy associated with all synchronous generators in the grid. While coordination has shown promising results, it has relied on computing linear sensitivities based on the grid model. This paper proposes a data-informed framework to accurately estimate total action and subsequently determine an optimal switching combination. The estimation is provided by a multivariate function approximator that captures the nonlinear relationship between system-wide area measurements, the status of damping controllers, and the conditions of the disturbance. By enabling real-time coordination, electromechanical oscillations are reduced, enhancing power system stability. The concept is tested in the Western North America Power System (wNAPS) and compared with the model-based approach for coordination. The proposed coordination outperforms the model-based approach, demonstrating effective adaptability and performance in handling multi-mode events. Additionally, the results show significant reductions in low-frequency electromechanical oscillations even under various operating conditions, fault locations, and time delay considerations.",
        "citation_title": "Coordination of Damping Controllers: A Novel Data-Informed Approach for Adaptability",
        "date_delivered": "[Submitted on 12 Dec 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this study, we focus on building up a model that aims to Segment Anything in medical scenarios, driven by Text prompts, termed as SAT. Our main contributions are three folds: (i) for dataset construction, we combine multiple knowledge sources to construct the first multi-modal knowledge tree on human anatomy, including 6502 anatomical terminologies; Then we build up the largest and most comprehensive segmentation dataset for training, by collecting over 22K 3D medical image scans from 72 segmentation datasets with careful standardization on both image scans and label space; (ii) for architecture design, we formulate a universal segmentation model, that can be prompted by inputting medical terminologies in text form. We present knowledge-enhanced representation learning on the combination of a large number of datasets; (iii) for model evaluation, we train a SAT-Pro with only 447M parameters, to segment 72 different segmentation datasets with text prompt, resulting in 497 classes. We have thoroughly evaluated the model from three aspects: averaged by body regions, averaged by classes, and average by datasets, demonstrating comparable performance to 72 specialist nnU-Nets, i.e., we train nnU-Net models on each dataset/subset, resulting in 72 nnU-Nets with around 2.2B parameters for the 72 datasets. We will release all the codes, and models in this work.",
        "citation_title": "One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts",
        "date_delivered": "[Submitted on 28 Dec 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Three-dimensional (3D) freehand ultrasound (US) is a widely used imaging modality that allows non-invasive imaging of medical anatomy without radiation exposure. The surface reconstruction of US volume is vital to acquire the accurate anatomical structures needed for modeling, registration, and visualization. However, traditional methods cannot produce a high-quality surface due to image noise. Despite improvements in smoothness, continuity, and resolution from deep learning approaches, research on surface reconstruction in freehand 3D US is still limited. This study introduces FUNSR, a self-supervised neural implicit surface reconstruction method to learn signed distance functions (SDFs) from US volumes. In particular, FUNSR iteratively learns the SDFs by moving the 3D queries sampled around the volumetric point clouds to approximate the surface, guided by two novel geometric constraints: sign consistency constraint and on-surface constraint with adversarial learning. Our approach has been thoroughly evaluated across four datasets to demonstrate its adaptability to various anatomical structures, including a hip phantom dataset, two vascular datasets and one publicly available prostate dataset. We also show that smooth and continuous representations greatly enhance the visual appearance of US data. Furthermore, we highlight the robustness of our method to noise distribution and its potential to improve segmentation performance.",
        "citation_title": "Neural Implicit Surface Reconstruction of Freehand 3D Ultrasound Volume with Geometric Constraints",
        "date_delivered": "[Submitted on 11 Jan 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "This paper proposes a distributed controller synthesis framework for safe navigation of multi-agent systems. We leverage control barrier functions to formulate collision avoidance with obstacles and teammates as constraints on the control input for a state-dependent network optimization problem that encodes team formation and the navigation task. Our algorithmic solution is valid for general nonlinear control dynamics and optimization problems. The resulting controller is distributed, satisfies the safety constraints at all times, and is asymptotically optimal. We illustrate its performance in a team of differential-drive robots in a variety of complex environments, both in simulation and in hardware.",
        "citation_title": "Distributed Safe Navigation of Multi-Agent Systems using Control Barrier Function-Based Optimal Controllers",
        "date_delivered": "[Submitted on 9 Feb 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Epilepsy is a prevalent neurological disorder that affects approximately 1% of the global population. Around 30-40% of patients do not respond to pharmacological treatment, leading to a significant negative impact on their quality of life. Closed-loop deep brain stimulation (DBS) is a promising treatment for individuals who do not respond to medical therapy. To achieve effective seizure control, algorithms play an important role in identifying relevant electrographic biomarkers from local field potentials (LFPs) to determine the optimal stimulation timing. In this regard, the detection and classification of events from ongoing brain activity, while achieving low power through computationally unexpensive implementations, represents a major challenge in the field. To address this challenge, we here present two lightweight algorithms, the ZdensityRODE and the AMPDE, for identifying relevant events from LFPs by utilizing semantic segmentation, which involves extracting different levels of information from the LFP and relevant events from it. The algorithms performance was validated against epileptiform activity induced by 4-minopyridine in mouse hippocampus-cortex (CTX) slices and recorded via microelectrode array, as a case study. The ZdensityRODE algorithm showcased a precision and recall of 93% for ictal event detection and 42% precision for interictal event detection, while the AMPDE algorithm attained a precision of 96% and recall of 90% for ictal event detection and 54% precision for interictal event detection. While initially trained specifically for detection of ictal activity, these algorithms can be fine-tuned for improved interictal detection, aiming at seizure prediction. Our results suggest that these algorithms can effectively capture epileptiform activity; their light weight opens new possibilities for real-time seizure detection and seizure prediction and control.",
        "citation_title": "Time series segmentation for recognition of epileptiform patterns recorded via Microelectrode Arrays in vitro",
        "date_delivered": "[Submitted on 12 Feb 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Jamming devices pose a significant threat by disrupting signals from the global navigation satellite system (GNSS), compromising the robustness of accurate positioning. Detecting anomalies in frequency snapshots is crucial to counteract these interferences effectively. The ability to adapt to diverse, unseen interference characteristics is essential for ensuring the reliability of GNSS in real-world applications. In this paper, we propose a few-shot learning (FSL) approach to adapt to new interference classes. Our method employs quadruplet selection for the model to learn representations using various positive and negative interference classes. Furthermore, our quadruplet variant selects pairs based on the aleatoric and epistemic uncertainty to differentiate between similar classes. We recorded a dataset at a motorway with eight interference classes on which our FSL method with quadruplet loss outperforms other FSL techniques in jammer classification accuracy with 97.66%. Dataset available at: this https URL",
        "citation_title": "Few-Shot Learning with Uncertainty-based Quadruplet Selection for Interference Classification in GNSS Data",
        "date_delivered": "[Submitted on 9 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "This paper proposes a fitting procedure that aims to identify the statistical properties of the parameters that describe the most widely known multipath propagation model (MPM) used in power line communication (PLC). Firstly, the MPM parameters are computed by fitting the theoretical model to a large database of single-input-single-output (SISO) experimental measurements, carried out in typical home premises. Secondly, the determined parameters are substituted back into the MPM formulation with the aim to prove their faithfulness, thus validating the proposed computation procedure. Then, the MPM parameters properties have been evaluated. In particular, the statistical behavior is established identifying the best fitting distribution by comparing the most common distributions through the use of the likelihood function. Moreover, the relationship among the different paths is highlighted in terms of statistical correlation. The identified statistical behavior for the MPM parameters confirms the assumptions of the previous works that, however, were mostly established in an heuristic way.",
        "citation_title": "On the Statistical Analysis of the Multipath Propagation Model Parameters for Power Line Communications",
        "date_delivered": "[Submitted on 26 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "This paper delves into a rendezvous scenario involving a chaser and a target spacecraft, focusing on the application of Model Predictive Control (MPC) to design a controller capable of guiding the chaser toward the target. The operational principle of spacecraft thrusters, requiring a minimum activation time that leads to the existence of a control deadband, introduces mixed-integer constraints into the optimization, posing a considerable computational challenge due to the exponential complexity on the number of integer constraints. We address this complexity by presenting two solver algorithms that efficiently approximate the optimal solution in significantly less time than standard solvers, making them well-suited for real-time applications.",
        "citation_title": "Convex MPC and Thrust Allocation with Deadband for Spacecraft Rendezvous",
        "date_delivered": "[Submitted on 5 Apr 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The images produced by diffusion models can attain excellent perceptual quality. However, it is challenging for diffusion models to guarantee distortion, hence the integration of diffusion models and image compression models still needs more comprehensive explorations. This paper presents a diffusion-based image compression method that employs a privileged end-to-end decoder model as correction, which achieves better perceptual quality while guaranteeing the distortion to an extent. We build a diffusion model and design a novel paradigm that combines the diffusion model and an end-to-end decoder, and the latter is responsible for transmitting the privileged information extracted at the encoder side. Specifically, we theoretically analyze the reconstruction process of the diffusion models at the encoder side with the original images being visible. Based on the analysis, we introduce an end-to-end convolutional decoder to provide a better approximation of the score function $\\nabla_{\\mathbf{x}_t}\\log p(\\mathbf{x}_t)$ at the encoder side and effectively transmit the combination. Experiments demonstrate the superiority of our method in both distortion and perception compared with previous perceptual compression methods.",
        "citation_title": "Correcting Diffusion-Based Perceptual Image Compression with Privileged End-to-End Decoder",
        "date_delivered": "[Submitted on 7 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Distributed massive multiple-input multiple output (mMIMO) system for low earth orbit (LEO) satellite networks is introduced as a promising technique to provide broadband connectivity. Nevertheless, several challenges persist in implementing distributed mMIMO systems for LEO satellite networks. These challenges include providing scalable massive access implementation as the system complexity increases with network size. Another challenging issue is the asynchronous arrival of signals at the user terminals due to the different propagation delays among distributed antennas in space, which destroys the coherent transmission, and consequently degrades the system performance. In this paper, we propose a scalable distributed mMIMO system for LEO satellite networks based on dynamic user-centric clustering. Aiming to obtain scalable implementation, new algorithms for initial cooperative access, cluster selection, and cluster handover are provided. In addition, phase shift-aware precoding is implemented to compensate for the propagation delay phase shifts. The performance of the proposed user-centric distributed mMIMO is compared with two baseline configurations: the non-cooperative transmission systems, where each user connects to only a single satellite, and the full-cooperative distributed mMIMO systems, where all satellites contribute serving each user. The numerical results show the potential of the proposed distributed mMIMO system to enhance system spectral efficiency when compared to noncooperative transmission systems. Additionally, it demonstrates the ability to minimize the serving cluster size for each user, thereby reducing the overall system complexity in comparison to the full-cooperative distributed mMIMO systems.",
        "citation_title": "Distributed Massive MIMO System with Dynamic Clustering in LEO Satellite Networks",
        "date_delivered": "[Submitted on 9 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "A well-known retinal disease that sends blurry visions to the affected patients is Macular Degeneration. This research is based on classifying the healthy and macular degeneration fundus by localizing the affected region of the fundus. A CNN architecture and CNN with ResNet architecture (ResNet50, ResNet50v2, ResNet101, ResNet101v2, ResNet152, ResNet152v2) as the backbone are used to classify the two types of fundus. The data are split into three categories including (a) Training set is 90% and Testing set is 10% (b) Training set is 80% and Testing set is 20%, (c) Training set is 50% and Testing set is 50%. After the training, the best model has been selected from the evaluation metrics. Among the models, CNN with a backbone of ResNet50 performs best which gives the training accuracy of 98.7% for 90% train and 10% test data split. With this model, we have performed the Grad-CAM visualization to get the region of the affected area of the fundus.",
        "citation_title": "Perception and Localization of Macular Degeneration Applying Convolutional Neural Network, ResNet and Grad-CAM",
        "date_delivered": "[Submitted on 24 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The variability of renewable energy generation and the unpredictability of electricity demand create a need for real-time economic dispatch (ED) of assets in microgrids. However, solving numerical optimization problems in real-time can be incredibly challenging. This study proposes using a convolutional neural network (CNN) based on deep learning to address these challenges. Compared to traditional methods, CNN is more efficient, delivers more dependable results, and has a shorter response time when dealing with uncertainties. While CNN has shown promising results, it does not extract explainable knowledge from the data. To address this limitation, a physics-inspired CNN model is developed by incorporating constraints of the ED problem into the CNN training to ensure that the model follows physical laws while fitting the data. The proposed method can significantly accelerate real-time economic dispatch of microgrids without compromising the accuracy of numerical optimization techniques. The effectiveness of the proposed data-driven approach for optimal allocation of microgrid resources in real-time is verified through a comprehensive comparison with conventional numerical optimization approaches.",
        "citation_title": "Physics-informed Convolutional Neural Network for Microgrid Economic Dispatch",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "This paper aims to introduce a new statistical learning technique based on sparsity promoting for data-driven modeling and control of solar photovoltaic (PV) systems. Compared with conventional sparse regression techniques that might introduce computational complexities when the number of candidate functions increases, an innovative algorithm, named adaptive regulated sparse regression (ARSR) is proposed that adaptively regulates the hyperparameter weights of candidate functions to best represent the dynamics of PV systems. Utilizing this algorithm, open-loop and closed-loop models of single-stage and two-stage PV systems are obtained from measurements and are utilized for control design purposes. Moreover, it is demonstrated that the proposed data-driven approach can successfully be employed for fault analysis studies, which distinguishes its capabilities compared with other data-driven techniques. Finally, the proposed approach is validated through real-time simulations.",
        "citation_title": "Adaptive Regulated Sparsity Promoting Approach for Data-Driven Modeling and Control of Grid-Connected Solar Photovoltaic Generation",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Unmanned Aerial Vehicle (UAV) swarms play an effective role in timely data collection from ground sensors in remote and hostile areas. Optimizing the collective behavior of swarms can improve data collection performance. This paper puts forth a new mean field flight resource allocation optimization to minimize age of information (AoI) of sensory data, where balancing the trade-off between the UAVs movements and AoI is formulated as a mean field game (MFG). The MFG optimization yields an expansive solution space encompassing continuous state and action, resulting in significant computational complexity. To address practical situations, we propose, a new mean field hybrid proximal policy optimization (MF-HPPO) scheme to minimize the average AoI by optimizing the UAV's trajectories and data collection scheduling of the ground sensors given mixed continuous and discrete actions. Furthermore, a long short term memory (LSTM) is leveraged in MF-HPPO to predict the time-varying network state and stabilize the training. Numerical results demonstrate that the proposed MF-HPPO reduces the average AoI by up to 45 percent and 57 percent in the considered simulation setting, as compared to multi-agent deep Q-learning (MADQN) method and non-learning random algorithm, respectively.",
        "citation_title": "Age of Information Minimization using Multi-agent UAVs based on AI-Enhanced Mean Field Resource Allocation",
        "date_delivered": "[Submitted on 24 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Background: X-ray grating-based dark-field imaging can sense the small angle scattering caused by an object's micro-structure. This technique is sensitive to lung's porous alveoli and is able to detect lung disease at an early stage. Up to now, a human-scale dark-field CT has been built for lung imaging. Purpose: This study aimed to develop a more thorough optimization method for dark-field lung CT and summarize principles for system design. Methods: We proposed a metric in the form of contrast-to-noise ratio (CNR) for system parameter optimization, and designed a phantom with concentric circle shape to fit the task of lung disease detection. Finally, we developed the calculation method of the CNR metric, and analyzed the relation between CNR and system parameters. Results: We showed that with other parameters held constant, the CNR first increases and then decreases with the system auto-correlation length (ACL). The optimal ACL is nearly not influenced by system's visibility, and is only related to phantom's property, i.e., scattering material's size and phantom's absorption. For our phantom, the optimal ACL is about 0.21 {\\mu}m. As for system geometry, larger source-detector and isocenter-detector distance can increase the system's maximal ACL, helping the system meet the optimal ACL more easily. Conclusions: This study proposed a more reasonable metric and a task-based process for optimization, and demonstrated that the system optimal ACL is only related to the phantom's property.",
        "citation_title": "Optimization of Dark-Field CT for Lung Imaging",
        "date_delivered": "[Submitted on 1 May 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Re-configurable Intelligent Surfaces (RIS) technology is increasingly becoming a potential component for next-generation wireless networks, offering enhanced performance in terms of throughput, spectral, and energy efficiency. However, the broadcast nature of RIS-assisted wireless communication makes it vulnerable to malicious attacks at the physical layer. At the same time, physical layer authentication is gaining popularity as a solution to secure wireless networks, thwarting different attacks such as cloning, spoofing, and impersonation by using the random features of the physical layer.\nIn this paper, we investigate RIS-assisted wireless communication systems to unlock the potential of using RIS for physical layer authentication (PLA). In particular, we exploit two distinct features of the physical layer: pathloss and channel impulse response (CIR) for PLA in RIS-assisted wireless communication. We construct hypothesis tests for the estimated features and derive closed-form error expressions. Further, we consider the critical error, i.e., missed detection, as our objective function to minimize by optimizing the phase shift of the RIS pannel. We compare the performance of our proposed mechanisms with PLA schemes using the same features but with no RIS. Furthermore, we thoroughly evaluate our proposed schemes using performance metrics such as the probability of false alarm (PFA), the probability of missed detection (PMD), and the receiver operating characteristic (ROC) curves. The results demonstrate a clear positive impact of RIS on PLA, as it effectively reduces PMD values to zero when determining the optimal phase shift.",
        "citation_title": "On the Potential of Re-configurable Intelligent Surface (RIS)-assisted Physical Layer Authentication (PLA)",
        "date_delivered": "[Submitted on 1 May 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this paper, we have studied the effects of pole dark energy on the evolution of gravitational waves. The background evolution of gravitational waves in a flat FRW universe is considered and its dynamics are studied in the presence of pole dark energy. Two different potential functions are considered for the study. Using the field equations, we formulated the perturbed equations governing the evolution of gravitational waves with respect to redshift z within the background of the FRW Universe. Subsequently, we delved into the characteristics of gravitational waves for the pole dark energy model and reached interesting results.",
        "citation_title": "The effects of the pole dark energy on gravitational waves",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We revisit the problem of the structure and physical properties of electrically charged static spherically symmetric solutions of the Einstein-Maxwell system of equations where the matter model is a polytropic gas. We consider a relativistic polytrope equation of state and take the electric charge density to be proportional to the rest mass density. We construct the families of solutions corresponding to various sets of parameters and analyze their stability and compliance with the causality requirement, with special emphasis on the possibility of constructing black hole mimickers. Concretely, we want to test how much electric charge a given object can hold and how compact it can be. We conclude that there is a microscopic bound on the charge density to rest mass density ratio coincident with the macroscopic bound regarding the extremal Reissner-Nordst\u00f6m black hole. The macroscopic charge to mass ratio for the object can exceed the corresponding microscopic ratio if the object is non-extremal. Crucially, the only way to obtain a black hole mimicker is by taking a subtle limit in which an electrically counterpoised dust solution is obtained.",
        "citation_title": "Revisiting relativistic electrically charged polytropic spheres",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In this paper, we investigate the gravitational collapse to form the black hole in the acceleratingly expanding universe in the frame of Einstein--Gauss-Bonnet theory having two scalar fields and we study the propagation of the gravitational wave (GW). The collapsing spacetime can be obtained by using the formulation of the ``reconstruction'', that is, we find a model that realises the desired or given geometry. In the reconstructed models, ghosts often appear, which could be eliminated by imposing constraints. We show that the standard cosmological solutions or self-gravitating objects such as a planet, the Sun, various types of stars, etc., in Einstein's gravity, are also solutions in this model. Using the dynamical value of Gauss-Bonnet coupling, the propagation of the high-frequency GW is investigated. The propagating speed changes due to the coupling during the period of the black hole formation. The speed at which the GW propagates The speed at which the GW propagates going into the black hole is different from that of the wave going out.",
        "citation_title": "Gravitational collapse and gravitational wave in Einstein--Gauss-Bonnet theory with two scalar fields",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work we compute numerical bounds on the mass $\\mu$ of superradiantly unstable scalar fields in a Kerr black hole background using the continued fraction method. We show that the normalized upper bound on the mass $\\mu$ increases with the angular momentum number $\\ell$ and the azimuthal number $m$, approaching the most stringent analytical bound known to date when $\\ell=m \\gg 1$. We also provide an analytical fit to the numerically determined mass bound as a function of the dimensionless spin parameter $a/M$ of the black hole with an accuracy of the order $0.1\\%$ for the fundamental mode with $\\ell=m=1$, and of the order $1\\%$ for higher-order modes (up to $\\ell=m=20$). We argue that this analytical fit is particularly useful in astrophysical scenarios, since the lowest $\\ell=m$ modes are capable of producing the strongest observable imprints of superradiance.",
        "citation_title": "Bounds on the mass of superradiantly unstable scalar fields around Kerr black holes",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We develop a procedure for re-summing the large logarithms induced in gravity by loops of inflationary scalars. We first show how the scalar can be integrated out of the field equations in the presence of constant graviton field. We then extend this result to a fully conserved form which explains the need for a finite renormalization of the cosmological constant which was previously inferred from explicit computation. A variant of the renormalization group turns out to explain the large logarithmic corrections revealed by explicit computation in the electric field strength of gravitational radiation and in the potentials which characterize the response to a point mass. The implications for graviton loops are discussed.",
        "citation_title": "Summing Gravitational Effects from Loops of Inflationary Scalars",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The canonical quantisation of General Relativity including matter on a spacetime manifold in the globally hyperbolic setting involves in particular the representation theory of the spatial diffeomorphism group (SDG), and/or its Lie algebra (SDA), of the underlying spatial submanifold. There are well known Fock representations of the SDA in one spatial dimension and non-Fock representations of the SDG in all dimensions. The latter are not strongly continuous and do not descend to representations of the SDA.\nIn this work we report some partial results on non anomalous representations of the SDA for both geometry and matter: 1. Background independent Fock representations of the SDA by operators exist in all dimensions. 2. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA by operators exist in one dimension but these do not extend to higher dimensions. 3. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA of volume preserving diffeomorphisms by operators exist in all dimensions. 4. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA by quadratic forms exist in all dimensions.\nExcept for 1. these representations do not descend from an invariant state of the Weyl algebra and 4. points to a new strategy for solving the quantum constraints.",
        "citation_title": "Observations on representations of the spatial diffeomorphism group and algebra in all dimensions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Perturbative quantum gravity starts from prescribing a background metric. That background metric is then used in order to carry out two separate steps: 1. One splits the non-perturbative metric into background and deviation from it (graviton) and expands the action in terms of the graviton which results in an ifinite series of unknown radius of convergence. 2. One constructs a Fock representation for the graviton and performs perturbative graviton quantum field theory on the fixed background as dictated by the perturbative action. The result is a non-renormalisable theory without predictive power.\nIt is therefore widely believed that a non-perturbative approach is mandatory in order to construct a fundamental, not only effective, predictive quantum field theory of the gravitational interaction. Since perturbation theory is by definition background dependent, the notions of background dependence (BD) and perturbation theory (PT) are often considered as symbiotic, as if they imply each other.\nIn the present work we point out that there is no such symbiosis, these two notions are in fact logically independent. In particular, one can use BD structures while while not using PT at all. Specifically, we construct BD Fock representations (step 2 above) for the full, non-perturbative metric rather than the graviton (not step 1 above) and therefore never perform a perturbative expansion. Despite the fact that the gravitational Lagrangean is a non-polynomial, not even analytic, function of the metric we show that e.g. the Hamiltonian constraint with any density weight can be defined as a quadratic form with dense form domain in such a representation.",
        "citation_title": "Non-perturbative Quantum Gravity in Fock representations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We numerically analyse solutions of the spherically symmetric gravitational Vlasov-Poisson system close to compactly supported stable steady states. We observe either partially undamped oscillations or macroscopically damped solutions. We investigate for many steady states to which of these behaviours they correspond. A linear relation between the exponents of polytropic steady states and the qualitative behaviour close to them is identified. Undamped oscillations are also observed around not too concentrated King models and around all shells with a sufficiently large vacuum region. We analyse all solutions both at the non-linear and linearised level and find that the qualitative behaviours are identical at both. To relate the observed phenomena to theoretical results, we further include a comprehensive numerical study of the radial particle periods in the equilibria.",
        "citation_title": "Numerical experiments on stationary, oscillating, and damped spherical galaxy models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We review the kinematic effects on a gravitational wave due to either a peculiar motion of the astrophysical source emitting it or a local motion of the observer. We show, at fully non-linear order in velocity, that the amplitude of the wave is amplified by the Doppler factor in the case in which the source moves with respect to a reference frame, while it is invariant if the observer moves (with respect to a reference observer). However, the observed specific intensity transforms in the same way under a boost of the source or of the observer. We also show at fully non-linear order that under a boost (of either source or observer), the polarization tensor is rotated in the same way the wave direction is rotated by aberration, such that the only net effect of a boost on polarization is to change the phase of the helicity components. We apply these results to a wave emitted by a binary system of compact objects in the cosmological context.",
        "citation_title": "Boosting gravitational waves: a review of kinematic effects on amplitude, polarization, frequency and energy density",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A brief review of the pseudo complex General Relativity (pcGR) will be presented, with its consequences, as the accumulation of a dark energy around a mass and a generalized Machs principle. The main objective in this contribution is to determine the Hawking temperature and the Entropy for various limits: i) The pc-Schwarzschild case with no minimal length present, ii) the pc-Kerr metric without a minimal length and iii) the general case, the pc-Kerr metric with a minimal length present. The physical consequences of a minimal length will be discussed, a possible interpretation of a gravitational Schwinger effect and the appearance of negative temperature. For large masses a minimal length does not show any sensible effect, but only for very small masses, several orders of the Planck mass, where non-trivial effects emerge, important for the production of mini-black holes in the early universe. Our results are more general than being restricted to pcGR. Any other model which assumes a distribution of dark energy around a stellar body produces the same effects. In contrast to these models, pcGR demands the presence of a dark energy term.",
        "citation_title": "The effects of a minimal length on the Kerr metric and the Hawking temperature",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "While numerical simulations offer unparalleled precision and robustness in studying complex physical systems, their execution is often hindered by complexity, costliness, and time consumption due to the intricate equations involved. This challenge is already encountered in General Relativity, where non-flat spacetimes exacerbate the computational burden. This complexity is further intensified when dealing with additional degrees of freedom. To address this challenge head-on, we introduce GRBoondi, a groundbreaking fixed-background numerical relativity code designed to provide a unified interface for numerically solving Generalized Proca theories. GRBoondi grants users the ability to make arbitrary modifications to the Proca equations of motion on any background, providing a robust and versatile tool for exploring diverse classes of Generalized Proca theories. This letter serves as part of the submission of GRBoondi to the Journal of Open Source Software. For access to the code, please visit this https URL.",
        "citation_title": "GRBoondi: A code for evolving Generalized Proca theories on arbitrary backgrounds",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This thesis is developed in the context of the spin-foam approach to quantum gravity; all results are concerned with the Lorentzian theory and with semiclassical methods. A correspondence is given between Majorana 2-spinors and time-like hypersurfaces in Minkowski 3-space based on complexified quaternions. It is shown that the former suggest a symplectic structure on the spinor phase space which, together with an area-matching constraint, yields a symplectomorphism to $T^*\\mathrm{SU}(1,1)$. A complete 3-dimensional Lorentzian spin-foam amplitude for both space- and time-like triangles is proposed. It is shown to asymptote to Regge theory in the semiclassical regime. The asymptotic limit of the 4-dimensional Conrady-Hnybida model for general polytopes is scrutinized. Minkowski's theorem on convex polyhedra is generalized to Lorentzian signature, and new boundary states for time-like polygons are introduced. It is found that the semiclassical amplitude for such polygons is insufficiently constrained. A method for the asymptotic evaluation of integrals subject to external parameters is discussed. The method is developed in detail for the special problem of spin-foam gluing constraints away from their dominant critical points. A relation to the gluing constraints of effective spin-foams is suggested.",
        "citation_title": "Investigations on Lorentzian Spin-foams and Semiclassical Space-times",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Different approaches to quantum gravity, such as loop quantum cosmology and group field theory, predict the resolution of the initial cosmological singularity via a 'bounce': a regular spacetime region that connects the expanding branch of the universe to a contracting branch. The cosmological arrow of time, which by definition points in the direction of cosmic expansion, is reversed at the bounce. Nonetheless, it is still possible to discriminate between the two branches by considering different arrows, as defined for instance by the growth of perturbations. After reviewing general aspects of the time arrow problem in cosmology, we examine the properties of different arrows of time in bouncing cosmologies, focusing on the loop quantum cosmology bounce as a case study. We also present a new exact solution to the effective Friedmann equations of loop quantum cosmology with pressureless dust and a cosmological constant, which is a simplified version of the $\\Lambda$CDM bounce scenario, where these issues can be examined in detail.",
        "citation_title": "Arrows of time in bouncing cosmologies",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Gertsenshtein effect could in principle be used to detect a single graviton by firing it through a region filled with a constant magnetic field that enables its conversion to a photon, which can be efficiently detected via standard techniques. The quantization of the gravitational field could then be inferred indirectly. We show that for currently available single-photon detector technology, the Gertsenshtein detector is generically inefficient, meaning that the probability of detection is $\\ll 1$. The Gertsenshtein detector can become efficient on astrophysical scales for futuristic single-photon detectors sensitive to frequencies in the Hz to kHz range. It is not clear whether such devices are in principle possible.",
        "citation_title": "Graviton-photon oscillations as a probe of quantum gravity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In previous papers of this series we analysed the reduced phase space approach to perturbations of Einstein-Maxwell theory to second order around spherically symmetric backgrounds in the Gullstrand Painlev\u00e9 Gauge and confirmed consistency with previous approaches. In this paper we generalize this result and show that the analysis can be performed in gauges for the background variables compatible with the Gullstrand Painlev\u00e9 gauge. We obtain the same structure for the reduced Hamiltonian that contains the well known Regge-Wheeler and Zerilli potentials. Possible applications of this generalization are discussed.",
        "citation_title": "Quantum Field Theory of Black Hole Perturbations with Backreaction IV. Spherically symmetric 2nd order Einstein-Maxwell sector in generalised gauges",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "During the past few decades, abundant evidence for physics beyond the two standard models of particle physics and cosmology was found. Yet, we are tapping into the dark regarding our understanding of the dark sector. For more than a century, open problems related to the nature of the vacuum remain unresolved. Besides the traditional high-energy frontier and cosmology, technological advancement provides complementary access to new physics via high-precision experiments. Among the latter, the Casimir And Non-Newtonian force EXperiment (\\cannex{}) has successfully completed its proof-of-principle phase and will soon commence operation. Benefiting from its plane parallel plate geometry, both interfacial and gravity-like forces are maximized, leading to increased sensitivity. A wide range of dark sector forces, Casimir forces in and out of thermal equilibrium, and gravity will be tested. This article describes the final experimental design, its sensitivity, and expected results.",
        "citation_title": "Force metrology with plane parallel plates: Final design review and outlook",
        "date_delivered": "[Submitted on 16 Mar 2024]"
    },
    {
        "abstract": "In 1993, the global stability of Minkowski spacetime has been proven in the celebrated work of Christodoulou and Klainerman \\cite{Ch-Kl}. In 2003, Klainerman and Nicol\u00f2 \\cite{Kl-Ni} revisited Minkowski stability in the exterior of an outgoing null cone. In \\cite{Shen23}, the author extended the results of \\cite{Ch-Kl} to minimal decay assumptions. In this paper, we prove that the exterior stability of Minkowski holds with decay which is borderline compared to the minimal decay considered in \\cite{Shen23}.",
        "citation_title": "Exterior stability of Minkowski spacetime with borderline decay",
        "date_delivered": "[Submitted on 29 Apr 2024]"
    },
    {
        "abstract": "Finsler geometry is a natural generalization of (pseudo-)Riemannian geometry, where the line element is not the square root of a quadratic form but a more general homogeneous function. Parameterizing this in terms of symmetric tensors suggests a possible interpretation in terms of higher-spin fields. We will see here that, at linear level in these fields, the Finsler version of the Ricci tensor leads to the curved-space Fronsdal equation for all spins, plus a Stueckelberg-like coupling. Nonlinear terms can also be systematically analyzed, suggesting a possible interacting structure. No particular choice of spacetime dimension is needed. The Stueckelberg mechanism breaks gauge transformations to a redundancy that does not change the geometry. This is however not enough to eliminate non-transverse modes, at least for some versions of Finsler dynamics.",
        "citation_title": "Higher spins and Finsler geometry",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Recently Danielson, Satishchandran, and Wald (DSW) have shown that quantum superpositions held outside of Killing horizons will decohere at a steady rate. This occurs because of the inevitable radiation of soft photons (gravitons), which imprint a electromagnetic (gravitational) ``which-path'' memory onto the horizon. Rather than appealing to this global description, an experimenter ought to also have a local description for the cause of decoherence. One might intuitively guess that this is just the bombardment of Hawking/Unruh radiation on the system, however simple calculations challenge this idea -- the same superposition held in a finite temperature inertial laboratory does not decohere at the DSW rate. In this work we provide a local description of the decoherence by mapping the DSW set-up onto a worldline-localized model resembling an Unruh-DeWitt particle detector. We present an interpretation in terms of random local forces which do not sufficiently self-average over long times. Using the Rindler horizon as a concrete example we clarify the crucial role of temperature, and show that the Unruh effect is the only quantum mechanical effect underlying these random forces. A general lesson is that for an environment which induces Ohmic friction on the central system (as one gets from the classical Abraham-Lorentz-Dirac force, in an accelerating frame) the fluctuation-dissipation theorem implies that when this environment is at finite temperature it will cause steady decoherence on the central system. Our results agree with DSW and provide the complementary local perspective.",
        "citation_title": "Decoherence by warm horizons",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The kinetic Sunyaev Zel'dovich (kSZ) effect is a blackbody cosmic microwave background (CMB) temperature anisotropy induced by Thomson scattering off free electrons in bulk motion with respect to the CMB rest frame. The statistically anisotropic cross-correlation between the CMB and galaxy surveys encodes the radial bulk velocity (more generally, the remote dipole field), which can be efficiently reconstructed using a quadratic estimator. Here, we develop and implement a quadratic estimator for the remote dipole field to data from the Planck satellite and the unWISE galaxy redshift catalog. With this data combination, we forecast a $\\sim 1$-$\\sigma$ detection within $\\Lambda$CDM assuming a simple model for the distribution of free electrons. Using reconstructions based on individual frequency temperature maps, we characterize the impact of foregrounds, concluding that they can be effectively mitigated by masking and removing the estimator monopole. We demonstrate that reconstructions based on component-separated CMB maps have no detectable biases from foregrounds or systematics at the level of the expected statistical error. We use these reconstructions to constrain the multiplicative optical depth bias to $b_v < 1.40$ at $68 \\%$ confidence. Our fiducial signal model with $b_v =1$ is consistent with this measurement. Our results support an optimistic future for kSZ velocity reconstruction with near-term datasets.",
        "citation_title": "Kinetic Sunyaev Zel'dovich velocity reconstruction from Planck and unWISE",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We derive the generalized second law (GSL) for arbitrary cuts of Killing horizons from the perspective of crossed-product gravitational algebras, making use of a recent proposal by one of us for the construction of local gravitational algebras. This construction relies on the existence of a state whose modular flow is geometric on the horizon. In both free and interacting quantum field theories, such states are guaranteed to exist by the properties of half-sided translations on the horizon. Using geometric identities derived from the canonical analysis of general relativity on null surfaces, we show that the crossed product entropy agrees with the generalized entropy of the horizon cut in a semiclassical limit, and further reproduce Wall's result relating the GSL to monotonicity of relative entropy of the quantum field algebras. We also give a novel generalization of the GSL for interacting theories in asymptotically flat spacetimes involving the concept of an algebra at infinity for a half-sided translation, which accounts for triviality of the algebra of fields smeared only on the horizon. Going beyond the semiclassical limit, we compute subleading corrections to the crossed product entropy, but are unable to determine if the GSL continues to hold after accounting for these. We speculate that an improved GSL could follow from a hidden subalgebra structure of the crossed products, assuming the existence of an operator-valued weight between horizon cut algebras.",
        "citation_title": "Gravitational algebras and the generalized second law",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We consider the Euclidean vacuum for linearized gravity on the global de Sitter space, obtained from the Euclidean Green's function on the 4-sphere. We use the notion of Calder\u00f3n projectors to recover a quantum state for the Lorentzian theory on de Sitter space. We show that while the state is gauge invariant and Hadamard, it is not positive on the whole of the phase space. We show however that a suitable modification at low energies yields a well-defined Hadamard state on global de Sitter space.",
        "citation_title": "IR-fixed Euclidean vacuum for linearized gravity on de Sitter space",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In the AdS$_3$/CFT$_2$ framework, the Euclidean BTZ black hole corresponds to the dominant high-temperature phase of its dual field theory. We initially employ perturbative methods to solve the Einstein equations as boundary value problems, providing correlators for the energy-momentum tensor operator at low points. Utilizing operator equations established in our previous work, we further compute arbitrary high-point correlators for the energy-momentum tensor operator in the high-temperature phase and recursive relations for these high-point functions. Concurrently, we employ the Chern-Simons formalism to derive consistent results. Further, using the cut-off AdS/$T\\bar{T}$-deformed CFT duality, we calculate the energy-momentum tensor correlators, contributing to the comprehensive understanding of the system's dynamics. Finally, stress tensor correlators enable us to ascertain the corresponding KdV operator correlators at low-temperature.",
        "citation_title": "Note on holographic torus stress tensor correlators in $AdS_3$ gravity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We derive the cutoff length scale of the quadratic gravity in $d \\geq 5$ dimensional spacetime by demanding that the quantum focusing conjecture for the smeared quantum expansion holds at the classical level. The cutoff scale has different dependence on the spacetime dimension depending on the sign of the coupling constant of the quadratic gravity. We also investigate a concrete example of the 5-dimensional Schwarzschild spacetime and directly confirm that the quantum focusing conjecture holds when the quantum expansion is smeared over the scale larger than our cutoff scale.",
        "citation_title": "Cutoff Scale of Quadratic Gravity from Quantum Focusing Conjecture",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we systematically investigate the inflationary complexity of the two-mode squeezed state with thermal effect for the single field inflation, modified dispersion relation, and non-trivial sound speed with the method of closed system and open system, respectively, which our analysis is valid for most inflationary models. First, the numeric of Krylov complexity in the method of the closed system indicates that the evolution of Krylov complexity highly depends on the squeezed angle parameter once taking the thermal effect into account, which will decay into some very tiny values, but the Krylov complexity will always enhance without thermal effect. For comparison, the numeric of circuit complexity shows that the evolution is always increasing no matter whether there are thermal effects or not which is independent of the evolution of squeezed angle parameter. By utilizing the method of open system, we first construct the wave function. As for the Krylov complexity with the method of open system, our investigations show the evolution of Krylov complexity will enhance upon some peaks factoring in the thermal effects. For completeness, we also calculate the Krylov entropy in the method of closed system and open system, which indicates that the hotter universe, the more chaotic the universe. Furthermore, our derivation for the Krylov complexity and Krylov entropy could nicely recover into the case of closed system under weak dissipative approximation, which confirms the validity of construction for the wave function. Finally, our numeric of Lanczos coefficient shows that the non-trivial sound speed has minimal chaos compared to the other two cases.",
        "citation_title": "Inflationary complexity of thermal state",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Neutron stars represent unique laboratories, offering insights into the physics of supranuclear-density matter and serving as potential hosts for dark matter. This study explores the impact of dark matter cores on rapidly rotating neutron stars through the two-fluid approximation, assuming minimal interaction between baryonic matter and dark matter. The investigation employs phenomenological models for fermionic and bosonic dark matter, revealing that universal relations governing mass and radius changes due to rotation remain largely unaffected in the presence of a dark matter core. Specifically, for a 5 % dark matter mass fraction, the percent deviations in total mass ($M_{tot}$), the baryonic equatorial radius ($R_{Be}$), and polar-to-equatorial baryonic radius ratio ($R_{ratioB}$) are within 3.9 %, 1.8 %, and 1.4 %, respectively. These findings suggest that the universal relations governing neutron star shape can be utilized to infer constraints on the properties of dark matter cores even in cases where the dark matter significantly softens the neutron star's equation of state.",
        "citation_title": "The Effect of a Dark Matter Core on the Structure of a Rotating Neutron Star",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Effective Lifshitz black holes with arbitrary dynamical exponent are addressed in the fluid/gravity membrane paradigm. The transport and the response coefficients in the dual Lifshitz field theory are calculated and analyzed, including the charge diffusion constant and the shear mode damping constant, along with the shear-viscosity-to-entropy density ratio. The Kubo formula is employed to obtain the electrical DC conductivity for the gauge sector corresponding to impurity through the holographic linear response of gauge vector fluctuations in the Lifshitz black brane geometry.",
        "citation_title": "Effective Lifshitz black holes, hydrodynamics, and transport coefficients in fluid/gravity correspondence",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We show that when the Wald-Zoupas prescription is implemented, the resulting charges realize the BMS symmetry algebra without any 2-cocycle nor central extension, at any cut of future null infinity. We refine the covariance prescription for application to the charge aspects, and introduce a new aspect for Geroch's super-momentum with better covariance properties. For the extended BMS symmetry with singular conformal Killing vectors we find that a Wald-Zoupas symplectic potential exists, if one is willing to modify the symplectic structure by a corner term. The resulting algebra of Noether currents between two arbitrary cuts is center-less. The charge algebra at a given cut has a residual field-dependent 2-cocycle, but time-independent and non-radiative. More precisely, super-rotation fluxes act covariantly, but super-rotation charges act covariantly only on global translations. The take home message is that in any situation where 2-cocycles appears in the literature, covariance has likely been lost in the charge prescription, and that the criterium of covariance is a powerful one to reduce ambiguities in the charges, and can be used also for ambiguities in the charge aspects.",
        "citation_title": "Centerless-BMS charge algebra",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider two non-relativistic quantum clocks interacting with a Newtonian gravitational field produced by a spherical mass. In the framework of Page and Wootters approach, we derive a time dilation for the time states of the clocks. The delay is in agreement up to first order with the gravitational time dilation obtained from the Schwarzschild metric. This result can be extended by considering the relativistic gravitational potential: in this case we obtain the agreement with the exact Schwarzschild solution.",
        "citation_title": "Time dilation of quantum clocks in a Newtonian gravitational field",
        "date_delivered": "[Submitted on 9 Apr 2023 (v1), last revised 1 May 2024 (this version, v4)]"
    },
    {
        "abstract": "In this work we study the gravitational radiation produced by a keplerian binary system within the context of Very Special Linear Gravity (VSLG), a novel theory of linearized gravity in the framework of Very Special Relativity (VSR) allowing for a gauge-invariant mass $m_g$ of the graviton. For this task, we exploit Effective Field Theory's techniques, which require, among others, the calculation of the squared amplitude of the emission process and therefore the polarization sum for VSLG gravitons. Working in the radiation zone and using the standard energy momentum tensor's expression for keplerian binaries, we derive and study the properties of the VSLG energy loss and period decrease rates, also verifying they reduce to the correct General Relativity limit when sending $m_g\\to0$. Finally, using astronomical data from the Hulse-Taylor binary and the Double Pulsar J0737-3039, we obtain an upperbound on the VSLG graviton mass of $m_g\\sim 10^{-21}eV$ that, while being comparable to bounds obtained in this same way for other massive gravity models, is still weaker than the kinematical bound $\\sim 10^{-22}eV$ obtained from the combined observation of the astronomical events GW170817 and GRB170817A, which should still hold in VSLG.",
        "citation_title": "Graviton Mass Bounds in Very Special Relativity from Binary Pulsar's Gravitational Waves",
        "date_delivered": "[Submitted on 4 Jun 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We propose a setting that simulates Hawking radiation from an analogue bouncing geometry, i.e., a collapsing geometry that reverts its collapse after a finite time, in a setup consisting of a coplanar waveguide terminated in superconducting quantum-interference devices at both ends. We demonstrate experimental feasibility of the proposed setup within the current technology. Our analysis illustrates the resilience of Hawking radiation under changes in the physics at energy scales much larger than the temperature, supporting the idea that regular alternatives to black holes would also emit Hawking radiation.",
        "citation_title": "Hawking radiation from an analogue bouncing geometry",
        "date_delivered": "[Submitted on 8 Jun 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We investigate the late-time cosmological dynamics in a simple case of explicit spacetime-symmetry breaking. By expanding in a small symmetry-breaking coefficient we are able to write the Friedmann equations as $\\Lambda$CDM + dynamical dark energy, which we show contains logarithmic dependence of the scale factor. We find that the dark energy equation of state displays divergencies and phantom behaviour for certain values of the symmetry-breaking coefficient, where the NEC is also broken. We discuss the adiabatic sound speed of dark energy and compare the model to current constraints using the Chevallier-Polarski-Linder parametrisation. Remarkably, although the constraints on the same symmetry-breaking coefficient from e.g. gravitational-wave propagation are orders of magnitude stronger than what we obtain in this paper, we are able to cut those constraints, which are more or less symmetric around zero, in half by showing that same coefficient must be negative (or zero) if one wishes to keep the NEC intact.",
        "citation_title": "Dynamical dark energy from spacetime-symmetry breaking -- late-time behaviour and phantom crossing",
        "date_delivered": "[Submitted on 18 Jul 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "An effective field theory framework is used to investigate some Lorentz-violating effects on the generation of electromagnetic and gravitational waves, complementing previous work on propagation. Specifically we find solutions to a modified, anisotropic wave equation, sourced by charge or fluid matter. We derive the radiation fields for scalars, classical electromagnetic radiation, and partial results for gravitational radiation. For gravitational waves, the results show longitudinal and breathing polarizations proportional to coefficients for spacetime-symmetry breaking.",
        "citation_title": "Classical radiation fields for scalar, electromagnetic, and gravitational waves with spacetime-symmetry breaking",
        "date_delivered": "[Submitted on 25 Jul 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "General Relativity (GR) and Unimodular Gravity (UG) provide two equivalent descriptions of gravity that differ in the nature of the cosmological constant. While GR is based on the group of diffeomorphisms that permits the cosmological constant in the action, UG is based on the subgroup of volume-preserving diffeomorphisms together with Weyl transformations which forbid the presence of the cosmological constant. However, the cosmological constant reappears in UG as an integration constant so it arises as a global degree of freedom. Since gauge symmetries are simply redundancies in our description of physical systems, a natural question is whether there exists a \"parent theory\" with the full diffeomorphisms and Weyl transformations as gauge symmetries so that it reduces to GR and UG respectively by performing suitable (partial) gauge fixings. We will explore this question by introducing Stueckelberg fields both in GR and UG to complete the gauge symmetries in each theory to that of the would-be parent theory. Despite the dynamical equivalence of the two theories, we find that precisely the additional global degree of freedom provided by the cosmological constant in UG obstructs the construction of the parent theory.",
        "citation_title": "Nonexistence of a parent theory for general relativity and unimodular gravity",
        "date_delivered": "[Submitted on 13 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Gravitational Waves (GWs) have been detected in the $\\sim$100 Hz and nHz bands, but most of the gravitational spectrum remains unobserved. A variety of detector concepts have been proposed to expand the range of observable frequencies. In this work, we study the capability of GW detectors in the ``mid-band'', the $\\sim$30 mHz -- 10 Hz range between LISA and LIGO, to measure the signals from and constrain the properties of ${\\sim}$1 -- 100 $M_\\odot$ compact binaries. We focus on atom-interferometer-based detectors. We describe a Fisher matrix code, AIMforGW, which we created to evaluate their capabilities, and present numerical results for two benchmarks: terrestrial km-scale detectors, and satellite-borne detectors in medium Earth orbit. Mid-band GW detectors are particularly well-suited to pinpointing the location of GW sources on the sky. We demonstrate that a satellite-borne detector could achieve sub-degree sky localization for any detectable source with chirp mass $\\mathcal{M}_c \\lesssim 50 M_\\odot$. We also compare different detector configurations, including different locations of terrestrial detectors and various choices of the orbit of a satellite-borne detector. As we show, a network of only two terrestrial single-baseline detectors or one single-baseline satellite-borne detector would each provide close-to-uniform sky-coverage, with signal-to-noise ratios varying by less than a factor of two across the entire sky. We hope that this work contributes to the efforts of the GW community to assess the merits of different detector proposals.",
        "citation_title": "Gravitational Wave Measurement in the Mid-Band with Atom Interferometers",
        "date_delivered": "[Submitted on 14 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Quantum-clock interferometry has been suggested as a quantum probe to test the universality of free fall (UFF) and the universality of gravitational redshift (UGR). In typical experimental schemes it seems advantageous to employ Doppler-free E1-M1 transitions which have so far been investigated in quantum gases at rest. Here, we consider the fully quantized atomic degrees of freedom and study the interplay of the quantum center-of-mass (COM) $-$ that can become delocalized $-$ together with the internal clock transitions. In particular, we derive a model for finite-time E1-M1 transitions with atomic intern-extern coupling and arbitrary position-dependent laser intensities. We further provide generalizations to the ideal expressions for perturbed recoilless clock pulses. Finally, we show at the example of a Gaussian laser beam that the proposed quantum-clock interferometers are stable against perturbations from varying optical fields for a sufficiently small quantum delocalization of the atomic COM.",
        "citation_title": "Finite Pulse-Time Effects in Long-Baseline Quantum Clock Interferometry",
        "date_delivered": "[Submitted on 25 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We construct a doped holographic superconductor in the Gubser-Rocha model, and realize a superconducting dome in the middle of the temperature-doping phase diagram. It is worth noting that unlike the previous researches, the profile of our dome shrinks inward near zero temperature. From the numerical observation for the coupling dependence of the phase diagram, we find that the coupling between the two gauge fields plays a crucial role in the formation of dome. We also analytically calculate the DC conductivity of the normal phase of the system in the momentum dissipation and obtain the resistivity which is proportional to temperature. The AC conductivity is calculated numerically.",
        "citation_title": "Doped Holographic Superconductors in Gubser-Rocha model",
        "date_delivered": "[Submitted on 26 Sep 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We discuss relations between closed and open string amplitudes at one-loop. While at tree-level these relations are known as Kawai-Lewellen-Tye (KLT) and/or double copy relations, here we investigate how such relations are manifested at one-loop. While there exist examples of one-loop closed string amplitudes that can strikingly be written as sum over squares of one-loop open string amplitudes, generically the one-loop closed string amplitudes assume a form reminiscent from the one-loop doubly copy structure of gravitational amplitudes involving a loop momentum. This double copy structure represents the one-loop generalization of the KLT relations.",
        "citation_title": "One-loop Double Copy Relation in String Theory",
        "date_delivered": "[Submitted on 11 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In the Randall-Sundrum (RS) II braneworld scenario, general relativity (GR) is modified by adding an extra dimension such that it is indistinguishable from GR in the weak gravity limit. However, such modifications may leave a mark in the strong field regime. We therefore analyze massive scalar perturbations around rotating black holes in the RS II model. Unlike black holes in GR, these braneworld black holes carry a tidal charge that contains information about the extra spatial dimension, and the rotation parameter for such black holes can exceed unity. Through the method of continued fractions, we investigate the quasinormal mode spectra, and the superradiant instabilities associated with the existence of quasibound states, that is, gravitational atoms. In comparison to the four-dimensional Kerr black hole, we report distinctive signatures of the tidal charge and the rotation parameter, which manifest as signals of the extra dimension, on both the fundamental quasinormal mode and the formation of gravitational atoms. These findings offer insights into testing modifications to GR and detecting ultralight bosonic particles around black holes.",
        "citation_title": "Gravitational atoms in the braneworld scenario",
        "date_delivered": "[Submitted on 12 Dec 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We develop a numerical approach to compute polar parity perturbations within fully relativistic models of black hole systems embedded in generic, spherically symmetric, anisotropic fluids. We apply this framework to study gravitational wave generation and propagation from extreme mass-ratio inspirals in the presence of several astrophysically relevant dark matter models, namely the Hernquist, Navarro-Frenk-White, and Einasto profiles. We also study dark matter spike profiles obtained from a fully relativistic calculation of the adiabatic growth of a BH within the Hernquist profile, and provide a closed-form analytic fit of these profiles. Our analysis completes prior numerical work in the axial sector, yielding a fully numerical pipeline to study black hole environmental effects. We study the dependence of the fluxes on the DM halo mass and compactness. We find that, unlike the axial case, polar fluxes are not adequately described by simple gravitational-redshift effects, thus offering an exciting avenue for the study of black hole environments with gravitational waves.",
        "citation_title": "Black holes surrounded by generic matter distributions: polar perturbations and energy flux",
        "date_delivered": "[Submitted on 1 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The generalized post-Keplerian parametrization for compact binaries on eccentric bound orbits is established at second post-Newtonian (2PN) order in a class of massless scalar-tensor theories. This result is used to compute the orbit-averaged flux of energy and angular momentum at Newtonian order, which means relative 1PN order beyond the leading-order dipolar radiation of scalar-tensor theories. The secular evolution of the orbital elements is then computed at 1PN order. At leading order, the closed form \"Peters and Mathews\" relation between the semi-major axis $a$ and the eccentricity $e$ is found to be independent of any scalar-tensor parameter, and is given by $a \\propto e^{4/3}/(1-e^2)$. Finally, the waveform is obtained at Newtonian order in the form of a spherical harmonic mode decomposition, extending to eccentric orbits the results obtained in arXiv:2201.10924.",
        "citation_title": "Quasi-Keplerian parametrization for eccentric compact binaries in scalar-tensor theories at second post-Newtonian order and applications",
        "date_delivered": "[Submitted on 12 Jan 2024 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We present a new method and implementation to obtain Bayesian posteriors on the amplitude parameters $\\{h_0, \\cos \\iota, \\psi, \\phi_0\\}$ of continuous-gravitational waves emitted by known pulsars. This approach leverages the well-established $\\mathcal{F}$-statistic framework and software. We further explore the benefits of employing a likelihood function that is analytically marginalized over $\\phi_0$, which avoids signal degeneracy problems in the $\\psi$-$\\phi_0$ subspace. The method is tested on simulated signals, hardware injections in Advanced-LIGO detector data, and by performing percentile-percentile (PP) self-consistency tests of the posteriors via Monte-Carlo simulations. We apply our methodology to PSR J1526-2744, a recently discovered millisecond pulsar. We find no evidence for a signal and obtain a Bayesian upper limit $h_0^{95\\%}$ on the gravitational-wave amplitude of approximately $7 \\times 10^{-27}$, comparable with a previous frequentist upper limit.",
        "citation_title": "Bayesian $\\mathcal{F}$-statistic-based parameter estimation of continuous gravitational waves from known pulsars",
        "date_delivered": "[Submitted on 30 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Low-acceleration gravitational anomaly is investigated with a new method of exploiting the normalized velocity profile $\\tilde{v}\\equiv v_p/v_c$ of wide binary stars as a function of the normalized sky-projected radius $s/r_{\\rm{M}}$ where $v_p$ is the sky-projected relative velocity between the pair, $v_c$ is the Newtonian circular velocity at the sky-projected separation $s$, and $r_{\\rm{M}}$ is the MOND radius. With a Monte Carlo method Gaia observed binaries and their virtual Newtonian counterparts are probabilistically distributed on the $s/r_{\\rm{M}}$ versus $\\tilde{v}$ plane and a logarithmic velocity ratio parameter $\\Gamma$ is measured in the bins of $s/r_{\\rm{M}}$. With three samples of binaries covering a broad range in size, data quality, and implied fraction of hierarchical systems including a new sample of 6389 binaries selected with accurate distances and radial velocities, I find a unanimous systematic variation from the Newtonian flat line. With $\\Gamma=0$ at $s/r_{\\rm{M}}\\lesssim 0.15$ or $s\\lesssim 1$~kilo astronomical units (kau), I get $\\Gamma=0.068\\pm 0.015$ (stat) $_{-0.015}^{+0.024}$ (syst) for $s/r_{\\rm{M}} \\gtrsim 0.7$ or $s\\gtrsim 5$~kau. The gravitational anomaly (i.e.\\ acceleration boost) factor given by $\\gamma_g = 10^{2\\Gamma}$ is measured to be $\\gamma_g = 1.37_{-0.09}^{+0.10}$ (stat) $_{-0.09}^{+0.16}$ (syst). With a reduced $\\chi^2$ test of Newtonian and Milgromian nonrelativistic theories, I find that Newtonian gravity is ruled out at $5.8\\sigma$ ($\\chi^2_\\nu=9.4$) by the new sample (and $9.2\\sigma$ by the largest sample used). The Milgromian AQUAL theory is acceptable with $0.5\\lesssim \\chi^2_\\nu\\lesssim 3.1$. These results agree well with earlier results with the ``acceleration-plane analysis'' for a variety of samples and the ``stacked velocity profile analysis'' for a pure binary sample.",
        "citation_title": "Measurements of the Low-Acceleration Gravitational Anomaly from the Normalized Velocity Profile of Gaia Wide Binary Stars and Statistical Testing of Newtonian and Milgromian Theories",
        "date_delivered": "[Submitted on 8 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In Atazadeh and Hadi (JCAP \\textbf{01}, 067 (2024)), the authors proposed that black bounce solutions, such as the Simpson-Visser and the Bardeen-type spacetimes, can be obtained from Rastall gravity. To achieve these spacetimes, the authors consider the presence of a phantom scalar field with nonlinear electrodynamics. However, in this comment, we obtained different electromagnetic Lagrangians from the original work. The most problematic issue is not the incorrect expression of the electromagnetic Lagrangian itself. We show that the method obtains electromagnetic functions that are inconsistent. To obtain the black bounce spacetimes as solutions of Rastall gravity, it is necessary to consider an isotropic fluid, combined with the nonlinear electrodynamics and the phantom scalar field.",
        "citation_title": "Comment on \"Source of black bounces in Rastall gravity''",
        "date_delivered": "[Submitted on 27 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "What is the bulk Hilbert space of quantum gravity? In this paper, we resolve this problem in 2d JT gravity, both with and without matter, providing the first example of an explicit definition of a non-perturbative Hilbert space specified in terms of metric variables. The states are wavefunctions of the length and matter state, but with a non-trivial and highly degenerate inner product. We explicitly identify the null states, and discuss their importance for defining operators non-perturbatively. To highlight the power of the formalism we developed, we study the non-perturbative effects for two bulk linear operators that may serve as proxies for the experience of an observer falling into a two-sided black hole: one captures the length of an Einstein-Rosen bridge and the other captures the center-of-mass collision energy between two particles falling from opposite sides. We track the behavior of these operators up to times of order $e^{S_\\text{BH}}$, at which point the wavefunction spreads to the complete set of eigenstates of these operators. If these observables are indeed good proxies for the experience of an infalling observer, our results indicate an O(1) probability of detecting a firewall at late times that is self-averaging and universal.",
        "citation_title": "On the non-perturbative bulk Hilbert space of JT gravity",
        "date_delivered": "[Submitted on 13 Mar 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In view to scrutinize the idea that nonlocal modifications of General Relativity could dynamically address the dark energy problem, we investigate the evolution of the Universe at infrared scales as an Infinite Derivative Gravity model of the Ricci scalar, without introducing the cosmological constant $\\Lambda$ or any scalar field. The accelerated expansion of the late Universe is shown to be compatible with the emergence of nonlocal gravitational effects at sufficiently low energies. A technique for circumventing the mathematical complexity of the nonlocal cosmological equations is developed and, after drawing a connection with the Starobinsky gravity, verifiable predictions are considered, like a possible decreasing in the strength of the effective gravitational constant. In conclusion, the emergence of nonlocal gravity corrections at given scales could be an efficient mechanism to address the dark energy problem.",
        "citation_title": "Can nonlocal gravity really explain dark energy?",
        "date_delivered": "[Submitted on 17 Mar 2024 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "The Event Horizon Telescope (EHT) has revolutionized our ability to study black holes by providing unprecedented spatial resolution and unveiling horizon-scale details. With advancements leading to the next-generation EHT, there is potential to probe even deeper into the black hole's dark region, especially the inner shadow characterized by low-intensity foreground emissions from the jet, thanks to a significant enhancement in dynamic range by two orders of magnitude. We demonstrate how such enhanced observations could transform supermassive black holes into powerful probes for detecting annihilating dark matter, which can form a dense profile in the vicinity of supermassive black holes, by examining the morphology of the black hole image.",
        "citation_title": "Illuminating Black Hole Shadow with Dark Matter Annihilation",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "A search is presented for an extended Higgs sector with two new particles, X and $\\phi$, in the process X $\\to$ $\\phi\\phi$ $\\to$ $(\\gamma\\gamma)(\\gamma\\gamma)$. Novel neural networks classify events with diphotons that are merged and determine the diphoton masses. The search uses LHC proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected with the CMS detector, corresponding to an integrated luminosity of 138 fb$^{-1}$. No evidence of such resonances is seen. Upper limits are set on the production cross section versus the resonance masses, representing the most sensitive search in this channel.",
        "citation_title": "Search for new resonances decaying to pairs of merged diphotons in proton-proton collisions at $\\sqrt{s}$ = 13 TeV",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Liquid argon is an excellent medium for detecting particles, given its yields and transport properties of light and charge. The technology of liquid argon time projection chambers has reached its full maturity after four decades of continuous developments and is, or will be, used in world class experiments for neutrino and dark matter searches. The collection of ionization charge in these detectors allows to perform a complete tridimensional reconstruction of the tracks of charged particles, calorimetric measurements, particle identification. This work proposes a novel approach to the problem of charge recombination in liquid argon which moves from a microscopic model and is applied to the cases of low energy electrons, alpha particles and nuclear recoils. The model is able to describe precisely several sets of experimental data available in the literature, over wide ranges of electric field strengths and kinetic energies and can be easily extended to other particles.",
        "citation_title": "Properties of Charge Recombination in Liquid Argon",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We report a methodology to determine the quality factor ($Q$) in implementations of the so-called dielectric haloscope, a new concept of wavy dark matter detector equipped with a multilayered resonator. An anechoic chamber enables the observation of the resonance frequency and its amplitude for an unlimited series of layers for the first time, which is conveniently filtered. The frequency-normalized power enhancement measured in a Dark-photons \\& Axion-Like particles Interferometer (DALI) prototype is a few hundred per layer over a sweep bandwidth of half a hundred MHz. In light of this result, this scaled-down prototype is sensitive to axions saturating the local dark matter density with a coupling to photons between $g_{a\\gamma\\gamma}\\gtrsim10^{-12}$ GeV$^{-1}$ and $g_{a\\gamma\\gamma}\\gtrsim$ few $\\times 10^{-14}$ GeV$^{-1}$ at frequencies of several dozens of GHz once cooled down to the different working temperatures of the experiment and immersed in magnetic fields ranging from 1 T to 10 T; while the sensitivity of the full-scale DALI is projected at $g_{a\\gamma\\gamma}\\gtrsim\\mathrm{few}\\times10^{-15}$ GeV$^{-1}$ over the entire 25--250 {\\mu}eV range since $Q\\gtrsim10^4$ is expected.",
        "citation_title": "Echo-free quality factor of a multilayer axion haloscope",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the recent decades of high-energy physics research, it was demonstrated that strongly interacting quark-gluon plasma (sQGP) is created in ultra-relativistic nucleus-nucleus collisions. Investigation and understanding of the properties of the hadronic matter are among the most important goals of the NA61/SHINE collaboration at CERN SPS. Mapping of the phase diagram is achieved by varying the collision energy (5 GeV $\\sqrt{s_{NN}}<17$ GeV) and by changing the collision system ($p$+$p$, $p$+Pb, Be+Be, Ar+Sc, Xe+La, Pb+Pb). Femtoscopic correlations reveal the space-time structure of the hadron emitting source.\nIn this article, we report on the measurement of femtoscopic correlations in small to intermediate systems. Comparing the measurements to calculations based on symmetric L\u00e9vy sources, we discuss the results on L\u00e9vy source parameters as a function of average pair transverse mass. One of the physical parameters is of particular importance, the L\u00e9vy exponent $\\alpha$, which describes the shape of the source and may be related to the critical exponent $\\eta$ in the proximity of the critical point. Therefore, measuring it may shed light on the location of the critical endpoint of the QCD phase diagram.",
        "citation_title": "Femtoscopy with L\u00e9vy sources at NA61/SHINE",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the production of $X(3872)$ mesons in photon-induced nuclear reactions near the threshold within the collision model based on the nuclear spectral function. The model accounts for direct photon-nucleon $X(3872)$ production processes as well as five different scenarios for their internal structure. We calculate the absolute and relative excitation functions for $X(3872)$ production off $^{12}$C and $^{184}$W target nuclei at near-threshold incident photon energies of 8--16 GeV, the absolute differential cross sections for their production off these target nuclei at laboratory angles of 0$^{\\circ}$--10$^{\\circ}$ and for incident photon energy of 13 GeV as well as the A dependences of the relative (transparency ratios) cross sections for $X(3872)$ production from ${\\gamma}A$ collisions at photon energies around 13 GeV within the adopted scenarios for the $X(3872)$ meson internal structure. We show that the absolute and relative observables considered reveal distinct sensitivity to these scenarios. Therefore, the measurement of such observables in a dedicated experiment at the CEBAF facility in the near-threshold energy range will allow us to get valuable information on the $X(3872)$ inner structure.",
        "citation_title": "Probing the structure of $X(3872)$ in photoproduction",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Liquid argon detectors are ubiquitous in particle, astroparticle, and applied physics. They reached an unprecedented level of maturity thanks to more than 20 years of R&D and the operation of large-scale facilities at CERN, Fermilab, and the Gran Sasso laboratories. This article reviews such an impressive advance - from the grounding of the experimental technique up to cutting-edge applications. We commence the review by describing the physical and chemical properties of liquid argon as an active and target medium for particle detection, together with advantages and limitations compared with other liquefied noble gases. We examine the opportunities and challenges of liquid argon detectors operated as calorimeters, scintillators, and time projection chambers. We then delve into the core applications of liquid argon detectors at colliders (ATLAS), accelerator neutrino beams (SBN, DUNE), and underground laboratories (DarkSide, DEAP, ICARUS) for the observation of rare events. We complete the review by looking at unconventional developments (pixelization, combined light-charge readout, Xe-doped devices, all-optical readout) and applications in medical and applied physics to extend this technology's scope toward novel research fields.",
        "citation_title": "The science and technology of liquid argon detectors",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Based on the expectations that the lowest-lying double-bottom tetraquark $T_{bb\\bar u \\bar d}$ ($J^P=1^+$) and the bottom-charm tetraquark $T_{bc\\bar u \\bar d}$ ($J^P=0^+$) are stable against strong and electromagnetic decays, we work out a number of semileptonic and non-leptonic weak decays of these hadrons, making use of the heavy quark symmetry. In doing this, we concentrate on the exclusive decays involving also tetraquarks in the final states, i.e., transitions such as $T_{bb\\bar u \\bar d} \\to T_{bc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$ and $T_{bc\\bar u \\bar d} \\to T_{cc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$, where $h^-=\\pi^-,\\rho^-,a_1^-$. So far, only the $J^P=1^+$ tetraquark $T_{cc\\bar u \\bar d}$ has been discovered, which we identify with the $I=0$ $T_{cc}^{+}$ object, compatible with $J^P=1^+$ and having the mass and decay widths $\\delta m =M(T_{cc}^+) -(M(D^{*+}) - M(D^0))= -360 \\pm 40 ^{+4}_{-0}$ keV and $\\Gamma(T_{cc}^+)=48^{+2}_{-14}$ keV. Experimental discoveries of the transitions worked out here will go a long way in establishing the nature of these tetraquarks as (mainly) compact four-quark objects.",
        "citation_title": "Signature decay modes of the compact doubly-heavy tetraquarks $T_{bb\\bar{u} \\bar{d}}$ and $T_{bc\\bar{u} \\bar{d}}$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper presents a novel method for low maintenance, low ambiguity in-situ drift velocity monitoring in large volume Time Projection Chambers (TPCs). The method was developed and deployed for the 40m^3 TPC tracker system of the NA61/SHINE experiment at CERN, which has a one meter of drift length. The method relies on a low-cost multi-wire proportional chamber (MWPC) placed downstream of the TPCs to be monitored. The drift velocity is then determined by matching the reconstructed tracks in the TPC to the hits of the pertinent monitoring chamber, called Geometry Reference Chamber (GRC), which is then used as a differential length scale. An important design requirement on the GRC was minimal added complexity to the existing system, in particular, compatibility with Front-End Electronics (FEE) cards already used to read out the TPCs. Moreover, the GRC system was designed to operate both in large and small particle flux. The system is capable of monitoring the evolution of the in-situ drift velocity down to a one permil precision, with a few minutes of time sampling.",
        "citation_title": "Novel method for in-situ drift velocity measurement in large volume TPCs: the Geometry Reference Chamber of the NA61/SHINE experiment at CERN",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose to measure the weak decay constant $\\alpha_-$ for the decay $\\Lambda\\rightarrow p\\pi^-$ using a both circularly and linearly polarized photon beam with the GlueX spectrometer in Hall D. The measurement will take advantage of the fact that a measurement with both linear and circular photon beam polarization results in an over-constrained set of amplitudes which can be fitted to data and used to extract $\\alpha_-$ which will be left as a free parameter in the fit. We expect to determine $\\alpha_-$ with statistical uncertainties comparable to existing measurements and independent systematic uncertainties. This measurement can be performed alongside GlueX-II running and requires no new hardware or new beam time. The measurement requires that a sufficient fraction of the electron beam polarization be longitudinal in the Hall D tagger.",
        "citation_title": "Proposal for PAC 52: Measurement of $\u03b1_-$ for $\u039b\\rightarrow p\u03c0^-$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The reconstruction of photon conversions is importantin order to improve the reconstruction efficiency of the physics measurements involving photons. However, there are significant number of conversions in which only one of the two tracks emitted electrons is reconstructed in the detector due to very asymmetric energy sharing between the electron-positron pair. The momentum determination of the parent photon can be improved by estimating the missing energy in such conversions. In this study, we propose a simple statistical method that can be used to determine the mean value of the missing energy. By using simulated minimum bias events at LHC conditions and a toy detector simulation, the performance of the method is tested for several decay channels commonly used in particle physics analyses. A considerable improvement in the mass reconstruction precision is obtained when reconstructing particles decaying to photons whose energies are less than 20 GeV.",
        "citation_title": "A Statistical Method for Improving Momentum Measurement of Photon Conversions Reconstructed from Single Electrons",
        "date_delivered": "[Submitted on 18 Mar 2024]"
    },
    {
        "abstract": "We propose a new solution to the hierarchy (naturalness) problem, concerning quantum corrections of the Higgs mass. Assuming the Higgs boson as a system with a self-similar internal structure, we calculate its two-point function and find that the quadratic divergence is replaced by a logarithmic one in the mass corrections. It is shown that the partonic-like distribution follows the Tsallis statistics and also high energy physics experimental data for the Higgs transverse momentum distribution can be described by a self-similar statistical model.",
        "citation_title": "The Higgs boson as a self-similar system: Towards a new solution to the hierarchy problem",
        "date_delivered": "[Submitted on 21 Apr 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "There are undeniable benefits of binding Python and C++ to take advantage of the best features of both languages. This is especially relevant to the HEP and other scientific communities that have invested heavily in the C++ frameworks and are rapidly moving their data analyses to Python. Version 2 of Awkward Array, a Scikit-HEP Python library, introduces a set of header-only C++ libraries that do not depend on any application binary interface. Users can directly include these libraries in their compilation instead of linking against platform-specific libraries. This new development makes the integration of Awkward Arrays into other projects easier and more portable, as the implementation is easily separable from the rest of the Awkward Array codebase. The code is minimal; it does not include all of the code needed to use Awkward Arrays in Python, nor does it include references to Python or pybind11. The C++ users can use it to make arrays and then copy them to Python without any specialized data types - only raw buffers, strings, and integers. This C++ code also simplifies the process of just-in-time (JIT) compilation in ROOT. This implementation approach solves some of the drawbacks, like packaging projects where native dependencies can be challenging. In this paper, we demonstrate the technique to integrate C++ and Python using a header-only approach. We also describe the implementation of a new LayoutBuilder and a GrowableBuffer. Furthermore, examples of wrapping the C++ data into Awkward Arrays and exposing Awkward Arrays to C++ without copying them are discussed.",
        "citation_title": "The Awkward World of Python and C++",
        "date_delivered": "[Submitted on 3 Mar 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The AC-coupled Strip LGAD (Strip AC-LGAD) is a novel LGAD design that diminishes the density of readout electronics through the use of strip electrodes, enabling the simultaneous measurement of time and spatial information. The Institute of High Energy Physics has designed a long Strip AC-LGAD prototype with a strip electrode length of 5.7 mm and pitches of 150 $\\mu m$, 200 $\\mu m$, and 250 $\\mu m$. Spatial and timing resolutions of the long Strip AC-LGAD are studied by pico-second laser test and beta source tests. The laser test demonstrates that spatial resolution improves as the pitch size decreases, with an optimal resolution achieved at 8.3 $\\mu$m. Furthermore, the Beta source test yields a timing resolution of 37.6 ps.",
        "citation_title": "The Performance of AC-coupled Strip LGAD developed by IHEP",
        "date_delivered": "[Submitted on 8 Jul 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Kaon physics is at a turning point -- while the rare-kaon experiments NA62 and KOTO are in full swing, the end of their lifetime is approaching and the future experimental landscape needs to be defined. With HIKE, KOTO-II and LHCb-Phase-II on the table and under scrutiny, it is a very good moment in time to take stock and contemplate about the opportunities these experiments and theoretical developments provide for particle physics in the coming decade and beyond. This paper provides a compact summary of talks and discussions from the Kaons@CERN 2023 workshop.",
        "citation_title": "Workshop summary -- Kaons@CERN 2023",
        "date_delivered": "[Submitted on 6 Nov 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We investigate the role of the electromagnetic interaction in the formation and decay of the $X(3872)$. The binding properties of the $X(3872)$ are studied by assuming the molecular nature and considering the $S$-$D$ wave mixing, isospin breaking, and coupled channel effects, and in particular the correction from the electromagnetic interaction. The radiative decays can better reflect the difference between the charged and neutral $D\\bar D^*$ components, since the electromagnetic interaction explicitly breaks the isospin symmetry. We further study the radiative decay widths with the obtained wave functions for different $D\\bar D^*$ channels. We also explore other similar hidden-charm molecular states. The electromagnetic interaction can make the molecule tighter. Our result of the radiative decay width for $X(3872)\\rightarrow \\gamma J/\\psi$ is in agreement with the experiment. The branching ratio $R_{\\gamma\\psi}$ is less than 1 in our framework, which supports the Belle and BESIII measurements.",
        "citation_title": "The role of electromagnetic interaction in the $X(3872)$ and its analogs",
        "date_delivered": "[Submitted on 11 Jan 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Anisotropy scaling functions derived from comprehensive measurements of transverse momentum- and centrality-dependent anisotropy coefficients $v_2(p_T,\\text{cent})$ and $v_3(p_T,\\text{cent})$ in Pb+Pb collisions at 5.02 and 2.76 TeV, and Xe+Xe collisions at 5.44 TeV at the LHC, offer new insights into the `ultra-central flow puzzle'. These functions integrate diverse measurements into a single curve, clarifying anisotropy attenuation throughout the entire $p_T$ and centrality range. They reveal the influence of initial-state eccentricities ($\\varepsilon_{n}$), dimensionless size ($\\mathbb{R}$), radial flow, viscous correction to the thermal distribution function ($\\delta_f$), the medium's stopping power ($\\hat{q}$), and specific shear viscosity ($\\eta/s$) on the observed anisotropies. This analysis not only enhances understanding of transport coefficients but also provides crucial constraints on nuclear deformation.",
        "citation_title": "Anisotropy Scaling Functions in Heavy-Ion Collisions: Insights into the `Ultra-Central Flow Puzzle' and Constraints on Transport Coefficients and Nuclear Deformation",
        "date_delivered": "[Submitted on 14 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We systematically study the electromagnetic properties of controversial states whose internal structure is not elucidated and we try to offer a different point of view to unravel the internal structure of these states. Inspired by the $\\Omega_c$ states observed by the LHCb Collaboration, we study the electromagnetic properties of the $\\Omega_c$-like states as the compact diquark-diquark-antiquark pentaquarks with both $J^P = \\frac{1}{2}^-$ and $J^P = \\frac{3}{2}^-$ in the context of the QCD light-cone sum rule model. From the obtained numerical results, we conclude that the magnetic dipole moments of the $\\Omega_c$-like states can reflect their inner structures, which can be used to distinguish their spin-parity quantum numbers. Measuring the magnetic moment of the $\\Omega_c$-like states in future experimental facilities can be very helpful for understanding the internal organization and identifying the quantum numbers of these states.",
        "citation_title": "Revealing the nature of $\u03a9_{c}$-like states from pentaquark perspective",
        "date_delivered": "[Submitted on 29 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The magnetic moment yields an excellent framework to explore the inner structure of particles determined by the quark-gluon dynamics of QCD, as it is the leading-order response of a bound system to a weak external magnetic field. Motivated by this, in this study, the magnetic moments of possible axial-vector $T_{bc\\bar u \\bar u}$, $T_{bc\\bar d \\bar d}$, and $T_{bc\\bar u \\bar d}$ tetraquarks are obtained with the help of light-cone QCD sum rules. For this purpose, we assume that these states are represented as a diquark-antidiquark picture with different structures and interpolating currents. The magnetic moment results derived using different diquark-antidiquark configurations differ substantially from each other. This can be translated into more than one tetraquark state with the same quantum number and quark content yet possessing different magnetic moments. From the numerical results obtained, we have concluded that the magnetic moments of the $T_{bc}$ states can project their inner structure, which can be used for their quantum numbers and quark-gluon organization. The contribution of individual quarks to the magnetic moments is also analyzed for completeness. We hope that our predictions of the magnetic moments of the $T_{bc}$ tetraquarks, together with the results of other theoretical investigations of the spectroscopic parameters and decay widths of these interesting tetraquarks, may be valuable in the search for these states in future experiments and in unraveling the internal structure of these tetraquarks.",
        "citation_title": "Unveiling the underlying structure of axial-vector bottom-charm tetraquarks in the light of their magnetic moments",
        "date_delivered": "[Submitted on 24 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The mass and width of the tensor tetraquark $T=bb\\overline{c}\\overline{c}$ with spin-parity $J^{\\mathrm{P}}=2^{+}$ are calculated in the context of the QCD sum rule method. The tetraquark $T$ is modeled as a diquark-antidiquark state built of components $b^{T}C\\gamma _{\\mu }b$ and $\\overline{c}\\gamma _{\\nu }C\\overline{c}^{T}$ with $C$ being the charge conjugation matrix. The mass $m=(12795\\pm 90)~\\mathrm{MeV}$ of the exotic tensor meson $T$ is found by means of the two-point sum rule approach. Its full width $\\Gamma$ is evaluated by considering processes $T \\to B_{c}^{-}B_{c}^{-}$, $ B_{c}^{-}B_{c}^{\\ast -}$, and $B_{c}^{\\ast -}B_{c}^{\\ast -}$. Partial widths of these decays are computed by means of the three-point sum rule approach which is used to determine the strong couplings at relevant tetraquark-meson-meson vertices. Predictions obtained for the width $ \\Gamma=(55.5 \\pm 8.7)~\\mathrm{MeV}$, as well as the mass of the tetraquark $ T $ can be useful in investigations of fully heavy four-quark mesons.",
        "citation_title": "Parameters of the tensor tetraquark $bb\\overline{c}\\overline{c}$",
        "date_delivered": "[Submitted on 5 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "A global interpretation of several significant indications of scalar resonances observed at LHC is achieved using the Georgi Machacek model. Among many other consequences, one predicts large cross sections for the processes ggFH(320)->h(125)h(125) and A(151)A(151) and ggF->A(420)->H(320)Z, where H(320) has been observed in A(420)->H(320)Z->bbbbl+l- and where A(151) is observed into two photons accompanied by a b jet, a lepton or missing transverse energy. We predict that ggF->H(320) has a cross section of about 2000 fb with a BR into h(125)(125) of 17% and into A(151)A(151) of 45%. Henceforth we predict that ggF->H(320)->hh will dominate over the SM process h*->hh, where h is the SM h(125) scalar. One expects that H(320)->A(151)A(151) can be observed into bbbb, becoming one of the most significant BSM phenomena identified so far. Arguments in favor of a SUSY version of GM are provided.",
        "citation_title": "Triple Higgs couplings at LHC",
        "date_delivered": "[Submitted on 15 Apr 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Spectral densities encode non-perturbative information that enters the calculation of a plethora of physical observables in strongly coupled field theories. Phenomenological applications encompass aspects of standard-model hadronic physics, observable at current colliders, as well as correlation functions characterizing new physics proposals, testable in future experiments. By making use of numerical data produced with lattice gauge theories, we perform a systematic study to demonstrate the effectiveness of recent technological progress in the reconstruction of spectral densities. To this purpose, we write and test new software packages that use energy-smeared spectral densities to analyze the mass spectrum of mesons. We assess the effectiveness of different smearing kernels and optimize the smearing parameters to the characteristics of available lattice ensembles. For concreteness, we analyze the Sp(4) lattice gauge theory with matter transforming in an admixture of fundamental and 2-index antisymmetric representations of the gauge group. We generate new ensembles for this theory, with lattices that have a longer extent in the time direction with respect to the spatial ones. We run our tests on these ensembles, obtaining new results about the spectrum of light mesons and their excitations. We make available our algorithm and software for the extraction of spectral densities, that can be applied to theories with other gauge groups, including the theory of strong interactions (QCD) governing hadronic physics in the standard model.",
        "citation_title": "Meson spectroscopy from spectral densities in lattice gauge theories",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We compute the $\\bar{h}c$ (pseudo)scalar, (axial-)vector and (axial-)tensor susceptibilities as a function of $u=m_c/m_h$ between $u=m_c/m_b$ and $u=0.8$ using fully relativistic lattice QCD, employing nonperturbative current renormalisation and using the second generation 2+1+1 MILC HISQ gluon field configurations. We include ensembles with $a\\approx 0.09\\mathrm{fm}$, $0.06\\mathrm{fm}$, $0.045\\mathrm{fm}$ and $0.033\\mathrm{fm}$ and we are able to reach the physical $b$-quark on the two finest ensembles. At the physical $m_h=m_b$ point we find $\\overline{m}_b^2 \\chi_{1^+}={0.720(34)\\times 10^{-2}}$, $\\overline{m}_b^2 \\chi_{1^-}={1.161(54)\\times 10^{-2}}$, $\\chi_{0^-}={2.374(33)\\times 10^{-2}}$, $\\chi_{0^+}={0.609(14)\\times 10^{-2}}$. Our results for the (pseudo)scalar, vector and axial-vector are compatible with the expected small size of nonperturbative effects at $u=m_c/m_b$. We also give the first nonperturbative determination of the tensor susceptibilities, finding $\\overline{m}_b^2 \\chi_{T}={0.891(44)\\times 10^{-2}}$ and $\\overline{m}_b^2 \\chi_{AT}={0.441(33)\\times 10^{-2}}$. Our value of $\\overline{m}_b^2\\chi_{AT}$ is in good agreement with the $\\mathcal{O}(\\alpha_s)$ perturbation theory, while our result for $\\overline{m}_b^2\\chi_{T}$ is in tension with the $\\mathcal{O}(\\alpha_s)$ perturbation theory at the level of $2\\sigma$. These results will allow for dispersively bounded parameterisations to be employed using lattice inputs for the full set of $h\\to c$ semileptonic form factors in future calculations, for heavy-quark masses in the range $1.25\\times m_c \\leq m_h \\leq m_b$.",
        "citation_title": "$\\bar{b}c$ susceptibilities from fully relativistic lattice QCD",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Gauge-field configurations with non-trivial topology have profound consequences for the physics of Abelian and non-Abelian gauge theories. Over time, arguments have been gathering for the existence of gauge-field configurations with fractional topological charge, called fractons. Ground-state properties of gauge theories can drastically change in presence of fractons in the path integral. However, understanding the origin of such fractons is usually restricted to semi-classical argumentation. Here, we show that fractons persist in strongly correlated many-body systems, using the multiflavor Schwinger model of quantum electrodynamics as a paradigm example. Through detailed numerical tensor-network analysis, we find strong fracton signatures even in highly discretized lattice models, at sizes that are implementable on already existing quantum-simulation devices. Our work sheds light on how the non-trivial topology of gauge theories persists in challenging non-perturbative regimes, and it shows a path forward to probing it in table-top experiments.",
        "citation_title": "Non-perturbative signatures of fractons in the twisted multi-flavor Schwinger Model",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Based on the expectations that the lowest-lying double-bottom tetraquark $T_{bb\\bar u \\bar d}$ ($J^P=1^+$) and the bottom-charm tetraquark $T_{bc\\bar u \\bar d}$ ($J^P=0^+$) are stable against strong and electromagnetic decays, we work out a number of semileptonic and non-leptonic weak decays of these hadrons, making use of the heavy quark symmetry. In doing this, we concentrate on the exclusive decays involving also tetraquarks in the final states, i.e., transitions such as $T_{bb\\bar u \\bar d} \\to T_{bc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$ and $T_{bc\\bar u \\bar d} \\to T_{cc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$, where $h^-=\\pi^-,\\rho^-,a_1^-$. So far, only the $J^P=1^+$ tetraquark $T_{cc\\bar u \\bar d}$ has been discovered, which we identify with the $I=0$ $T_{cc}^{+}$ object, compatible with $J^P=1^+$ and having the mass and decay widths $\\delta m =M(T_{cc}^+) -(M(D^{*+}) - M(D^0))= -360 \\pm 40 ^{+4}_{-0}$ keV and $\\Gamma(T_{cc}^+)=48^{+2}_{-14}$ keV. Experimental discoveries of the transitions worked out here will go a long way in establishing the nature of these tetraquarks as (mainly) compact four-quark objects.",
        "citation_title": "Signature decay modes of the compact doubly-heavy tetraquarks $T_{bb\\bar{u} \\bar{d}}$ and $T_{bc\\bar{u} \\bar{d}}$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We perform a nonperturbative lattice study of the electroweak phase transition in the real singlet scalar extension of the Standard Model.We consider both the heavy and light singlet-like scalar regimes at non-zero singlet-doublet mixing angle. After reviewing features of the lattice method relevant for phase transition studies, we analyze the dependence of phase transition thermodynamics on phenomenologically relevant parameters. In the heavy singlet-like scalar regime, we find that the transition is crossover for small doublet-singlet mixing angles, despite the presence of an energy barrier in the tree-level potential. The transition becomes first order for sufficiently large mixing angles. We find two-loop perturbation theory to agree closely with the lattice results for all thermodynamical quantities considered here (critical temperature, order parameter discontinuity, latent heat) when the transition is strongly first order. For the light singlet-like scalar regime relevant to exotic Higgs decays, we update previous one-loop perturbative results using the two-loop loop dimensionally reduced effective field theory and assess the nature of the transition with lattice simulations at set of benchmark parameter points. For fixed singlet-like scalar mass the transition becomes crossover when the magnitude of the Higgs-singlet portal coupling is small. We perform our simulations in the high-temperature effective theory, which we briefly review, and present analytic expressions for the relevant lattice-continuum relations.",
        "citation_title": "Nonperturbative study of the electroweak phase transition in the real scalar singlet extended Standard Model",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We report a lattice QCD study of the heavy-light meson-meson interactions with an explicitly exotic flavor content $bc\\bar u\\bar d$, isospin $I\\!=\\!0$, and axialvector $J^P=1^+$ quantum numbers in search of possible tetraquark bound states. The calculation is performed at four values of lattice spacing, ranging $\\sim$0.058 to $\\sim$0.12 fm, and at five different values of valence light quark mass $m_{u/d}$, corresponding to pseudoscalar meson mass $M_{ps}$ of about 0.5, 0.6, 0.7, 1.0, and 3.0 GeV. The energy eigenvalues in the finite-volume are determined through a variational procedure applied to correlation matrices built out of two-meson interpolating operators as well as diquark-antidiquark operators. The continuum limit estimates for $D\\bar B^*$ elastic $S$-wave scattering amplitude are extracted from the lowest finite-volume eigenenergies, corresponding to the ground states, using amplitude parametrizations supplemented by a lattice spacing dependence. Light quark mass $m_{u/d}$ dependence of the $D\\bar B^*$ scattering length ($a_0$) suggests that at the physical pion mass $a_0^{phys} = +0.57(^{+4}_{-5})(17)$ fm, which clearly points to an attractive interaction between the $D$ and $\\bar B^*$ mesons that is strong enough to host a real bound state $T_{bc}$, with a binding energy of $-43(_{-7}^{+6})(_{-24}^{+14})$ MeV with respect to the $D\\bar B^*$ threshold. We also find that the strength of the binding decreases with increasing $m_{u/d}$ and the system becomes unbound at a critical light quark mass $m^{*}_{u/d}$ corresponding to $M^{*}_{ps} = 2.73(21)(19)$ GeV.",
        "citation_title": "Bound isoscalar axial-vector $bc\\bar u\\bar d$ tetraquark $T_{bc}$ from lattice QCD using two-meson and diquark-antidiquark variational basis",
        "date_delivered": "[Submitted on 26 Jul 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We introduce a Hamiltonian lattice model for the $(1+1)$-dimensional $\\text{SU}(N_c)$ gauge theory coupled to one adjoint Majorana fermion of mass $m$. The discretization of the continuum theory uses staggered Majorana fermions. We analyze the symmetries of the lattice model and find lattice analogs of the anomalies of the corresponding continuum theory. An important role is played by the lattice translation by one lattice site, which in the continuum limit involves a discrete axial transformation. On a lattice with periodic boundary conditions, the Hilbert space breaks up into sectors labeled by the $N_c$-ality $p=0, \\ldots N_c-1$. Our symmetry analysis implies various exact degeneracies in the spectrum of the lattice model. In particular, it shows that, for $m=0$ and even $N_c$, the sectors $p$ and $p'$ are degenerate if $|p-p'| = N_c/2$. In the $N_c = 2$ case, we explicitly construct the action of the Hamiltonian on a basis of gauge-invariant states, and we perform both a strong coupling expansion and exact diagonalization for lattices of up to $12$ lattice sites. Upon extrapolation of these results, we find good agreement with the spectrum computed previously using discretized light-cone quantization. One of our new results is the first numerical calculation of the fermion bilinear condensate.",
        "citation_title": "Lattice Hamiltonian for Adjoint QCD$_2$",
        "date_delivered": "[Submitted on 15 Nov 2023 (v1), last revised 30 Apr 2024 (this version, v2)]"
    },
    {
        "abstract": "We study the R\u00e9nyi entanglement entropy (EE) of the two-dimensional $J$-$Q$ model, the emblematic quantum spin model of deconfined criticality at the phase transition between antiferromagnetic and valence-bond-solid ground states. State-of-the-art quantum Monte Carlo calculations of the EE reveal critical corner contributions that scale logarithmically with the system size, with a coefficient in remarkable agreement with the form expected from a large-$N$ conformal field theory with SO($N=5$) symmetry. However, details of the bipartition of the lattice are crucial in order to observe this behavior. If the subsystem for the reduced density matrix does not properly accommodate valence-bond fluctuations, logarithmic contributions appear even for corner-less bipartitions. We here use a $45^\\circ$ tilted cut on the square lattice. Beyond supporting an SO($5$) deconfined quantum critical point, our results for both the regular and tilted cuts demonstrate important microscopic aspects of the EE that are not captured by conformal field theory.",
        "citation_title": "Entanglement entropy and deconfined criticality: emergent SO(5) symmetry and proper lattice bipartition",
        "date_delivered": "[Submitted on 25 Jan 2024 (v1), last revised 30 Apr 2024 (this version, v2)]"
    },
    {
        "abstract": "Kaon physics is at a turning point -- while the rare-kaon experiments NA62 and KOTO are in full swing, the end of their lifetime is approaching and the future experimental landscape needs to be defined. With HIKE, KOTO-II and LHCb-Phase-II on the table and under scrutiny, it is a very good moment in time to take stock and contemplate about the opportunities these experiments and theoretical developments provide for particle physics in the coming decade and beyond. This paper provides a compact summary of talks and discussions from the Kaons@CERN 2023 workshop.",
        "citation_title": "Workshop summary -- Kaons@CERN 2023",
        "date_delivered": "[Submitted on 6 Nov 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We study the SYK model -- an important toy model for quantum gravity on IBM's superconducting qubit quantum computers. By using a graph-coloring algorithm to minimize the number of commuting clusters of terms in the qubitized Hamiltonian, we find the gate complexity of the time evolution using the first-order product formula for $N$ Majorana fermions is $\\mathcal{O}(N^5 J^{2}t^2/\\epsilon)$ where $J$ is the dimensionful coupling parameter, $t$ is the evolution time, and $\\epsilon$ is the desired precision. With this improved resource requirement, we perform the time evolution for $N=6, 8$ with maximum two-qubit circuit depth of 343. We perform different error mitigation schemes on the noisy hardware results and find good agreement with the exact diagonalization results on classical computers and noiseless simulators. In particular, we compute return probability after time $t$ and out-of-time order correlators (OTOC) which is a standard observable of quantifying the chaotic nature of quantum systems.",
        "citation_title": "Sachdev-Ye-Kitaev model on a noisy quantum computer",
        "date_delivered": "[Submitted on 29 Nov 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We systematically study the electromagnetic properties of controversial states whose internal structure is not elucidated and we try to offer a different point of view to unravel the internal structure of these states. Inspired by the $\\Omega_c$ states observed by the LHCb Collaboration, we study the electromagnetic properties of the $\\Omega_c$-like states as the compact diquark-diquark-antiquark pentaquarks with both $J^P = \\frac{1}{2}^-$ and $J^P = \\frac{3}{2}^-$ in the context of the QCD light-cone sum rule model. From the obtained numerical results, we conclude that the magnetic dipole moments of the $\\Omega_c$-like states can reflect their inner structures, which can be used to distinguish their spin-parity quantum numbers. Measuring the magnetic moment of the $\\Omega_c$-like states in future experimental facilities can be very helpful for understanding the internal organization and identifying the quantum numbers of these states.",
        "citation_title": "Revealing the nature of $\u03a9_{c}$-like states from pentaquark perspective",
        "date_delivered": "[Submitted on 29 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The mass and width of the tensor tetraquark $T=bb\\overline{c}\\overline{c}$ with spin-parity $J^{\\mathrm{P}}=2^{+}$ are calculated in the context of the QCD sum rule method. The tetraquark $T$ is modeled as a diquark-antidiquark state built of components $b^{T}C\\gamma _{\\mu }b$ and $\\overline{c}\\gamma _{\\nu }C\\overline{c}^{T}$ with $C$ being the charge conjugation matrix. The mass $m=(12795\\pm 90)~\\mathrm{MeV}$ of the exotic tensor meson $T$ is found by means of the two-point sum rule approach. Its full width $\\Gamma$ is evaluated by considering processes $T \\to B_{c}^{-}B_{c}^{-}$, $ B_{c}^{-}B_{c}^{\\ast -}$, and $B_{c}^{\\ast -}B_{c}^{\\ast -}$. Partial widths of these decays are computed by means of the three-point sum rule approach which is used to determine the strong couplings at relevant tetraquark-meson-meson vertices. Predictions obtained for the width $ \\Gamma=(55.5 \\pm 8.7)~\\mathrm{MeV}$, as well as the mass of the tetraquark $ T $ can be useful in investigations of fully heavy four-quark mesons.",
        "citation_title": "Parameters of the tensor tetraquark $bb\\overline{c}\\overline{c}$",
        "date_delivered": "[Submitted on 5 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We investigate the possibility of using the Short-Baseline Near Detector (SBND) at Fermilab to constrain lepton flavor violating decays of pions and kaons. We study how to leverage SBND-PRISM, the use of the neutrino beam angular spread to mitigate systematic uncertainties, to enhance this analysis. We show that SBND-PRISM can put stringent limits on the flavor violating branching ratios $\\rm{BR}(\\pi^+ \\to \\mu^+ \\nu_e) = 8.9 \\times 10^{-4}$, $\\rm{BR}(K^+ \\to \\mu^+ \\nu_e) = 3.2 \\times 10^{-3}$, improving previous constraints by factors 9 and 1.25, respectively. We also estimate the SBND-PRISM sensitivity to lepton number violating decays, $\\rm{BR}(\\pi^+ \\to \\mu^+ \\overline{\\nu}_e)= 2.1 \\times 10^{-3}$ and $\\rm{BR}(K^+ \\to \\mu^+ \\overline{\\nu}_e) = 7.4 \\times 10^{-3}$, though not reaching previous BEBC limits. Last, we identify several ways how the SBND collaboration could improve this analysis.",
        "citation_title": "Could SBND-PRISM probe Lepton Flavor Violation?",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "If the cosmological dark matter (DM) couples to Standard Model (SM) fields, it can decay promptly to SM states in a highly energetic hard process, which subsequently showers and hadronizes to give stable particles including $e^\\pm$, $\\gamma$, $p^{\\pm}$ and $\\nu\\bar{\\nu}$ at lower energy. If the DM particle is very heavy, the high-energy $e^\\pm$, due to the Klein-Nishina cross section suppression, preferentially lose energy via synchrotron emission which, in turn, can be of unusually high energies. Here, we present novel bounds on heavy decaying DM up to the Planck scale, by studying the synchrotron emission from the $e^\\pm$ produced in the ambient Galactic magnetic field. In particular, we explore the sensitivity of the resulting constraints on the DM decay width to (i) different SM decay channels, to (ii) the Galactic magnetic field configurations, and (iii) to various different DM density profiles proposed in the literature. We find that constraints from the synchrotron component complement and improve on constraints from very high-energy cosmic-ray and gamma-ray observatories targeting the prompt emission when the DM is sufficiently massive, most significantly for masses in excess of $10^{12}\\text{ GeV}$.",
        "citation_title": "Astrophysical constraints from synchrotron emission on very massive decaying dark matter",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The production of single and paired lepton bound states in the decay of the Higgs boson has been studied. We explore different decay mechanisms that contribute significantly to the decay width. The decay widths are calculated taking into account relativistic corrections in the decay amplitude and in the wave function of the bound state of leptons.",
        "citation_title": "Production of dileptonic bound states in the Higgs boson decay",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Modular flavor symmetries refers to scenarios in which fermion masses respect modular symmetries. Such scenarios have been studied in the bottom-up approach and have an explicit realization in string theory. They rely on the remarkable properties of vector-valued modular forms.",
        "citation_title": "Aspects of Modular Flavor Symmetries",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "At present, the Standard Model (SM) agrees with almost all collider data. Yet, three finetuning issues -- the Higgs mass problem, the strong CP problem and the cosmological constant problem -- all call for new physics. The most plausible solutions at present are weak scale SUSY, the PQWW axion and the string landscape. A re-evaluation of EW finetuning in SUSY allows for a higgsino-like LSP and naturalness upper bounds well beyond LHC limits. Rather general arguments from string theory allow for statistical predictions that m_h~ 125 GeV with sparticles beyond present LHC limits. The most lucrative LHC search channel may be for light higgsino pair production. Dark matter turns out to be a SUSY DFSZ axion along with a diminished abundance of higgsino-like WIMPs.",
        "citation_title": "Beyond the Standard Model: An overview",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "By considering the one loop background field method for a quark-antiquark interaction mediated by one (non perturbative) gluon exchange, sixth order quark effective interactions are derived and investigated in the limit of zero momentum transfer. They extend fourth order quark interactions worked out in previous works of the author. These interactions break $U_A(1)$ symmetry and may be either momentum independent or dependent. Part of these $U_A(1)$ breaking interactions vanish in the limit of massless quarks and several other - involving vector and/or axial quark currents - survive even in the absence of Dynamical Symmetry Breaking. By means of the auxiliary field method, these interactions give rise to three meson interactions whose values are compared to phenomenological values found in the literature. By restricting to the up and down quark sector, some three-meson couplings correspond to the strong decay of specific light axial-vector mesons. These (strong) decays are shown to yield an asymmetry in the production rate of positive and negative pions by means of interference effects that arise due to neutral meson mixings.",
        "citation_title": "$U_A(1)$ symmetry breaking quark interactions from vacuum polarization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study the scenario of inflection point inflation where a flat direction of the minimal supersymmetric standard model (MSSM) is identified with the inflaton. Specifically, we consider in full generality the cases where a MSSM flat direction is lifted by a higher-dimensional superpotential whose dimension is n = 4, 5, 6, 7, 9. We confront the inflection point inflation scenarios with various n with the Planck and BICEP data, and thereby constrain the soft SUSY breaking mass and the coefficient of the higher-dimensional operator that lifts the flat direction.",
        "citation_title": "Confronting MSSM flat direction inflation with Planck/BICEP data",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "There is strong evidence for the existence of dark matter in a number of current experiments. We study dark matter using the $U(1)_X$SSM obtained from the $U(1)_X$ extension of the minimal supersymmetric standard model (MSSM). In the $U(1)_X$SSM, we use the right-handed neutrino as a dark matter candidate, whose lightest mass eigenstate has cold dark matter features. In this paper, the relic density of right-handed neutrino as dark matter is investigated. For dark matter scattering, both spin-independent and spin-dependent cross sections are studied. In the final numerical results obtained, some parameter spaces can satisfy the constraints of the relic density and dark matter direct detection experiments.",
        "citation_title": "Right-handed neutrino dark matter in $U(1)_X$SSM",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The nonlinear massive plane wave solution of the classical scalar field in the Higgs potential is revisited to study the mass generation and particle creation. In particular, by assuming that the Higgs system is in the slightly excited state in early universe and it is described by the nonlinear solution, we study the mass generation mechanism for massive vector bosons and a heavy fermion in the quantum field theory around the nonlinear massive classical field. The nonlinear massive classical solution gives the transition from the vacuum to a pair of vector bosons and fermions. We present the new formulae of the probability density of the production process for particles in the standard model of elementary particle physics. The probability densities of the particle productions vanish when the nonlinear massive solution reduces to the constant solution (the classical vacuum expectation value); while the probability densities are expressed as the function of the free parameter in the classical solution in general case. We discuss the behavior of the probability densities for the three oscillating modes in the classical solution.",
        "citation_title": "Mass generation via nonlinear massive solution in Higgs potential and particle creations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Based on analyses of the mass and the strong decay features, $D_{s0}(2590)^+$ observed by LHCb collaboration is identified as a radial excitation of the pseudoscalar $D_s$, and $D_{sJ}(3040)^+$ observed by BaBar collaboration is identified as a radial excitation of $D_{s1}(2536)^\\pm$. $D_{s0}(2590)^+$ is possibly a pure $D_{s}(2~^1S_0)$ meson, both basic $D_{s1}(2536)^\\pm$ and radially excited $D_{sJ}(3040)^+$ are possibly the mixtures $D_s(nP_1)$ between spin triplet $D_s(n~^3P_1)$ and spin singlet $D_s(n~^1P_1)$. In this arrangement, their masses meet the linear behavior of the radial Regge trajectory very well. In the $^3P_0$ strong decay model, the decay channels of $D_{s0}(2590)^+$ are $D^{*0}K^+$ and $D^{*+}K^0$, the total decay width is predicted with $\\Gamma=76.12$ MeV. The main decay channels of $D_{sJ}(3040)^+$ are $D^{*0}K^+$/$D^{*+}K^0$ and $D^{*0}K^{*+}$/$D^{*+}K^{*0}$, the total decay width is predicted with $\\Gamma=283.46$ MeV. These numerical strong decay results are consistent with the experiment data and support our arrangement. The dimensionless strength creation parameter $\\gamma$ plays an important role in the calculation, and $\\gamma=9.57$ is fixed through a comparison of the predicted strong decay widths of $D^*_{s2}(2573)$ and $D^*_{s3}(2860)^{\\pm}$ with experimental data.",
        "citation_title": "Assignment of charmed-strange $D_{s0}(2590)^+$ and $D_{sJ}(3040)^+$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study in detail the influence of different chemical potentials (baryon, charged, strange, and neutrino) on how and how fast a free gas of quarks in the zero-temperature limit reaches the conformal limit. We discuss the influence of non-zero masses, the inclusion of leptons, and different constraints, such as charge neutrality, zero-net strangeness, and fixed lepton fraction. We also investigate for the first time how the symmetry energy of the system under some of these conditions approaches the conformal limit. Finally, we briefly discuss what kind of corrections are expected from perturbative QCD as one goes away from the conformal limit.",
        "citation_title": "Approaching the conformal limit of quark matter with different chemical potentials",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the production of $X(3872)$ mesons in photon-induced nuclear reactions near the threshold within the collision model based on the nuclear spectral function. The model accounts for direct photon-nucleon $X(3872)$ production processes as well as five different scenarios for their internal structure. We calculate the absolute and relative excitation functions for $X(3872)$ production off $^{12}$C and $^{184}$W target nuclei at near-threshold incident photon energies of 8--16 GeV, the absolute differential cross sections for their production off these target nuclei at laboratory angles of 0$^{\\circ}$--10$^{\\circ}$ and for incident photon energy of 13 GeV as well as the A dependences of the relative (transparency ratios) cross sections for $X(3872)$ production from ${\\gamma}A$ collisions at photon energies around 13 GeV within the adopted scenarios for the $X(3872)$ meson internal structure. We show that the absolute and relative observables considered reveal distinct sensitivity to these scenarios. Therefore, the measurement of such observables in a dedicated experiment at the CEBAF facility in the near-threshold energy range will allow us to get valuable information on the $X(3872)$ inner structure.",
        "citation_title": "Probing the structure of $X(3872)$ in photoproduction",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study LHC searches for an extension of the Standard Model (SM) by exploiting an additional Abelian $U_D(1)$ gauge symmetry and a complex scalar Higgs portal. As the scalar is charged under this gauge symmetry, a vector dark matter (VDM) candidate can satisfy the observed relic abundance and limits from direct dark matter (DM) searches. The ATLAS and CMS experiments have developed a broad search program for the DM candidates, including associate production of Higgs boson, Z boson and top quark which couple to DM. In this paper, we perform an extensive analysis to constrain the model by using these experiments at LHC. It can be seen that the LHC results can exclude some parts of the parameter space which are still allowed by relic density and the direct detection searches. Furthermore, exclusion limits on the parameter space of the model by using the new results of CMS and ATLAS collaborations for new light Higgs boson with mass $\\sim95~\\rm GeV$ are provided.",
        "citation_title": "The Vector Dark Matter, LHC Constraints Including a 95 GeV Light Higgs Boson",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Understanding the properties and physical phase of the dense strongly interacting matter present in the cores of neutron stars or created in their binary mergers remains one of the most prominent open problems in nuclear astrophysics. While most microscopic analyses have historically relied on solvable phenomenological models of nuclear and quark matter, in recent years a model-independent approach utilizing only controlled ab-initio calculations and astrophysical observations has emerged as a viable alternative.\nIn these lecture notes, I review recent progress in first-principles weak-coupling calculations within high-density quark matter, shedding light on its thermodynamic and transport properties. I cover the most important technical tools used in such calculations, introduce selected highlight results, and explain how this information can be used in phenomenological studies of neutron-star physics. The notes do not offer a self-consistent treatment of the topics covered, but rather aim at filling gaps in existing textbooks on thermal field theory and at connecting the dots in a story developed in several recent research articles, to which the interested reader is directed for further technical details.",
        "citation_title": "Particle-theory input for neutron-star physics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Twist-3 collinear parton distribution functions (PDFs) are matrix elements of quark-gluon-quark or three-gluons light-cone operators. They depend on three momentum fraction variables, which are restricted to a hexagon region, and the evolution kernels are defined via two-dimensional convolution in these variables. We present the numerical realisation of the twist-3 evolution equations at leading order in the strong coupling for all kinds of twist-3 PDF (quark, gluon, chiral-even/odd, etc). We provide two independent codes (in C and Fortran) that have been extensively cross-checked, and are ready-to-use. We supplement the paper with a review of known properties of twist-3 PDFs.",
        "citation_title": "Numerical implementation of evolution equations for twist-3 collinear PDFs",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Based on the expectations that the lowest-lying double-bottom tetraquark $T_{bb\\bar u \\bar d}$ ($J^P=1^+$) and the bottom-charm tetraquark $T_{bc\\bar u \\bar d}$ ($J^P=0^+$) are stable against strong and electromagnetic decays, we work out a number of semileptonic and non-leptonic weak decays of these hadrons, making use of the heavy quark symmetry. In doing this, we concentrate on the exclusive decays involving also tetraquarks in the final states, i.e., transitions such as $T_{bb\\bar u \\bar d} \\to T_{bc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$ and $T_{bc\\bar u \\bar d} \\to T_{cc\\bar u \\bar d} (\\ell^- \\nu_\\ell, h^-)$, where $h^-=\\pi^-,\\rho^-,a_1^-$. So far, only the $J^P=1^+$ tetraquark $T_{cc\\bar u \\bar d}$ has been discovered, which we identify with the $I=0$ $T_{cc}^{+}$ object, compatible with $J^P=1^+$ and having the mass and decay widths $\\delta m =M(T_{cc}^+) -(M(D^{*+}) - M(D^0))= -360 \\pm 40 ^{+4}_{-0}$ keV and $\\Gamma(T_{cc}^+)=48^{+2}_{-14}$ keV. Experimental discoveries of the transitions worked out here will go a long way in establishing the nature of these tetraquarks as (mainly) compact four-quark objects.",
        "citation_title": "Signature decay modes of the compact doubly-heavy tetraquarks $T_{bb\\bar{u} \\bar{d}}$ and $T_{bc\\bar{u} \\bar{d}}$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We perform a nonperturbative lattice study of the electroweak phase transition in the real singlet scalar extension of the Standard Model.We consider both the heavy and light singlet-like scalar regimes at non-zero singlet-doublet mixing angle. After reviewing features of the lattice method relevant for phase transition studies, we analyze the dependence of phase transition thermodynamics on phenomenologically relevant parameters. In the heavy singlet-like scalar regime, we find that the transition is crossover for small doublet-singlet mixing angles, despite the presence of an energy barrier in the tree-level potential. The transition becomes first order for sufficiently large mixing angles. We find two-loop perturbation theory to agree closely with the lattice results for all thermodynamical quantities considered here (critical temperature, order parameter discontinuity, latent heat) when the transition is strongly first order. For the light singlet-like scalar regime relevant to exotic Higgs decays, we update previous one-loop perturbative results using the two-loop loop dimensionally reduced effective field theory and assess the nature of the transition with lattice simulations at set of benchmark parameter points. For fixed singlet-like scalar mass the transition becomes crossover when the magnitude of the Higgs-singlet portal coupling is small. We perform our simulations in the high-temperature effective theory, which we briefly review, and present analytic expressions for the relevant lattice-continuum relations.",
        "citation_title": "Nonperturbative study of the electroweak phase transition in the real scalar singlet extended Standard Model",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Nontopological fermionic solitons exist across a diverse range of particle physics models and have rich cosmological implications. This study establishes a general framework for calculating fermionic soliton profiles under arbitrary scalar potentials, utilizing relativistic mean field theory to accurately depict the interaction between the fermion condensate and the background scalar field. Within this framework, the conventional fermion bound states are revealed as a subset of fermionic solitons. In addition, we demonstrate how the analytical formulae in previous studies are derived as special cases of our algorithm, discussing the validity of such approximations. Furthermore, we explore the phenomenology of fermionic solitons, highlighting new formation mechanisms and evolution paths, and reconsidering the possibility of collapse into primordial black holes.",
        "citation_title": "Revisiting the fermion-field nontopological solitons",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We develop a method to calculate helicity amplitudes of an arbitrary tree-level process in Feynman-Diagram (FD) gauge for an arbitrary gauge model with MadGraph5_aMC@NLO. We start from the 't Hooft-Feynman gauge Lagrangian in FeynRules and generate scattering amplitudes by identifying the Goldstone boson as the $5^{\\rm th}$ component of each weak boson. All the vertices of the 5-component weak bosons are then created automatically by assembling the relevant weak boson and Goldstone boson vertices in the Feynman gauge. The 5-component weak boson vertices are then connected by the $5\\times5$ matrix propagator in the FD gauge. As a demonstration of the method we calculate the cross section for the process $\\mu^-\\mu^+\\to\\nu_\\mu\\bar{\\nu}_\\mu t\\bar{t}H$ with complex top Yukawa coupling, which can be obtained by adding a gauge invariant dimension-6 operator to the Standard Model (SM) Lagrangian. The FD gauge and the unitary (U) gauge amplitudes give exactly the same cross section, and subtle gauge theory cancellation among diagrams in the U gauge at high energies is absent in the FD gauge, as has been observed for various SM processes. In addition, we find that the total cross sections at high energies are dominated by a single, or a set of non-vanishing Feynman amplitudes with the higher dimensional vertices in the FD gauge.",
        "citation_title": "Automatic generation of helicity amplitudes in Feynman-Diagram gauge",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Comparisons of higher-order predictions within the Standard Model of Particle Physics (SM) to data are central to high-energy collider experiments like the Large Hadron Collider (LHC). Processes with multiple kinematic scales, such as multi-jet and prompt photon production, provide a unique possibility for probing Quantum Chromodynamics (QCD). These processes directly test perturbative QCD and can be used to extract fundamental parameters like the strong coupling constant and to search for BSM physics. Recent developments enabled lifting three-jet, photon plus two-jet, photon-pair plus jet, and three-photon cross-sections to QCD's next-to-next-to-leading order (NNLO). This contribution presents phenomenological results at NNLO QCD for three-jet and photon plus two-jet production.",
        "citation_title": "High-precision prediction for multi-scale processes at the LHC",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the paper Int.J.Mod.Phys.E 23 (2014) 1450004, the potential of short-baseline experiments was proposed to measure the mass (and parameters of Lorentz-violating effects) of the muon neutrino, where a roughly estimated upper bound of 420 eV was given as a possibility with large unknown uncertainties. In the present work, we improve upon this study by focusing on a feasible and improved experimental setup with today's technology, eliminating most large uncertainties, with the use of the Geant4 simulation toolkit. High-energy protons collide with a tungsten target, producing a variety of particles, most importantly pions that decay into muon neutrinos. The detector records the time of flight for both muon and anti-muon neutrinos, utilizing light as a reference signal. Additionally, it captures the energy deposited by neutrinos. By applying the dispersion relation, we determine the muon and/or anti-muon neutrino mass. Our improved results reveal a less optimistic but more accurate and realistic estimated upper bound of the muon neutrino mass, providing a new limit of about 150 keV. Notably, this finding is a factor of three lower than the best upper bound previously established in the literature originating from pion decay in flight.",
        "citation_title": "New upper bound of muon neutrino mass in a short-baseline experiment",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "During the past few decades, abundant evidence for physics beyond the two standard models of particle physics and cosmology was found. Yet, we are tapping into the dark regarding our understanding of the dark sector. For more than a century, open problems related to the nature of the vacuum remain unresolved. Besides the traditional high-energy frontier and cosmology, technological advancement provides complementary access to new physics via high-precision experiments. Among the latter, the Casimir And Non-Newtonian force EXperiment (\\cannex{}) has successfully completed its proof-of-principle phase and will soon commence operation. Benefiting from its plane parallel plate geometry, both interfacial and gravity-like forces are maximized, leading to increased sensitivity. A wide range of dark sector forces, Casimir forces in and out of thermal equilibrium, and gravity will be tested. This article describes the final experimental design, its sensitivity, and expected results.",
        "citation_title": "Force metrology with plane parallel plates: Final design review and outlook",
        "date_delivered": "[Submitted on 16 Mar 2024]"
    },
    {
        "abstract": "We propose a simple fit function, $L_{\\nu_i}(t) = C\\, t^{-\\alpha}\\, e^{-(t/\\tau)^{n}}$, to parametrize the luminosities of neutrinos and antineutrinos of all flavors during the protoneutron star (PNS) cooling phase at post-bounce times $t \\gtrsim 1$ s. This fit is based on results from a set of neutrino-hydrodynamics simulations of core-collapse supernovae in spherical symmetry. The simulations were performed with an energy-dependent transport for six neutrino species and took into account the effects of convection and muons in the dense and hot PNS interior. We provide values of the fit parameters $C$, $\\alpha$, $\\tau$, and $n$ for different neutron star masses and equations of state as well as correlations between these fit parameters. Our functional description is useful for analytic supernova modeling, for characterizing the neutrino light curves in large underground neutrino detectors, and as a tool to extract information from measured signals on the mass and equation of state of the PNS and on secondary signal components on top of the PNS's neutrino emission.",
        "citation_title": "Simple fits for the neutrino luminosities from protoneutron star cooling",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The baryon acoustic oscillation (BAO) analysis from the first year of data from the Dark Energy Spectroscopic Instrument (DESI), when combined with data from the cosmic microwave background (CMB), has placed an upper-limit on the sum of neutrino masses, $\\sum m_\\nu < 70$ meV (95%). In addition to excluding the minimum sum associated with the inverted hierarchy, the posterior is peaked at $\\sum m_\\nu = 0$ and is close to excluding even the minumum sum, 58 meV at 2$\\sigma$. In this paper, we explore the implications of this data for cosmology and particle physics. The sum of neutrino mass is determined in cosmology from the suppression of clustering in the late universe. Allowing the clustering to be enhanced, we extended the DESI analysis to $\\sum m_\\nu < 0$ and find $\\sum m_\\nu = - 160 \\pm 90$ meV (68%), and that the suppression of power from the minimum sum of neutrino masses is excluded at 99% confidence. We show this preference for negative masses makes it challenging to explain the result by a shift of cosmic parameters, such as the optical depth or matter density. We then show how a result of $\\sum m_\\nu =0$ could arise from new physics in the neutrino sector, including decay, cooling, and/or time-dependent masses. These models are consistent with current observations but imply new physics that is accessible in a wide range of experiments. In addition, we discuss how an apparent signal with $\\sum m_\\nu < 0$ can arise from new long range forces in the dark sector or from a primordial trispectrum that resembles the signal of CMB lensing.",
        "citation_title": "No $\u03bd$s is Good News",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We report a methodology to determine the quality factor ($Q$) in implementations of the so-called dielectric haloscope, a new concept of wavy dark matter detector equipped with a multilayered resonator. An anechoic chamber enables the observation of the resonance frequency and its amplitude for an unlimited series of layers for the first time, which is conveniently filtered. The frequency-normalized power enhancement measured in a Dark-photons \\& Axion-Like particles Interferometer (DALI) prototype is a few hundred per layer over a sweep bandwidth of half a hundred MHz. In light of this result, this scaled-down prototype is sensitive to axions saturating the local dark matter density with a coupling to photons between $g_{a\\gamma\\gamma}\\gtrsim10^{-12}$ GeV$^{-1}$ and $g_{a\\gamma\\gamma}\\gtrsim$ few $\\times 10^{-14}$ GeV$^{-1}$ at frequencies of several dozens of GHz once cooled down to the different working temperatures of the experiment and immersed in magnetic fields ranging from 1 T to 10 T; while the sensitivity of the full-scale DALI is projected at $g_{a\\gamma\\gamma}\\gtrsim\\mathrm{few}\\times10^{-15}$ GeV$^{-1}$ over the entire 25--250 {\\mu}eV range since $Q\\gtrsim10^4$ is expected.",
        "citation_title": "Echo-free quality factor of a multilayer axion haloscope",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "An algorithm for providing analytical solutions to Schr\u00f6dinger's equation with non-exactly solvable potentials is elaborated. It represents a symbiosis between the logarithmic expansion method and the techniques of the superymmetric quantum mechanics as extended toward non shape invariant potentials. The complete solution to a given Hamiltonian $H_{0}$ is obtained from the nodeless states of the Hamiltonian $H_{0}$ and of a set of supersymmetric partners $H_{1}, H_{2},..., H_{r}$. The nodeless states (dubbed \"edge\" states) are unique and in general can be ground or excited states. They are solved using the logarithmic expansion which yields an infinite systems of coupled first order hierarchical differential equations, converted later into algebraic equations with recurrence relations which can be solved order by order. We formulate the aforementioned scheme, termed to as \"Supersymmetric Expansion Algorithm'' step by step and apply it to obtain for the first time the complete analytical solutions of the three dimensional Hulth\u00e9n--, and the one-dimensional anharmonic oscillator potentials.",
        "citation_title": "Supersymmetric Expansion Algorithm and complete analytical solution for the Hulth\u00e9n and anharmonic potentials",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Spectral densities encode non-perturbative information that enters the calculation of a plethora of physical observables in strongly coupled field theories. Phenomenological applications encompass aspects of standard-model hadronic physics, observable at current colliders, as well as correlation functions characterizing new physics proposals, testable in future experiments. By making use of numerical data produced with lattice gauge theories, we perform a systematic study to demonstrate the effectiveness of recent technological progress in the reconstruction of spectral densities. To this purpose, we write and test new software packages that use energy-smeared spectral densities to analyze the mass spectrum of mesons. We assess the effectiveness of different smearing kernels and optimize the smearing parameters to the characteristics of available lattice ensembles. For concreteness, we analyze the Sp(4) lattice gauge theory with matter transforming in an admixture of fundamental and 2-index antisymmetric representations of the gauge group. We generate new ensembles for this theory, with lattices that have a longer extent in the time direction with respect to the spatial ones. We run our tests on these ensembles, obtaining new results about the spectrum of light mesons and their excitations. We make available our algorithm and software for the extraction of spectral densities, that can be applied to theories with other gauge groups, including the theory of strong interactions (QCD) governing hadronic physics in the standard model.",
        "citation_title": "Meson spectroscopy from spectral densities in lattice gauge theories",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Cosmological emulators of observables such as the Cosmic Microwave Background (CMB) spectra and matter power spectra commonly use training data sampled from a Latin hypercube. This method often incurs high computational costs by covering less relevant parts of the parameter space, especially in high dimensions where only a small fraction of the parameter space yields a significant likelihood.\nIn this paper, we introduce hypersphere sampling, which instead concentrates sample points in regions with higher likelihoods, significantly enhancing the efficiency and accuracy of emulators. A novel algorithm for sampling within a high-dimensional hyperellipsoid aligned with axes of correlation in the cosmological parameters is presented. This method focuses the distribution of training data points on areas of the parameter space that are most relevant to the models being tested, thereby avoiding the computational redundancies common in Latin hypercube approaches.\nComparative analysis using the \\textsc{connect} emulation tool demonstrates that hypersphere sampling can achieve similar or improved emulation precision with more than an order of magnitude fewer data points and thus less computational effort than traditional methods. This was tested for both the $\\Lambda$CDM model and a 5-parameter extension including Early Dark Energy, massive neutrinos, and additional ultra-relativistic degrees of freedom. Our results suggest that hypersphere sampling holds potential as a more efficient approach for cosmological emulation, particularly suitable for complex, high-dimensional models.",
        "citation_title": "Cutting corners: Hypersphere sampling as a new standard for cosmological emulators",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we systematically investigate the inflationary complexity of the two-mode squeezed state with thermal effect for the single field inflation, modified dispersion relation, and non-trivial sound speed with the method of closed system and open system, respectively, which our analysis is valid for most inflationary models. First, the numeric of Krylov complexity in the method of the closed system indicates that the evolution of Krylov complexity highly depends on the squeezed angle parameter once taking the thermal effect into account, which will decay into some very tiny values, but the Krylov complexity will always enhance without thermal effect. For comparison, the numeric of circuit complexity shows that the evolution is always increasing no matter whether there are thermal effects or not which is independent of the evolution of squeezed angle parameter. By utilizing the method of open system, we first construct the wave function. As for the Krylov complexity with the method of open system, our investigations show the evolution of Krylov complexity will enhance upon some peaks factoring in the thermal effects. For completeness, we also calculate the Krylov entropy in the method of closed system and open system, which indicates that the hotter universe, the more chaotic the universe. Furthermore, our derivation for the Krylov complexity and Krylov entropy could nicely recover into the case of closed system under weak dissipative approximation, which confirms the validity of construction for the wave function. Finally, our numeric of Lanczos coefficient shows that the non-trivial sound speed has minimal chaos compared to the other two cases.",
        "citation_title": "Inflationary complexity of thermal state",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Neutron stars represent unique laboratories, offering insights into the physics of supranuclear-density matter and serving as potential hosts for dark matter. This study explores the impact of dark matter cores on rapidly rotating neutron stars through the two-fluid approximation, assuming minimal interaction between baryonic matter and dark matter. The investigation employs phenomenological models for fermionic and bosonic dark matter, revealing that universal relations governing mass and radius changes due to rotation remain largely unaffected in the presence of a dark matter core. Specifically, for a 5 % dark matter mass fraction, the percent deviations in total mass ($M_{tot}$), the baryonic equatorial radius ($R_{Be}$), and polar-to-equatorial baryonic radius ratio ($R_{ratioB}$) are within 3.9 %, 1.8 %, and 1.4 %, respectively. These findings suggest that the universal relations governing neutron star shape can be utilized to infer constraints on the properties of dark matter cores even in cases where the dark matter significantly softens the neutron star's equation of state.",
        "citation_title": "The Effect of a Dark Matter Core on the Structure of a Rotating Neutron Star",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We get the dual gravity metric of the rotating nuclear matter by performing a standard Lorentz transformation on the static metric in the D-instanton background. Then, we study the effects of the angular velocity, the instanton density and the temperature on the heavy quark potential. It is shown that the angular velocity and the temperature promote dissociation of the quark-antiquark pair, and the instanton density suppresses dissociation. Similarly, according to the result of the jet quenching parameter, we found that the jet quenching parameter increases with the increase of angular velocity, instanton density and temperature, and the jet quenching parameter in the rotating D-instanton background is larger than that of N = 4 SYM theory.",
        "citation_title": "Heavy quark potential and jet quenching parameter in a rotating D-instanton background",
        "date_delivered": "[Submitted on 2 Feb 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We propose a new solution to the hierarchy (naturalness) problem, concerning quantum corrections of the Higgs mass. Assuming the Higgs boson as a system with a self-similar internal structure, we calculate its two-point function and find that the quadratic divergence is replaced by a logarithmic one in the mass corrections. It is shown that the partonic-like distribution follows the Tsallis statistics and also high energy physics experimental data for the Higgs transverse momentum distribution can be described by a self-similar statistical model.",
        "citation_title": "The Higgs boson as a self-similar system: Towards a new solution to the hierarchy problem",
        "date_delivered": "[Submitted on 21 Apr 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "A novel analysis is performed, incorporating time-of-flight (TOF) information to study the interactions of dark matter (DM) with standard model particles. After supernova (SN) explosions, DM with mass $m_\\chi\\lesssim\\mathcal{O}({\\rm MeV})$ in the halo can be boosted by SN neutrinos (SN$\\nu$) to relativistic speed. The SN$\\nu$ boosted DM (BDM) arrives on Earth with TOF which depends only on $m_\\chi$ and is independent of the cross section. These BDMs can interact with detector targets in low-background experiments and manifest as afterglow events after the arrival of SN$\\nu$. The characteristic TOF spectra of the BDM events can lead to large background suppression and unique determination of $m_\\chi$. New cross section constraints on $\\sqrt{\\sigma_{\\chi e} \\sigma_{\\chi\\nu}}$ are derived from SN1987a in the Large Magellanic Cloud with data from the Kamiokande and Super-Kamiokande experiments. Potential sensitivities for the next galactic SN with Hyper-Kamiokande are projected. This analysis extends the existing bounds on $\\sqrt{\\sigma_{\\chi e}\\sigma_{\\chi \\nu}}$ over a broad range of $r_\\chi=\\sigma_{\\chi \\nu}/\\sigma_{\\chi e}$. In particular, the improvement is by 1-3 orders of magnitude for $m_\\chi<\\mathcal{O}(100\\,{\\rm keV})$ for $\\sigma_{\\chi e}\\sim\\sigma_{\\chi \\nu}$. Prospects of exploiting TOF information in other astrophysical systems to probe exotic physics with other DM candidates are discussed.",
        "citation_title": "Searching for Afterglow: Light Dark Matter Boosted by Supernova Neutrinos",
        "date_delivered": "[Submitted on 14 Jun 2022 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We compute the complete set of two-loop master integrals for the scattering of four massless particles and a massive one. Our results are ready for phenomenological applications, removing a major obstacle to the computation of complete next-to-next-to-leading order (NNLO) QCD corrections to processes such as the production of a $H/Z/W$ boson in association with two jets at the LHC. Furthermore, they open the door to new investigations into the structure of quantum-field theories and provide precious analytic data for studying the mathematical properties of Feynman integrals.",
        "citation_title": "All Two-Loop Feynman Integrals for Five-Point One-Mass Scattering",
        "date_delivered": "[Submitted on 27 Jun 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In the hypersphere soliton model (HSM), we study the geometrical inner structures and the ensuing charge distributions of the nucleons by exploiting the aspect of the HSM where the hypersphere soliton is described by an extended object possessing the parameter $\\lambda$ $(0\\le\\lambda<\\infty)$ which corresponds to the radial distance from the center of $S^{3}$ to the foliation leaves of the hypersphere soliton. To do this, we investigate the foliation and topology related with geometry on a hypersphere described by $(\\mu,\\theta,\\phi)$. Exploiting the so-called scanning algorithm we study geometrical relations between spherical shell foliation leave on a northern hemi-hypersphere $S^{3}_{+}$ and that on a flat equatorial solid sphere $E^{3}$ which contains the center of $S^{3}$. We then elucidate the physical meaning of $\\mu$ in $S^{3}$ of radius $\\lambda$ by showing that $\\mu$ plays the role of an auxiliary angle to fix the radius $\\lambda\\sin\\mu$ of the $S^{2}$ spherical shell sharing the center of $S^{3}(=S^{2}\\times S^{1})$, at a given angle $\\mu$. Next, using the charge density profiles of nucleons with $\\mu$ dependence, we construct the nucleon fractional charges of spherically symmetric and nontrivial distributions. In the HSM we note that the proton and neutron charges do not leak out from the hypersphere soliton, and the positive and negative charges in the neutron are confined inside and outside its core, respectively. Explicitly we predict the fractional volumes and charges of the neutron. The proton and neutron are shown to be described by a topological structure of two Hopf-linked M\u00f6bius strip type twist circles in $S^{3}$. We also note that the characteristic ratio of the hypersphere volume to the corresponding solid sphere one is given by a geometrical invariant related with hyper-compactness.",
        "citation_title": "Foliation, topology and nucleon charge profiles in hypersphere soliton model",
        "date_delivered": "[Submitted on 26 Jun 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Recent observations have granted to us two unique insights into the early universe: the presence of a low-frequency stochastic gravitational wave background detected by the NANOGrav and Pulsar Timing Array (PTA) experiments and the emergence of unusually massive galaxy candidates at high redshifts reported by the James Webb Space Telescope (JWST). In this letter, we consider the possibility that both observations have a common origin, namely primordial black holes (PBHs) in the mass range between $10^{6}~M_{\\odot}$ and $10^{13}~M_{\\odot}$. While superheavy PBHs act as seeds for accelerated galaxy formation capable of explaining the JWST extreme galaxies, they can also form binary mergers that source gravitational waves which can be potentially identified as the PTA signal. The analysis is performed taking into account the constraints on the relevant region of the PBH parameter space including the novel bound imposed by the Ultraviolet Luminosity Function of galaxies observed by the Hubble Space Telescope. We conclude that PTA's and JWST's interpretations in terms of PBH binary mergers and Poissonian gas of PBHs, respectively, are strongly excluded.",
        "citation_title": "Scrutinizing the Primordial Black Holes Interpretation of PTA Gravitational Waves and JWST Early Galaxies",
        "date_delivered": "[Submitted on 4 Jul 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Supernova neutrino boosted dark matter (SN$\\nu$ BDM) and its afterglow effect have been shown to be a promising signature for beyond Standard Model (bSM) physics. The time-evolution feature of SN$\\nu$ BDM allows for %the possibly direct inference of DM mass $m_\\chi$, and results in significant background suppression with improving sensitivity. This paper extends the earlier study and provides a general framework for computing the SN$\\nu$ BDM fluxes for a supernova that occurs at any location in our galaxy. A bSM $U(1)_{L_\\mu-L_\\tau}$ model with its gauge boson coupling to both DM and the second and third generation of leptons is considered, which allows for both DM-$\\nu$ and DM-$e$ interactions. Detailed analysis of the temporal profile, angular distribution, and energy spectrum of the SN$\\nu$ BDM are performed. Unique signatures in SN$\\nu$ BDM allowing extraction of $m_\\chi$ and detail features that contain information of the underlying interaction type are discussed. Expected sensitivities on the above new physics model from Super-Kamiokande, Hyper-Kamiokande, and DUNE detections of BDM events induced by the next galactic SN are derived and compared with the existing bounds.",
        "citation_title": "Signatures of afterglows from light dark matter boosted by supernova neutrinos in current and future large underground detectors",
        "date_delivered": "[Submitted on 7 Jul 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Gravitational Waves (GWs) have been detected in the $\\sim$100 Hz and nHz bands, but most of the gravitational spectrum remains unobserved. A variety of detector concepts have been proposed to expand the range of observable frequencies. In this work, we study the capability of GW detectors in the ``mid-band'', the $\\sim$30 mHz -- 10 Hz range between LISA and LIGO, to measure the signals from and constrain the properties of ${\\sim}$1 -- 100 $M_\\odot$ compact binaries. We focus on atom-interferometer-based detectors. We describe a Fisher matrix code, AIMforGW, which we created to evaluate their capabilities, and present numerical results for two benchmarks: terrestrial km-scale detectors, and satellite-borne detectors in medium Earth orbit. Mid-band GW detectors are particularly well-suited to pinpointing the location of GW sources on the sky. We demonstrate that a satellite-borne detector could achieve sub-degree sky localization for any detectable source with chirp mass $\\mathcal{M}_c \\lesssim 50 M_\\odot$. We also compare different detector configurations, including different locations of terrestrial detectors and various choices of the orbit of a satellite-borne detector. As we show, a network of only two terrestrial single-baseline detectors or one single-baseline satellite-borne detector would each provide close-to-uniform sky-coverage, with signal-to-noise ratios varying by less than a factor of two across the entire sky. We hope that this work contributes to the efforts of the GW community to assess the merits of different detector proposals.",
        "citation_title": "Gravitational Wave Measurement in the Mid-Band with Atom Interferometers",
        "date_delivered": "[Submitted on 14 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Kaon physics is at a turning point -- while the rare-kaon experiments NA62 and KOTO are in full swing, the end of their lifetime is approaching and the future experimental landscape needs to be defined. With HIKE, KOTO-II and LHCb-Phase-II on the table and under scrutiny, it is a very good moment in time to take stock and contemplate about the opportunities these experiments and theoretical developments provide for particle physics in the coming decade and beyond. This paper provides a compact summary of talks and discussions from the Kaons@CERN 2023 workshop.",
        "citation_title": "Workshop summary -- Kaons@CERN 2023",
        "date_delivered": "[Submitted on 6 Nov 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The strongly-coupled system like the quark-hadron transition (if it is of first order) is becoming an active play-yard for the physics of cosmological first-order phase transitions. However, the traditional field theoretic approach to strongly-coupled first-order phase transitions is of great challenge, driving recent efforts from holographic dual theories with explicit numerical simulations. These holographic numerical simulations have revealed an intriguing linear correlation between the phase pressure difference (pressure difference away from the wall) to the non-relativistic terminal velocity of an expanding planar wall, which has been reproduced analytically alongside both cylindrical and spherical walls from perfect-fluid hydrodynamics in our previous study but only for a bag equation of state. We have also found in our previous study a universal quadratic correlation between the wall pressure difference (pressure difference near the bubble wall) to the non-relativistic terminal wall velocity regardless of wall geometries. In this paper, we will generalize these analytic relations between the phase/wall pressure difference and terminal wall velocity into a more realistic equation of state beyond the simple bag model, providing the most general predictions so far for future tests from holographic numerical simulations of strongly-coupled first-order phase transitions",
        "citation_title": "General bubble expansion at strong coupling",
        "date_delivered": "[Submitted on 13 Nov 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Axion-like particles are predicted in many physics scenarios beyond the Standard Model (SM). Their interactions with SM particles may arise from the triangle anomaly of the associated global symmetry, along with other SM global and gauge symmetries, including anomalies with the global baryon number and electromagnetic gauge symmetries. We initiate the phenomenological study of the corresponding ``electrobaryonic axion\", a particle that couples with both the baryon chemical potential and the electromagnetic field. Neutron stars, particularly magnetars, possessing high baryon density and strong magnetic fields, can naturally develop a thin axion hair around their surface. In this study, we calculate this phenomenon, considering the effects of neutron star rotation and general relativity. For axion particles lighter than the neutron star rotation frequency, the anomalous interaction can also induce the emission of axion particles from the neutron star. This emission, in the light axion regime, can have a significant contribution to the neutron star cooling rate.",
        "citation_title": "Electrobaryonic axion: hair of neutron stars",
        "date_delivered": "[Submitted on 30 Nov 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Metastable cosmic strings appear in models of new physics with a two-step symmetry breaking $G\\to H\\to 1$, where $\\pi_1(H)\\neq 0$ and $\\pi_1(G)=0$. They decay via the monopole-antimonopole pair creation inside. Conventionally, the breaking rate has been estimated by an infinitely thin string approximation, which requires a large hierarchy between the symmetry breaking scales. In this paper, we reexamine it by taking into account the finite sizes of both the cosmic string and the monopole. We obtain a robust lower limit on the tunneling factor $e^{-S_B}$ even for regimes the conventional estimate is unreliable. In particular, it is relevant to the cosmic string interpretation of the gravitational wave signals recently reported by pulsar timing array experiments.",
        "citation_title": "Revisiting Metastable Cosmic String Breaking",
        "date_delivered": "[Submitted on 25 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Recent years have seen the emergence of a new understanding of scattering amplitudes in the simplest theory of colored scalar particles - the Tr$(\\phi^3)$ theory - based on combinatorial and geometric ideas in the kinematic space of scattering data. In this paper we report a surprise: far from the toy model it appears to be, the ''stringy'' Tr$(\\phi^3)$ amplitudes secretly contain the scattering amplitudes for pions, as well as non-supersymmetric gluons, in any number of dimensions. The amplitudes for the different theories are given by one and the same function, related by a simple shift of the kinematics. This discovery was spurred by another fundamental observation: the tree-level Tr$(\\phi^3)$ field theory amplitudes have a hidden pattern of zeros when a special set of non-planar Mandelstam invariants is set to zero. Furthermore, near these zeros, the amplitudes simplify, by factoring into a non-trivial product of smaller amplitudes. Remarkably the amplitudes for pions and gluons are observed to also vanish in the same kinematical locus. These properties further generalize to the ''stringy'' Tr$(\\phi^3)$ amplitudes. There is a unique shift of the kinematic data that preserves the zeros, and this shift is precisely the one that unifies colored scalars, pions, and gluons into a single object. We will focus in this paper on explaining the hidden zeros and factorization properties and the connection between all the colored theories, working for simplicity at tree-level. Subsequent works will describe this new formulation for the Non-linear Sigma Model and non-supersymmetric Yang-Mills theory, at all loop orders.",
        "citation_title": "Hidden zeros for particle/string amplitudes and the unity of colored scalars, pions and gluons",
        "date_delivered": "[Submitted on 26 Dec 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We develop a numerical approach to compute polar parity perturbations within fully relativistic models of black hole systems embedded in generic, spherically symmetric, anisotropic fluids. We apply this framework to study gravitational wave generation and propagation from extreme mass-ratio inspirals in the presence of several astrophysically relevant dark matter models, namely the Hernquist, Navarro-Frenk-White, and Einasto profiles. We also study dark matter spike profiles obtained from a fully relativistic calculation of the adiabatic growth of a BH within the Hernquist profile, and provide a closed-form analytic fit of these profiles. Our analysis completes prior numerical work in the axial sector, yielding a fully numerical pipeline to study black hole environmental effects. We study the dependence of the fluxes on the DM halo mass and compactness. We find that, unlike the axial case, polar fluxes are not adequately described by simple gravitational-redshift effects, thus offering an exciting avenue for the study of black hole environments with gravitational waves.",
        "citation_title": "Black holes surrounded by generic matter distributions: polar perturbations and energy flux",
        "date_delivered": "[Submitted on 1 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Thermal dark matter, which, after chemical decoupling from the Standard Model thermal plasma rises its yield before it freezes-out, is a new feature of thermal DM scenarios dubbed as \"Bouncing Dark Matter\". In the following short note, we introduce the topic, exemplifying the mechanism with a simple model that extends the SM with two dark matter particles, a fermion, and a pNGB, plus a second Higgs. We explore the features of the thermal freeze-out of this scenario, with particular emphasis on the exponential growth of the yield of the pNGB. We test the model under collider bounds, relic abundance, direct detection, and we study prospects for indirect detection observables.",
        "citation_title": "An example of a bouncing dark matter -- EPS-HEP2023",
        "date_delivered": "[Submitted on 26 Dec 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We investigate the role of the electromagnetic interaction in the formation and decay of the $X(3872)$. The binding properties of the $X(3872)$ are studied by assuming the molecular nature and considering the $S$-$D$ wave mixing, isospin breaking, and coupled channel effects, and in particular the correction from the electromagnetic interaction. The radiative decays can better reflect the difference between the charged and neutral $D\\bar D^*$ components, since the electromagnetic interaction explicitly breaks the isospin symmetry. We further study the radiative decay widths with the obtained wave functions for different $D\\bar D^*$ channels. We also explore other similar hidden-charm molecular states. The electromagnetic interaction can make the molecule tighter. Our result of the radiative decay width for $X(3872)\\rightarrow \\gamma J/\\psi$ is in agreement with the experiment. The branching ratio $R_{\\gamma\\psi}$ is less than 1 in our framework, which supports the Belle and BESIII measurements.",
        "citation_title": "The role of electromagnetic interaction in the $X(3872)$ and its analogs",
        "date_delivered": "[Submitted on 11 Jan 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We systematically study the electromagnetic properties of controversial states whose internal structure is not elucidated and we try to offer a different point of view to unravel the internal structure of these states. Inspired by the $\\Omega_c$ states observed by the LHCb Collaboration, we study the electromagnetic properties of the $\\Omega_c$-like states as the compact diquark-diquark-antiquark pentaquarks with both $J^P = \\frac{1}{2}^-$ and $J^P = \\frac{3}{2}^-$ in the context of the QCD light-cone sum rule model. From the obtained numerical results, we conclude that the magnetic dipole moments of the $\\Omega_c$-like states can reflect their inner structures, which can be used to distinguish their spin-parity quantum numbers. Measuring the magnetic moment of the $\\Omega_c$-like states in future experimental facilities can be very helpful for understanding the internal organization and identifying the quantum numbers of these states.",
        "citation_title": "Revealing the nature of $\u03a9_{c}$-like states from pentaquark perspective",
        "date_delivered": "[Submitted on 29 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We study the supersymmetric Q balls which decay at present and find that they create a distinctive spectrum of gamma rays at around O(10) MeV. The charge of the Q ball is lepton numbers in order for the lifetime to be as long as the present age of the universe, and the main decay products are light leptons. However, as the charge of the Q ball decreases, the decay channel into pions becomes kinematically allowed towards the end of the decay, and the pions are produced at rest. Immediately, $\\pi^0$ decays into two photons with the energy of 67.5 MeV, half the pion mass, which exhibits a unique emission line. In addition, $\\pi^\\pm$ decay into $\\mu^\\pm$, which further decay with emitting internal bremsstrahlung, whose spectrum has a sharp cutoff at $\\sim$50 MeV. If the observations would find these peculiar features of the gamma-ray spectrum in the future, it could be a smoking gun of the supersymmetric Q-ball decay at present.",
        "citation_title": "MeV gamma rays from Q-ball decay",
        "date_delivered": "[Submitted on 4 Mar 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Hidden-heavy hadrons can decay into pairs of heavy hadrons through transitions from confining Born-Oppenheimer potentials to hadron-pair potentials with the same Born-Oppenheimer quantum numbers. The transitions are also constrained by conservation of angular momentum and parity. From these constraints, we derive model-independent selection rules for decays of hidden-heavy hadrons into pairs of heavy hadrons. The coupling potentials are expressed as sums of products of Born-Oppenheimer transition amplitudes and angular-momentum coefficients. If there is a single dominant Born-Oppenheimer transition amplitude, it factors out of the coupling potentials between hidden-heavy hadrons in the same Born-Oppenheimer multiplet and pairs of heavy hadrons in specific heavy-quark-spin-symmetry doublets. If furthermore the kinetic energies of the heavy hadrons are much larger than their spin splittings, we obtain analytic expressions for the relative partial decay rates in terms of Wigner 6-j and 9-j symbols. We consider in detail the decays of quarkonia and quarkonium hybrids into the lightest heavy-meson pairs. For quarkonia, our model-independent selection rules and relative partial decay rates agree with previous results from quark-pair-creation models in simple cases and give stronger results in other cases. For quarkonium hybrids, we find disagreement even in simple cases.",
        "citation_title": "Model-independent predictions for decays of hidden-heavy hadrons into pairs of heavy hadrons",
        "date_delivered": "[Submitted on 19 Mar 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The magnetic moment yields an excellent framework to explore the inner structure of particles determined by the quark-gluon dynamics of QCD, as it is the leading-order response of a bound system to a weak external magnetic field. Motivated by this, in this study, the magnetic moments of possible axial-vector $T_{bc\\bar u \\bar u}$, $T_{bc\\bar d \\bar d}$, and $T_{bc\\bar u \\bar d}$ tetraquarks are obtained with the help of light-cone QCD sum rules. For this purpose, we assume that these states are represented as a diquark-antidiquark picture with different structures and interpolating currents. The magnetic moment results derived using different diquark-antidiquark configurations differ substantially from each other. This can be translated into more than one tetraquark state with the same quantum number and quark content yet possessing different magnetic moments. From the numerical results obtained, we have concluded that the magnetic moments of the $T_{bc}$ states can project their inner structure, which can be used for their quantum numbers and quark-gluon organization. The contribution of individual quarks to the magnetic moments is also analyzed for completeness. We hope that our predictions of the magnetic moments of the $T_{bc}$ tetraquarks, together with the results of other theoretical investigations of the spectroscopic parameters and decay widths of these interesting tetraquarks, may be valuable in the search for these states in future experiments and in unraveling the internal structure of these tetraquarks.",
        "citation_title": "Unveiling the underlying structure of axial-vector bottom-charm tetraquarks in the light of their magnetic moments",
        "date_delivered": "[Submitted on 24 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We investigate whether it is possible within the NMSSM to describe simultaneously a correct dark matter relic density complying with the latest null results from the LZ experiment, an extra Higgs boson with a mass of ~95 GeV visible in the bb channel at LEP and the diphoton channel at the LHC, and the deviation of the anomalous magnetic moment of the muon from its value within the Standard Model. We find that this is still possible for a variety of dark matter annihilation mechanisms, for singlino-like but also bino-like lightest supersymmetric particles. We show the signal rates of the extra Higgs boson and the dark matter detection rates as function of the dark matter mass and annihilation mechanism. The dark matter direct detection cross section may well fall below the neutrino floor. The masses of the lightest electroweakly interacting supersymmetric particles are typically not far above 100 GeV, but not excluded due to unconventional decays.",
        "citation_title": "NMSSM with correct relic density and an additional 95 GeV Higgs boson",
        "date_delivered": "[Submitted on 25 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The mass and width of the tensor tetraquark $T=bb\\overline{c}\\overline{c}$ with spin-parity $J^{\\mathrm{P}}=2^{+}$ are calculated in the context of the QCD sum rule method. The tetraquark $T$ is modeled as a diquark-antidiquark state built of components $b^{T}C\\gamma _{\\mu }b$ and $\\overline{c}\\gamma _{\\nu }C\\overline{c}^{T}$ with $C$ being the charge conjugation matrix. The mass $m=(12795\\pm 90)~\\mathrm{MeV}$ of the exotic tensor meson $T$ is found by means of the two-point sum rule approach. Its full width $\\Gamma$ is evaluated by considering processes $T \\to B_{c}^{-}B_{c}^{-}$, $ B_{c}^{-}B_{c}^{\\ast -}$, and $B_{c}^{\\ast -}B_{c}^{\\ast -}$. Partial widths of these decays are computed by means of the three-point sum rule approach which is used to determine the strong couplings at relevant tetraquark-meson-meson vertices. Predictions obtained for the width $ \\Gamma=(55.5 \\pm 8.7)~\\mathrm{MeV}$, as well as the mass of the tetraquark $ T $ can be useful in investigations of fully heavy four-quark mesons.",
        "citation_title": "Parameters of the tensor tetraquark $bb\\overline{c}\\overline{c}$",
        "date_delivered": "[Submitted on 5 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "A global interpretation of several significant indications of scalar resonances observed at LHC is achieved using the Georgi Machacek model. Among many other consequences, one predicts large cross sections for the processes ggFH(320)->h(125)h(125) and A(151)A(151) and ggF->A(420)->H(320)Z, where H(320) has been observed in A(420)->H(320)Z->bbbbl+l- and where A(151) is observed into two photons accompanied by a b jet, a lepton or missing transverse energy. We predict that ggF->H(320) has a cross section of about 2000 fb with a BR into h(125)(125) of 17% and into A(151)A(151) of 45%. Henceforth we predict that ggF->H(320)->hh will dominate over the SM process h*->hh, where h is the SM h(125) scalar. One expects that H(320)->A(151)A(151) can be observed into bbbb, becoming one of the most significant BSM phenomena identified so far. Arguments in favor of a SUSY version of GM are provided.",
        "citation_title": "Triple Higgs couplings at LHC",
        "date_delivered": "[Submitted on 15 Apr 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The Event Horizon Telescope (EHT) has revolutionized our ability to study black holes by providing unprecedented spatial resolution and unveiling horizon-scale details. With advancements leading to the next-generation EHT, there is potential to probe even deeper into the black hole's dark region, especially the inner shadow characterized by low-intensity foreground emissions from the jet, thanks to a significant enhancement in dynamic range by two orders of magnitude. We demonstrate how such enhanced observations could transform supermassive black holes into powerful probes for detecting annihilating dark matter, which can form a dense profile in the vicinity of supermassive black holes, by examining the morphology of the black hole image.",
        "citation_title": "Illuminating Black Hole Shadow with Dark Matter Annihilation",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this paper, we investigated two models incorporating LIV with nonminimal coupling in the ( e^+e^- \\rightarrow \\mu^+\\mu^- ) scattering process, employing cross-section calculations. We found that utilizing the dual electromagnetic tensor led to violations of unitarity in both vector and axial scenarios. However, in the other model the preservation of unitarity by the electromagnetic tensor in both scenarios implies its potential superiority, this implies promising prospects for this CPT and Lorentz symmetry-breaking model.",
        "citation_title": "The effects of Lorentz violation on unitarity in $e^{+}e^{-} \\rightarrow \u03bc^{+}\u03bc^{-}$",
        "date_delivered": "[Submitted on 28 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Parisi-Sourlas (PS) supersymmetry is known to emerge in some models with random field type of disorder. When PS SUSY is present the $d$-dimensional theory allows for a $d-2$-dimensional description. In this paper we investigate the reversed question and we provide new indications that any given CFT$_{d-2}$ can be uplifted to a PS SUSY CFT$_{d}$. We show that any scalar four-point function of a CFT$_{d-2}$ is mapped to a set of 43 four-point functions of the uplifted CFT$_{d}$ which are related to each other by SUSY and satisfy all necessary bootstrap axioms. As a byproduct we find 43 non trivial relations between conformal blocks across dimensions.\nWe test the uplift in generalized free field theory (GFF) and find that PS SUSY is a powerful tool to bootstrap an infinite class of previously unknown GFF observables. Some of this power is shown to persist in perturbation theory around GFF.\nWe explain why all diagonal minimal models admit an uplift and we show exact results for correlators and CFT data of the $4d$ uplift of the Ising model. Despite being strongly coupled $4d$ CFTs, the uplifted minimal models contain infinitely many conserved currents and are expected to be integrable.",
        "citation_title": "The Parisi-Sourlas Uplift and Infinitely Many Solvable 4d CFTs",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Chiral higher-spin gravity is a higher-spin extension of both self-dual Yang-Mills and self-dual gravity and is a unique local higher-spin gravity in four dimensions. Its existence implies that there are two closed subsectors in Chern-Simons matter theories. We make first steps in identifying these (anti-)chiral subsectors directly on the CFT side, which should result in a holographically dual pair where both sides are nontrivial, complete, yet exactly soluble. We also discuss closely related theories: self-dual Yang-Mills (SDYM) and self-dual gravity (SDGR) in the holographic context.",
        "citation_title": "Hidden sectors of Chern-Simons Matter theories and Exact Holography",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Finsler geometry is a natural generalization of (pseudo-)Riemannian geometry, where the line element is not the square root of a quadratic form but a more general homogeneous function. Parameterizing this in terms of symmetric tensors suggests a possible interpretation in terms of higher-spin fields. We will see here that, at linear level in these fields, the Finsler version of the Ricci tensor leads to the curved-space Fronsdal equation for all spins, plus a Stueckelberg-like coupling. Nonlinear terms can also be systematically analyzed, suggesting a possible interacting structure. No particular choice of spacetime dimension is needed. The Stueckelberg mechanism breaks gauge transformations to a redundancy that does not change the geometry. This is however not enough to eliminate non-transverse modes, at least for some versions of Finsler dynamics.",
        "citation_title": "Higher spins and Finsler geometry",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Recently Danielson, Satishchandran, and Wald (DSW) have shown that quantum superpositions held outside of Killing horizons will decohere at a steady rate. This occurs because of the inevitable radiation of soft photons (gravitons), which imprint a electromagnetic (gravitational) ``which-path'' memory onto the horizon. Rather than appealing to this global description, an experimenter ought to also have a local description for the cause of decoherence. One might intuitively guess that this is just the bombardment of Hawking/Unruh radiation on the system, however simple calculations challenge this idea -- the same superposition held in a finite temperature inertial laboratory does not decohere at the DSW rate. In this work we provide a local description of the decoherence by mapping the DSW set-up onto a worldline-localized model resembling an Unruh-DeWitt particle detector. We present an interpretation in terms of random local forces which do not sufficiently self-average over long times. Using the Rindler horizon as a concrete example we clarify the crucial role of temperature, and show that the Unruh effect is the only quantum mechanical effect underlying these random forces. A general lesson is that for an environment which induces Ohmic friction on the central system (as one gets from the classical Abraham-Lorentz-Dirac force, in an accelerating frame) the fluctuation-dissipation theorem implies that when this environment is at finite temperature it will cause steady decoherence on the central system. Our results agree with DSW and provide the complementary local perspective.",
        "citation_title": "Decoherence by warm horizons",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We investigate the 3-sphere partition functions of various 3d $\\mathcal{N}=2$ holographic SCFTs arising from the $N$ stack of M2-branes in the 't Hooft limit both analytically and numerically. We first employ a saddle point approximation to evaluate the free energy $F=-\\log Z$ at the planar level, tracking the first subleading corrections in the large 't Hooft coupling $\\lambda$ expansion. Subsequently, we improve these results by determining the planar free energy to all orders in the large $\\lambda$ expansion via numerical analysis. Remarkably, the resulting planar free energies turn out to take a universal form, supporting a prediction that these $S^3$ partition functions are all given in terms of an Airy function even beyond the special cases where the Airy formulae were derived analytically in the literature; in this context we also present new Airy conjectures in several examples. The subleading behaviors we captured encode a part of quantum corrections to the M-theory path integrals around dual asymptotically Euclidean AdS$_4$ backgrounds with the corresponding internal manifolds through holographic duality.",
        "citation_title": "Subleading analysis for $S^3$ partition functions of $\\mathcal{N}=2$ holographic SCFTs",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We derive the generalized second law (GSL) for arbitrary cuts of Killing horizons from the perspective of crossed-product gravitational algebras, making use of a recent proposal by one of us for the construction of local gravitational algebras. This construction relies on the existence of a state whose modular flow is geometric on the horizon. In both free and interacting quantum field theories, such states are guaranteed to exist by the properties of half-sided translations on the horizon. Using geometric identities derived from the canonical analysis of general relativity on null surfaces, we show that the crossed product entropy agrees with the generalized entropy of the horizon cut in a semiclassical limit, and further reproduce Wall's result relating the GSL to monotonicity of relative entropy of the quantum field algebras. We also give a novel generalization of the GSL for interacting theories in asymptotically flat spacetimes involving the concept of an algebra at infinity for a half-sided translation, which accounts for triviality of the algebra of fields smeared only on the horizon. Going beyond the semiclassical limit, we compute subleading corrections to the crossed product entropy, but are unable to determine if the GSL continues to hold after accounting for these. We speculate that an improved GSL could follow from a hidden subalgebra structure of the crossed products, assuming the existence of an operator-valued weight between horizon cut algebras.",
        "citation_title": "Gravitational algebras and the generalized second law",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study the notion of a metric over the space of AdS solution in string theory, leading to an associated distance between them. Such a distance is the idea underlying the AdS distance conjecture. We utilise the previously developed prescription for extracting such a metric: taking an off-shell quadratic variation of the string theory effective action and then evaluating it over the space of on-shell solutions. It was shown that this prescription leads to a well-defined positive metric over M-theory Freud-Rubin vacua. In this work, we use the same prescription to calculate the metric over type IIA DGKT vacua. These are much more involved, they have multiple flux parameters and exhibit scale separation. While it remains an open question whether these vacua exist as fully localised solutions of string theory, they are well-defined within the four-dimensional effective theory, which is all that is required for the calculation. We find that they also have a positive metric over them. Interestingly, this metric turns out to be independent of the many flux parameters in the solution, similarly to what happens for metrics over scalar field spaces. This non-trivial flux cancellation, as well as results from explicit vacua, lead us to propose a Swampland condition: that the metric over the space of vacua in quantum gravity, as defined by the above prescription, is always positive.",
        "citation_title": "A positive metric over DGKT vacua",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Using an exact holographic duality formula between the inhomogeneous 2d Ising model and 3d quantum gravity, we provide a formula for \"real\" zeroes of the 2d Ising partition function on finite graphs in terms of the geometry of a 2d triangulation embedded in the three-dimensional Euclidean space. The complex phase of those zeroes is given by the dihedral angles of the triangulation, which reflect its extrinsic curvature within the ambient 3d space, while the modulus is given by the angles within the 2d triangles, thus encoding the intrinsic geometry of the triangulation. Our formula can not cover the whole set of Ising zeroes, but we conjecture that a suitable complexification of these \"real\" zeroes would provide a more thorough formula. Nevertheless, in the thermodynamic limit, in the case of flat planar 2d triangulations, our Ising zeroes formula gives the critical couplings for isoradial graphs, confirming its generality. This approach shows an intricate, but precise, new relation between statistical mechanics and quantum geometry.",
        "citation_title": "2d Ising Critical Couplings from Quantum Gravity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the AdS$_3$/CFT$_2$ framework, the Euclidean BTZ black hole corresponds to the dominant high-temperature phase of its dual field theory. We initially employ perturbative methods to solve the Einstein equations as boundary value problems, providing correlators for the energy-momentum tensor operator at low points. Utilizing operator equations established in our previous work, we further compute arbitrary high-point correlators for the energy-momentum tensor operator in the high-temperature phase and recursive relations for these high-point functions. Concurrently, we employ the Chern-Simons formalism to derive consistent results. Further, using the cut-off AdS/$T\\bar{T}$-deformed CFT duality, we calculate the energy-momentum tensor correlators, contributing to the comprehensive understanding of the system's dynamics. Finally, stress tensor correlators enable us to ascertain the corresponding KdV operator correlators at low-temperature.",
        "citation_title": "Note on holographic torus stress tensor correlators in $AdS_3$ gravity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We derive the cutoff length scale of the quadratic gravity in $d \\geq 5$ dimensional spacetime by demanding that the quantum focusing conjecture for the smeared quantum expansion holds at the classical level. The cutoff scale has different dependence on the spacetime dimension depending on the sign of the coupling constant of the quadratic gravity. We also investigate a concrete example of the 5-dimensional Schwarzschild spacetime and directly confirm that the quantum focusing conjecture holds when the quantum expansion is smeared over the scale larger than our cutoff scale.",
        "citation_title": "Cutoff Scale of Quadratic Gravity from Quantum Focusing Conjecture",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study classical localised configurations - solitons - in a theory of self-interacting complex Proca field with the global $U(1)$ symmetry. We focus on spherically-symmetric solitons near the nonrelativistic limit, which are supported by the quartic interactions of the neutral Proca field. Such solitons can source the radial electric (magnetic) field if one introduces a parity-even (parity-odd) coupling of the Proca field to the electromagnetic field tensor. We discuss the conditions of existence of such nontopological ''electromagnetic hedgehogs'' and their properties.",
        "citation_title": "Nontopological Electromagnetic Hedgehogs",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we systematically investigate the inflationary complexity of the two-mode squeezed state with thermal effect for the single field inflation, modified dispersion relation, and non-trivial sound speed with the method of closed system and open system, respectively, which our analysis is valid for most inflationary models. First, the numeric of Krylov complexity in the method of the closed system indicates that the evolution of Krylov complexity highly depends on the squeezed angle parameter once taking the thermal effect into account, which will decay into some very tiny values, but the Krylov complexity will always enhance without thermal effect. For comparison, the numeric of circuit complexity shows that the evolution is always increasing no matter whether there are thermal effects or not which is independent of the evolution of squeezed angle parameter. By utilizing the method of open system, we first construct the wave function. As for the Krylov complexity with the method of open system, our investigations show the evolution of Krylov complexity will enhance upon some peaks factoring in the thermal effects. For completeness, we also calculate the Krylov entropy in the method of closed system and open system, which indicates that the hotter universe, the more chaotic the universe. Furthermore, our derivation for the Krylov complexity and Krylov entropy could nicely recover into the case of closed system under weak dissipative approximation, which confirms the validity of construction for the wave function. Finally, our numeric of Lanczos coefficient shows that the non-trivial sound speed has minimal chaos compared to the other two cases.",
        "citation_title": "Inflationary complexity of thermal state",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Effective Lifshitz black holes with arbitrary dynamical exponent are addressed in the fluid/gravity membrane paradigm. The transport and the response coefficients in the dual Lifshitz field theory are calculated and analyzed, including the charge diffusion constant and the shear mode damping constant, along with the shear-viscosity-to-entropy density ratio. The Kubo formula is employed to obtain the electrical DC conductivity for the gauge sector corresponding to impurity through the holographic linear response of gauge vector fluctuations in the Lifshitz black brane geometry.",
        "citation_title": "Effective Lifshitz black holes, hydrodynamics, and transport coefficients in fluid/gravity correspondence",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We show that when the Wald-Zoupas prescription is implemented, the resulting charges realize the BMS symmetry algebra without any 2-cocycle nor central extension, at any cut of future null infinity. We refine the covariance prescription for application to the charge aspects, and introduce a new aspect for Geroch's super-momentum with better covariance properties. For the extended BMS symmetry with singular conformal Killing vectors we find that a Wald-Zoupas symplectic potential exists, if one is willing to modify the symplectic structure by a corner term. The resulting algebra of Noether currents between two arbitrary cuts is center-less. The charge algebra at a given cut has a residual field-dependent 2-cocycle, but time-independent and non-radiative. More precisely, super-rotation fluxes act covariantly, but super-rotation charges act covariantly only on global translations. The take home message is that in any situation where 2-cocycles appears in the literature, covariance has likely been lost in the charge prescription, and that the criterium of covariance is a powerful one to reduce ambiguities in the charges, and can be used also for ambiguities in the charge aspects.",
        "citation_title": "Centerless-BMS charge algebra",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we have studied the effects of pole dark energy on the evolution of gravitational waves. The background evolution of gravitational waves in a flat FRW universe is considered and its dynamics are studied in the presence of pole dark energy. Two different potential functions are considered for the study. Using the field equations, we formulated the perturbed equations governing the evolution of gravitational waves with respect to redshift z within the background of the FRW Universe. Subsequently, we delved into the characteristics of gravitational waves for the pole dark energy model and reached interesting results.",
        "citation_title": "The effects of the pole dark energy on gravitational waves",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We develop a Schwinger-Keldysh field theory (SKFT) for open quantum systems interacting with a dissipative environment and apply it to the spin-boson model as an archetypical example where the environment is composed of a bosonic bath. Prior SKFT developments of this type have been confined to the Markovian regime, as an alternative to a conventional description by the Lindblad quantum master equation (QME) which is a time-local matrix differential equation. Here we combine SKFT with a two-particle irreducible (2PI) action that resums a class of Feynman diagrams to infinite order. We obtain the time-evolution of the spin density matrix in the form of a system of integro-differential equations applicable to both Markovian and non-Markovian regimes. The latter regime--where taking into account memory effects becomes essential--poses a challenge for standard methods when trying to incorporate arbitrary properties of the system, bath, and length of time evolution. The SKFT+2PI-computed time evolution of the spin expectation values in the Markovian regime reproduces the solution of the Lindblad QME, as long as the system-bath coupling in the latter is adjusted by increasing it. In the non-Markovian regime, SKFT+2PI yields a nonperturbative solution that mimics results from both hierarchical equations of motion and tensor networks methods that we employ as benchmarks. Our SKFT+2PI approach can also access challenging cases, such as zero-temperature and sub-Ohmic bath, as well as arbitrary long evolution times. Taking into account favorable numerical cost of solving the integro-differential equations with increasing number of spins, time steps or dimensionality the SKFT+2PI approach offers a promising route for simulation of driven-dissipative systems in quantum computing or quantum magnonics and spintronics in the presence of a variety of (single or multiple) dissipative environments.",
        "citation_title": "Schwinger-Keldysh nonequilibrium quantum field theory of open quantum systems beyond the Markovian regime: Application to the spin-boson model",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The baryon acoustic oscillation (BAO) analysis from the first year of data from the Dark Energy Spectroscopic Instrument (DESI), when combined with data from the cosmic microwave background (CMB), has placed an upper-limit on the sum of neutrino masses, $\\sum m_\\nu < 70$ meV (95%). In addition to excluding the minimum sum associated with the inverted hierarchy, the posterior is peaked at $\\sum m_\\nu = 0$ and is close to excluding even the minumum sum, 58 meV at 2$\\sigma$. In this paper, we explore the implications of this data for cosmology and particle physics. The sum of neutrino mass is determined in cosmology from the suppression of clustering in the late universe. Allowing the clustering to be enhanced, we extended the DESI analysis to $\\sum m_\\nu < 0$ and find $\\sum m_\\nu = - 160 \\pm 90$ meV (68%), and that the suppression of power from the minimum sum of neutrino masses is excluded at 99% confidence. We show this preference for negative masses makes it challenging to explain the result by a shift of cosmic parameters, such as the optical depth or matter density. We then show how a result of $\\sum m_\\nu =0$ could arise from new physics in the neutrino sector, including decay, cooling, and/or time-dependent masses. These models are consistent with current observations but imply new physics that is accessible in a wide range of experiments. In addition, we discuss how an apparent signal with $\\sum m_\\nu < 0$ can arise from new long range forces in the dark sector or from a primordial trispectrum that resembles the signal of CMB lensing.",
        "citation_title": "No $\u03bd$s is Good News",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We consider the Euclidean vacuum for linearized gravity on the global de Sitter space, obtained from the Euclidean Green's function on the 4-sphere. We use the notion of Calder\u00f3n projectors to recover a quantum state for the Lorentzian theory on de Sitter space. We show that while the state is gauge invariant and Hadamard, it is not positive on the whole of the phase space. We show however that a suitable modification at low energies yields a well-defined Hadamard state on global de Sitter space.",
        "citation_title": "IR-fixed Euclidean vacuum for linearized gravity on de Sitter space",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Modular flavor symmetries refers to scenarios in which fermion masses respect modular symmetries. Such scenarios have been studied in the bottom-up approach and have an explicit realization in string theory. They rely on the remarkable properties of vector-valued modular forms.",
        "citation_title": "Aspects of Modular Flavor Symmetries",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In this paper, we investigate the gravitational collapse to form the black hole in the acceleratingly expanding universe in the frame of Einstein--Gauss-Bonnet theory having two scalar fields and we study the propagation of the gravitational wave (GW). The collapsing spacetime can be obtained by using the formulation of the ``reconstruction'', that is, we find a model that realises the desired or given geometry. In the reconstructed models, ghosts often appear, which could be eliminated by imposing constraints. We show that the standard cosmological solutions or self-gravitating objects such as a planet, the Sun, various types of stars, etc., in Einstein's gravity, are also solutions in this model. Using the dynamical value of Gauss-Bonnet coupling, the propagation of the high-frequency GW is investigated. The propagating speed changes due to the coupling during the period of the black hole formation. The speed at which the GW propagates The speed at which the GW propagates going into the black hole is different from that of the wave going out.",
        "citation_title": "Gravitational collapse and gravitational wave in Einstein--Gauss-Bonnet theory with two scalar fields",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We develop a procedure for re-summing the large logarithms induced in gravity by loops of inflationary scalars. We first show how the scalar can be integrated out of the field equations in the presence of constant graviton field. We then extend this result to a fully conserved form which explains the need for a finite renormalization of the cosmological constant which was previously inferred from explicit computation. A variant of the renormalization group turns out to explain the large logarithmic corrections revealed by explicit computation in the electric field strength of gravitational radiation and in the potentials which characterize the response to a point mass. The implications for graviton loops are discussed.",
        "citation_title": "Summing Gravitational Effects from Loops of Inflationary Scalars",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study in detail the influence of different chemical potentials (baryon, charged, strange, and neutrino) on how and how fast a free gas of quarks in the zero-temperature limit reaches the conformal limit. We discuss the influence of non-zero masses, the inclusion of leptons, and different constraints, such as charge neutrality, zero-net strangeness, and fixed lepton fraction. We also investigate for the first time how the symmetry energy of the system under some of these conditions approaches the conformal limit. Finally, we briefly discuss what kind of corrections are expected from perturbative QCD as one goes away from the conformal limit.",
        "citation_title": "Approaching the conformal limit of quark matter with different chemical potentials",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The canonical quantisation of General Relativity including matter on a spacetime manifold in the globally hyperbolic setting involves in particular the representation theory of the spatial diffeomorphism group (SDG), and/or its Lie algebra (SDA), of the underlying spatial submanifold. There are well known Fock representations of the SDA in one spatial dimension and non-Fock representations of the SDG in all dimensions. The latter are not strongly continuous and do not descend to representations of the SDA.\nIn this work we report some partial results on non anomalous representations of the SDA for both geometry and matter: 1. Background independent Fock representations of the SDA by operators exist in all dimensions. 2. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA by operators exist in one dimension but these do not extend to higher dimensions. 3. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA of volume preserving diffeomorphisms by operators exist in all dimensions. 4. Infinitely many unitary equivalence classes of background dependent Fock representations of the SDA by quadratic forms exist in all dimensions.\nExcept for 1. these representations do not descend from an invariant state of the Weyl algebra and 4. points to a new strategy for solving the quantum constraints.",
        "citation_title": "Observations on representations of the spatial diffeomorphism group and algebra in all dimensions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Perturbative quantum gravity starts from prescribing a background metric. That background metric is then used in order to carry out two separate steps: 1. One splits the non-perturbative metric into background and deviation from it (graviton) and expands the action in terms of the graviton which results in an ifinite series of unknown radius of convergence. 2. One constructs a Fock representation for the graviton and performs perturbative graviton quantum field theory on the fixed background as dictated by the perturbative action. The result is a non-renormalisable theory without predictive power.\nIt is therefore widely believed that a non-perturbative approach is mandatory in order to construct a fundamental, not only effective, predictive quantum field theory of the gravitational interaction. Since perturbation theory is by definition background dependent, the notions of background dependence (BD) and perturbation theory (PT) are often considered as symbiotic, as if they imply each other.\nIn the present work we point out that there is no such symbiosis, these two notions are in fact logically independent. In particular, one can use BD structures while while not using PT at all. Specifically, we construct BD Fock representations (step 2 above) for the full, non-perturbative metric rather than the graviton (not step 1 above) and therefore never perform a perturbative expansion. Despite the fact that the gravitational Lagrangean is a non-polynomial, not even analytic, function of the metric we show that e.g. the Hamiltonian constraint with any density weight can be defined as a quadratic form with dense form domain in such a representation.",
        "citation_title": "Non-perturbative Quantum Gravity in Fock representations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This work investigates the intricate relationship between the q-boson model, a quantum integrable system, and classical integrable systems such as the Toda and KP hierarchies. Initially, we analyze scalar products of off-shell Bethe states and explore their connections to tau functions of integrable hierarchies. Furthermore, we discuss correlation functions within this formalism, examining their representations in terms of tau functions, as well as their Schur polynomial expansions.",
        "citation_title": "$Q$-Boson model and relations with integrable hierarchies",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This thesis is developed in the context of the spin-foam approach to quantum gravity; all results are concerned with the Lorentzian theory and with semiclassical methods. A correspondence is given between Majorana 2-spinors and time-like hypersurfaces in Minkowski 3-space based on complexified quaternions. It is shown that the former suggest a symplectic structure on the spinor phase space which, together with an area-matching constraint, yields a symplectomorphism to $T^*\\mathrm{SU}(1,1)$. A complete 3-dimensional Lorentzian spin-foam amplitude for both space- and time-like triangles is proposed. It is shown to asymptote to Regge theory in the semiclassical regime. The asymptotic limit of the 4-dimensional Conrady-Hnybida model for general polytopes is scrutinized. Minkowski's theorem on convex polyhedra is generalized to Lorentzian signature, and new boundary states for time-like polygons are introduced. It is found that the semiclassical amplitude for such polygons is insufficiently constrained. A method for the asymptotic evaluation of integrals subject to external parameters is discussed. The method is developed in detail for the special problem of spin-foam gluing constraints away from their dominant critical points. A relation to the gluing constraints of effective spin-foams is suggested.",
        "citation_title": "Investigations on Lorentzian Spin-foams and Semiclassical Space-times",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Different approaches to quantum gravity, such as loop quantum cosmology and group field theory, predict the resolution of the initial cosmological singularity via a 'bounce': a regular spacetime region that connects the expanding branch of the universe to a contracting branch. The cosmological arrow of time, which by definition points in the direction of cosmic expansion, is reversed at the bounce. Nonetheless, it is still possible to discriminate between the two branches by considering different arrows, as defined for instance by the growth of perturbations. After reviewing general aspects of the time arrow problem in cosmology, we examine the properties of different arrows of time in bouncing cosmologies, focusing on the loop quantum cosmology bounce as a case study. We also present a new exact solution to the effective Friedmann equations of loop quantum cosmology with pressureless dust and a cosmological constant, which is a simplified version of the $\\Lambda$CDM bounce scenario, where these issues can be examined in detail.",
        "citation_title": "Arrows of time in bouncing cosmologies",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Gertsenshtein effect could in principle be used to detect a single graviton by firing it through a region filled with a constant magnetic field that enables its conversion to a photon, which can be efficiently detected via standard techniques. The quantization of the gravitational field could then be inferred indirectly. We show that for currently available single-photon detector technology, the Gertsenshtein detector is generically inefficient, meaning that the probability of detection is $\\ll 1$. The Gertsenshtein detector can become efficient on astrophysical scales for futuristic single-photon detectors sensitive to frequencies in the Hz to kHz range. It is not clear whether such devices are in principle possible.",
        "citation_title": "Graviton-photon oscillations as a probe of quantum gravity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We expound upon our (polarization-free) definition of the quantization map in geometric quantization, which is justified using the Poisson sigma model and pieces together most known quantization schemes. We use it to obtain the noncommutative torus and a finite dimensional irreducible representation. We discuss invariance of polarization using Schur's lemma.",
        "citation_title": "Geometric Quantization Without Polarizations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose a new solution to the hierarchy (naturalness) problem, concerning quantum corrections of the Higgs mass. Assuming the Higgs boson as a system with a self-similar internal structure, we calculate its two-point function and find that the quadratic divergence is replaced by a logarithmic one in the mass corrections. It is shown that the partonic-like distribution follows the Tsallis statistics and also high energy physics experimental data for the Higgs transverse momentum distribution can be described by a self-similar statistical model.",
        "citation_title": "The Higgs boson as a self-similar system: Towards a new solution to the hierarchy problem",
        "date_delivered": "[Submitted on 21 Apr 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We construct a morphism of spectra from $\\mathrm{KO}((q))/\\mathrm{TMF}$ to $\\Sigma^{-20}I_{\\mathbb{Z}} \\mathrm{TMF}$, which we show to be an equivalence and to implement the Anderson self-duality of $\\mathrm{TMF}$. This morphism is then used to define another morphism from $\\mathrm{TMF}$ to $\\Sigma^{-20}I_{\\mathbb{Z}}(\\mathrm{MSpin}/\\mathrm{MString})$, which induces a differential geometric pairing and captures not only the invariant of Bunke and Naumann but also a finer invariant which detects subtle Anderson dual pairs of elements of $\\pi_\\bullet\\mathrm{TMF}$. Our analysis leads to conjectures concerning certain self-dual vertex operator superalgebras and some specific torsion classes in $\\pi_\\bullet\\mathrm{TMF}$.\nThis paper is written as an article in mathematics, but much of the discussions in it was originally motivated by a study in heterotic string theory. As such, we have a separate appendix for physicists, in which the contents of the paper are summarized and translated into a language more amenable to them. In physics terms, our result allows us to compute the discrete part of the Green-Schwarz coupling of the $B$-field in a couple of subtle hitherto-unexplored cases.",
        "citation_title": "Anderson self-duality of topological modular forms, its differential-geometric manifestations, and vertex operator algebras",
        "date_delivered": "[Submitted on 10 May 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We compute the complete set of two-loop master integrals for the scattering of four massless particles and a massive one. Our results are ready for phenomenological applications, removing a major obstacle to the computation of complete next-to-next-to-leading order (NNLO) QCD corrections to processes such as the production of a $H/Z/W$ boson in association with two jets at the LHC. Furthermore, they open the door to new investigations into the structure of quantum-field theories and provide precious analytic data for studying the mathematical properties of Feynman integrals.",
        "citation_title": "All Two-Loop Feynman Integrals for Five-Point One-Mass Scattering",
        "date_delivered": "[Submitted on 27 Jun 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "An effective field theory framework is used to investigate some Lorentz-violating effects on the generation of electromagnetic and gravitational waves, complementing previous work on propagation. Specifically we find solutions to a modified, anisotropic wave equation, sourced by charge or fluid matter. We derive the radiation fields for scalars, classical electromagnetic radiation, and partial results for gravitational radiation. For gravitational waves, the results show longitudinal and breathing polarizations proportional to coefficients for spacetime-symmetry breaking.",
        "citation_title": "Classical radiation fields for scalar, electromagnetic, and gravitational waves with spacetime-symmetry breaking",
        "date_delivered": "[Submitted on 25 Jul 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We investigate the gauging of a $\\mathbb{Z}_2$ symmetry in Narain conformal field theories (CFTs) constructed from qudit stabilizer codes. Considering both orbifold and fermionization, we establish a connection between $\\mathbb{Z}_2$ gauging procedures and modifications of the momentum lattice by vectors characterizing the $\\mathbb{Z}_2$ symmetry. We also provide three-dimensional interpretations of $\\mathbb{Z}_2$ gaugings through abelian Chern-Simons theories, which act as symmetry topological field theories.",
        "citation_title": "Narain CFTs from quantum codes and their $\\mathbb{Z}_2$ gauging",
        "date_delivered": "[Submitted on 3 Aug 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "General Relativity (GR) and Unimodular Gravity (UG) provide two equivalent descriptions of gravity that differ in the nature of the cosmological constant. While GR is based on the group of diffeomorphisms that permits the cosmological constant in the action, UG is based on the subgroup of volume-preserving diffeomorphisms together with Weyl transformations which forbid the presence of the cosmological constant. However, the cosmological constant reappears in UG as an integration constant so it arises as a global degree of freedom. Since gauge symmetries are simply redundancies in our description of physical systems, a natural question is whether there exists a \"parent theory\" with the full diffeomorphisms and Weyl transformations as gauge symmetries so that it reduces to GR and UG respectively by performing suitable (partial) gauge fixings. We will explore this question by introducing Stueckelberg fields both in GR and UG to complete the gauge symmetries in each theory to that of the would-be parent theory. Despite the dynamical equivalence of the two theories, we find that precisely the additional global degree of freedom provided by the cosmological constant in UG obstructs the construction of the parent theory.",
        "citation_title": "Nonexistence of a parent theory for general relativity and unimodular gravity",
        "date_delivered": "[Submitted on 13 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We construct a doped holographic superconductor in the Gubser-Rocha model, and realize a superconducting dome in the middle of the temperature-doping phase diagram. It is worth noting that unlike the previous researches, the profile of our dome shrinks inward near zero temperature. From the numerical observation for the coupling dependence of the phase diagram, we find that the coupling between the two gauge fields plays a crucial role in the formation of dome. We also analytically calculate the DC conductivity of the normal phase of the system in the momentum dissipation and obtain the resistivity which is proportional to temperature. The AC conductivity is calculated numerically.",
        "citation_title": "Doped Holographic Superconductors in Gubser-Rocha model",
        "date_delivered": "[Submitted on 26 Sep 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We discuss relations between closed and open string amplitudes at one-loop. While at tree-level these relations are known as Kawai-Lewellen-Tye (KLT) and/or double copy relations, here we investigate how such relations are manifested at one-loop. While there exist examples of one-loop closed string amplitudes that can strikingly be written as sum over squares of one-loop open string amplitudes, generically the one-loop closed string amplitudes assume a form reminiscent from the one-loop doubly copy structure of gravitational amplitudes involving a loop momentum. This double copy structure represents the one-loop generalization of the KLT relations.",
        "citation_title": "One-loop Double Copy Relation in String Theory",
        "date_delivered": "[Submitted on 11 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We classify connected \u00e9tale algebras $A$'s in pre-modular fusion categories $\\mathcal B$ with $\\text{rank}(\\mathcal B)\\le3$ including degenerate and non-(pseudo-)unitary ones. We comment on Lagrangian algebras and physical applications to ground state degeneracy and proof of spontaneous $\\mathcal B$-symmetry breaking.",
        "citation_title": "Classification of connected \u00e9tale algebras in pre-modular fusion categories up to rank three",
        "date_delivered": "[Submitted on 27 Nov 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We study the SYK model -- an important toy model for quantum gravity on IBM's superconducting qubit quantum computers. By using a graph-coloring algorithm to minimize the number of commuting clusters of terms in the qubitized Hamiltonian, we find the gate complexity of the time evolution using the first-order product formula for $N$ Majorana fermions is $\\mathcal{O}(N^5 J^{2}t^2/\\epsilon)$ where $J$ is the dimensionful coupling parameter, $t$ is the evolution time, and $\\epsilon$ is the desired precision. With this improved resource requirement, we perform the time evolution for $N=6, 8$ with maximum two-qubit circuit depth of 343. We perform different error mitigation schemes on the noisy hardware results and find good agreement with the exact diagonalization results on classical computers and noiseless simulators. In particular, we compute return probability after time $t$ and out-of-time order correlators (OTOC) which is a standard observable of quantifying the chaotic nature of quantum systems.",
        "citation_title": "Sachdev-Ye-Kitaev model on a noisy quantum computer",
        "date_delivered": "[Submitted on 29 Nov 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "Axion-like particles are predicted in many physics scenarios beyond the Standard Model (SM). Their interactions with SM particles may arise from the triangle anomaly of the associated global symmetry, along with other SM global and gauge symmetries, including anomalies with the global baryon number and electromagnetic gauge symmetries. We initiate the phenomenological study of the corresponding ``electrobaryonic axion\", a particle that couples with both the baryon chemical potential and the electromagnetic field. Neutron stars, particularly magnetars, possessing high baryon density and strong magnetic fields, can naturally develop a thin axion hair around their surface. In this study, we calculate this phenomenon, considering the effects of neutron star rotation and general relativity. For axion particles lighter than the neutron star rotation frequency, the anomalous interaction can also induce the emission of axion particles from the neutron star. This emission, in the light axion regime, can have a significant contribution to the neutron star cooling rate.",
        "citation_title": "Electrobaryonic axion: hair of neutron stars",
        "date_delivered": "[Submitted on 30 Nov 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In the Randall-Sundrum (RS) II braneworld scenario, general relativity (GR) is modified by adding an extra dimension such that it is indistinguishable from GR in the weak gravity limit. However, such modifications may leave a mark in the strong field regime. We therefore analyze massive scalar perturbations around rotating black holes in the RS II model. Unlike black holes in GR, these braneworld black holes carry a tidal charge that contains information about the extra spatial dimension, and the rotation parameter for such black holes can exceed unity. Through the method of continued fractions, we investigate the quasinormal mode spectra, and the superradiant instabilities associated with the existence of quasibound states, that is, gravitational atoms. In comparison to the four-dimensional Kerr black hole, we report distinctive signatures of the tidal charge and the rotation parameter, which manifest as signals of the extra dimension, on both the fundamental quasinormal mode and the formation of gravitational atoms. These findings offer insights into testing modifications to GR and detecting ultralight bosonic particles around black holes.",
        "citation_title": "Gravitational atoms in the braneworld scenario",
        "date_delivered": "[Submitted on 12 Dec 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "In this second installment of a series of two papers on the $\\tfrac{1}{2}$-BPS Wilson line defect CFT in $\\mathcal{N}=4$ super Yang-Mills, we focus on dynamical aspects of the theory, in particular studying four-point functions with analytic bootstrap methods. Relying on the results of a companion paper for the kinematics and strong coupling spectrum, we consider various four-point functions in the planar limit, in an expansion for large 't Hooft coupling. Our ultimate goal is to provide a detailed derivation of the four-point function of the displacement supermultiplet at three loops, first presented in 2103.10440. Along the way, we present a large amount of new results including four-point functions with zero, one or two long external supermultiplets. The last two represent a novelty in the analytic bootstrap literature and are instrumental in addressing the problem of operators degeneracy. Such phenomenon leads to the necessity of resolving a mixing problem that is more complicated than those usually encountered in the study of holographic correlators, thus leading us to the development of a new approach that we believe will have a wider range of applicability. Related to this issue, we analyze in some detail the structure of the dilatation operator in this model. Some of the ingredients that we use apply more generally to holographic theories, although a thorough investigation of these aspects is missing, to the best of our knowledge, in most interesting cases.",
        "citation_title": "Unmixing the Wilson line defect CFT. Part II: analytic bootstrap",
        "date_delivered": "[Submitted on 19 Dec 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We classify connected \u00e9tale algebras in (possibly non-unitary) modular fusion categories $\\mathcal B$'s with $\\text{rank}(\\mathcal B)\\le5$. We also comment on Lagrangian algebra, anyon condensation, and physical applications. Concretely, we prove certain spontaneous $\\mathcal B$-symmetry breaking and predict ground state degeneracies in massive renormalization group flows from non-unitary minimal models.",
        "citation_title": "Classification of connected \u00e9tale algebras in modular fusion categories up to rank five",
        "date_delivered": "[Submitted on 20 Dec 2023 (v1), last revised 2 May 2024 (this version, v5)]"
    },
    {
        "abstract": "Metastable cosmic strings appear in models of new physics with a two-step symmetry breaking $G\\to H\\to 1$, where $\\pi_1(H)\\neq 0$ and $\\pi_1(G)=0$. They decay via the monopole-antimonopole pair creation inside. Conventionally, the breaking rate has been estimated by an infinitely thin string approximation, which requires a large hierarchy between the symmetry breaking scales. In this paper, we reexamine it by taking into account the finite sizes of both the cosmic string and the monopole. We obtain a robust lower limit on the tunneling factor $e^{-S_B}$ even for regimes the conventional estimate is unreliable. In particular, it is relevant to the cosmic string interpretation of the gravitational wave signals recently reported by pulsar timing array experiments.",
        "citation_title": "Revisiting Metastable Cosmic String Breaking",
        "date_delivered": "[Submitted on 25 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Recent years have seen the emergence of a new understanding of scattering amplitudes in the simplest theory of colored scalar particles - the Tr$(\\phi^3)$ theory - based on combinatorial and geometric ideas in the kinematic space of scattering data. In this paper we report a surprise: far from the toy model it appears to be, the ''stringy'' Tr$(\\phi^3)$ amplitudes secretly contain the scattering amplitudes for pions, as well as non-supersymmetric gluons, in any number of dimensions. The amplitudes for the different theories are given by one and the same function, related by a simple shift of the kinematics. This discovery was spurred by another fundamental observation: the tree-level Tr$(\\phi^3)$ field theory amplitudes have a hidden pattern of zeros when a special set of non-planar Mandelstam invariants is set to zero. Furthermore, near these zeros, the amplitudes simplify, by factoring into a non-trivial product of smaller amplitudes. Remarkably the amplitudes for pions and gluons are observed to also vanish in the same kinematical locus. These properties further generalize to the ''stringy'' Tr$(\\phi^3)$ amplitudes. There is a unique shift of the kinematic data that preserves the zeros, and this shift is precisely the one that unifies colored scalars, pions, and gluons into a single object. We will focus in this paper on explaining the hidden zeros and factorization properties and the connection between all the colored theories, working for simplicity at tree-level. Subsequent works will describe this new formulation for the Non-linear Sigma Model and non-supersymmetric Yang-Mills theory, at all loop orders.",
        "citation_title": "Hidden zeros for particle/string amplitudes and the unity of colored scalars, pions and gluons",
        "date_delivered": "[Submitted on 26 Dec 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We use equivariant localization to construct off-shell entropy functions for supersymmetric black holes in $\\mathcal{N}=2$, $D=4$ gauged supergravity coupled to matter. This allows one to compute the black hole entropy without solving the supergravity equations of motion and provides a novel generalization of the attractor mechanism. We consider magnetically charged black holes in $AdS_4$ which have an $AdS_2\\times M_2$ near horizon geometry, where $M_2$ is a sphere or a spindle, and we also obtain entropy functions for ungauged supergravity as a simple corollary. We derive analogous results for black strings and rings in $D=5$ supergravity which have an $AdS_3\\times M_2$ near horizon geometry, and in this setting we derive an off-shell expression for the central charge of the dual $\\mathcal{N}=(0,2)$, $d=2$~SCFT.",
        "citation_title": "Localization and Attraction",
        "date_delivered": "[Submitted on 19 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Low-acceleration gravitational anomaly is investigated with a new method of exploiting the normalized velocity profile $\\tilde{v}\\equiv v_p/v_c$ of wide binary stars as a function of the normalized sky-projected radius $s/r_{\\rm{M}}$ where $v_p$ is the sky-projected relative velocity between the pair, $v_c$ is the Newtonian circular velocity at the sky-projected separation $s$, and $r_{\\rm{M}}$ is the MOND radius. With a Monte Carlo method Gaia observed binaries and their virtual Newtonian counterparts are probabilistically distributed on the $s/r_{\\rm{M}}$ versus $\\tilde{v}$ plane and a logarithmic velocity ratio parameter $\\Gamma$ is measured in the bins of $s/r_{\\rm{M}}$. With three samples of binaries covering a broad range in size, data quality, and implied fraction of hierarchical systems including a new sample of 6389 binaries selected with accurate distances and radial velocities, I find a unanimous systematic variation from the Newtonian flat line. With $\\Gamma=0$ at $s/r_{\\rm{M}}\\lesssim 0.15$ or $s\\lesssim 1$~kilo astronomical units (kau), I get $\\Gamma=0.068\\pm 0.015$ (stat) $_{-0.015}^{+0.024}$ (syst) for $s/r_{\\rm{M}} \\gtrsim 0.7$ or $s\\gtrsim 5$~kau. The gravitational anomaly (i.e.\\ acceleration boost) factor given by $\\gamma_g = 10^{2\\Gamma}$ is measured to be $\\gamma_g = 1.37_{-0.09}^{+0.10}$ (stat) $_{-0.09}^{+0.16}$ (syst). With a reduced $\\chi^2$ test of Newtonian and Milgromian nonrelativistic theories, I find that Newtonian gravity is ruled out at $5.8\\sigma$ ($\\chi^2_\\nu=9.4$) by the new sample (and $9.2\\sigma$ by the largest sample used). The Milgromian AQUAL theory is acceptable with $0.5\\lesssim \\chi^2_\\nu\\lesssim 3.1$. These results agree well with earlier results with the ``acceleration-plane analysis'' for a variety of samples and the ``stacked velocity profile analysis'' for a pure binary sample.",
        "citation_title": "Measurements of the Low-Acceleration Gravitational Anomaly from the Normalized Velocity Profile of Gaia Wide Binary Stars and Statistical Testing of Newtonian and Milgromian Theories",
        "date_delivered": "[Submitted on 8 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "What is the bulk Hilbert space of quantum gravity? In this paper, we resolve this problem in 2d JT gravity, both with and without matter, providing the first example of an explicit definition of a non-perturbative Hilbert space specified in terms of metric variables. The states are wavefunctions of the length and matter state, but with a non-trivial and highly degenerate inner product. We explicitly identify the null states, and discuss their importance for defining operators non-perturbatively. To highlight the power of the formalism we developed, we study the non-perturbative effects for two bulk linear operators that may serve as proxies for the experience of an observer falling into a two-sided black hole: one captures the length of an Einstein-Rosen bridge and the other captures the center-of-mass collision energy between two particles falling from opposite sides. We track the behavior of these operators up to times of order $e^{S_\\text{BH}}$, at which point the wavefunction spreads to the complete set of eigenstates of these operators. If these observables are indeed good proxies for the experience of an infalling observer, our results indicate an O(1) probability of detecting a firewall at late times that is self-averaging and universal.",
        "citation_title": "On the non-perturbative bulk Hilbert space of JT gravity",
        "date_delivered": "[Submitted on 13 Mar 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In view to scrutinize the idea that nonlocal modifications of General Relativity could dynamically address the dark energy problem, we investigate the evolution of the Universe at infrared scales as an Infinite Derivative Gravity model of the Ricci scalar, without introducing the cosmological constant $\\Lambda$ or any scalar field. The accelerated expansion of the late Universe is shown to be compatible with the emergence of nonlocal gravitational effects at sufficiently low energies. A technique for circumventing the mathematical complexity of the nonlocal cosmological equations is developed and, after drawing a connection with the Starobinsky gravity, verifiable predictions are considered, like a possible decreasing in the strength of the effective gravitational constant. In conclusion, the emergence of nonlocal gravity corrections at given scales could be an efficient mechanism to address the dark energy problem.",
        "citation_title": "Can nonlocal gravity really explain dark energy?",
        "date_delivered": "[Submitted on 17 Mar 2024 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We consider linear superpositions of single particle excitations in a scalar field theory on $AdS_3$ and evaluate their contribution to the bulk entanglement entropy across the Ryu-Takayanagi surface. We compare the entanglement entropy of these excitations obtained using the Faulkner-Lewkowycz-Maldacena formula to the entanglement entropy of linear superposition of global descendants of a conformal primary in a large $c$ CFT obtained using the replica trick. We show that the closed from expressions for the entanglement entropy in the small interval expansion both in gravity and the CFT precisely agree. The agreement serves as a non-trivial check of the FLM formula for the quantum corrections to holographic entropy which also involves a contribution from the back reacted minimal area. Our checks includes an example in which the state is time dependent and spatially in-homogenous as well another example involving a coherent state with a Ba\u00f1ados geometry as its holographic dual.",
        "citation_title": "Precision tests of bulk entanglement entropy",
        "date_delivered": "[Submitted on 4 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this paper, we propose a recipe for B-model computation of genus 1 Gromov-Witten invariants of Calabi-Yau and Fano Projective Hypersurfaces. Our formalism can be applied equally to both Calabi-Yau and Fano cases. In Calabi-Yau case, drastic cancellation of terms used in our formalism occurs and it results in another representation of BCOV-Zinger formula for projective Calabi-Yau hypersurfaces.",
        "citation_title": "Elliptic Virtual Structure Constants and Generalizations of BCOV-Zinger Formula to Projective Fano Hypersurfaces",
        "date_delivered": "[Submitted on 11 Apr 2024 (v1), last revised 2 May 2024 (this version, v5)]"
    },
    {
        "abstract": "A (2+1)D topological ordered phase with U(1) symmetry may or may not have a symmetric gapped edge state, even if both thermal and electric Hall conductivity are vanishing. It is recently discovered that there are \"higher\" versions of Hall conductivity valid for fermionic fractional quantum Hall (FQH) states, which obstructs symmetry-preserving gapped edge state beyond thermal and electric Hall conductivity. In this paper, we show that one can extract higher Hall conductivity from a single wave function of an FQH state, by evaluating the expectation value of the \"partial rotation\" unitary which is a combination of partial spatial rotation and a U(1) phase rotation. This result is verified numerically with the fermionic Laughlin state with $\\nu=1/3$, $1/5$, as well as the non-Abelian Moore-Read state. Together with topological entanglement entropy, we prove that the expectation values of the partial rotation completely determines if a bosonic/fermionic Abelian topological order with U(1) symmetry has a symmetry-preserving gappable edge state or not. We also show that thermal and electric Hall conductivity of Abelian topological order can be extracted by partial rotations. Even in non-Abelian FQH states, partial rotation provides the Lieb-Schultz-Mattis type theorem constraining the low-energy spectrum of the bulk-boundary system. The generalization of higher Hall conductivity to the case with Lie group symmetry is also presented.",
        "citation_title": "Higher Hall conductivity from a single wave function: Obstructions to symmetry-preserving gapped edge of (2+1)D topological order",
        "date_delivered": "[Submitted on 16 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We introduce the notion of paraquaternionic contact structures (pqc structures), which turns out to be a generalization of the para 3-Sasakian geometry. We derive a distinguished linear connection preserving the pqc structure. Its torsion tensor is expressed explicitly in terms of the structure tensors and the structure equations of a pqc manifold are presented. We define pqc-Einstein manifolds and show that para 3-Sasakian spaces are precisely pqc manifolds, which are pqc-Einstein. Furthermore, we introduce the paraquaternionic Heisenberg qroup and show that it is the flat model of the pqc geometry.",
        "citation_title": "Geometry of paraquaternionic contact structures",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this paper, we investigated two models incorporating LIV with nonminimal coupling in the ( e^+e^- \\rightarrow \\mu^+\\mu^- ) scattering process, employing cross-section calculations. We found that utilizing the dual electromagnetic tensor led to violations of unitarity in both vector and axial scenarios. However, in the other model the preservation of unitarity by the electromagnetic tensor in both scenarios implies its potential superiority, this implies promising prospects for this CPT and Lorentz symmetry-breaking model.",
        "citation_title": "The effects of Lorentz violation on unitarity in $e^{+}e^{-} \\rightarrow \u03bc^{+}\u03bc^{-}$",
        "date_delivered": "[Submitted on 28 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this paper, we employ the Nash-Moser iteration technique to study local and global properties of positive solutions to the equation $$\\Delta_pv+bv^q+cv^r =0$$ on complete Riemannian manifolds with Ricci curvature bounded from below, where $b, c\\in\\mathbb R$, $p>1$, and $q\\leq r$ are some real constants. Assuming certain conditions on $b,\\, c,\\, p,\\, q$ and $r$, we derive succinct Cheng-Yau type gradient estimates for positive solutions, which is of sharp form. These gradient estimates allow us to obtain some Liouville-type theorems and Harnack inequalities. Our Liouville-type results are novel even in Euclidean spaces. Based on the local gradient estimates and a trick of Sung and Wang, we also obtain the global gradient estimates for such solutions. As applications we show the uniqueness of positive solutions to some generalized Allen-Cahn equation and Fisher-KPP equation.",
        "citation_title": "Local and Global Log-Gradient estimates of solutions to $\u0394_pv+bv^q+cv^r =0$ on manifolds and applications",
        "date_delivered": "[Submitted on 22 Apr 2024]"
    },
    {
        "abstract": "Let $n\\ge2$ and $\\mathcal{L}=-\\mathrm{div}(A\\nabla\\cdot)$ be an elliptic operator on $\\mathbb{R}^n$. Given an exterior Lipschitz domain $\\Omega$, let $\\mathcal{L}_D$ and $\\mathcal{L}_N$ be the elliptic operators $\\mathcal{L}$ on $\\Omega$ subject to the Dirichlet and the Neumann boundary {conditions}, respectively. For the Neumann operator, we show that the reverse inequality $\\|\\mathcal{L}_N^{1/2}f\\|_{L^p(\\Omega)} \\le C\\|\\nabla f\\|_{L^p(\\Omega)}$ holds true for any $p\\in(1,\\infty)$. For the Dirichlet operator, it was known that the Riesz operator $\\nabla \\mathcal{L}_D^{-1/2}$ is not bounded for $p>2$ and $p\\ge n$, even if $\\mathcal{L}=-\\Delta$ being the Laplace operator. Suppose that $A$ are CMO coefficients or VMO coefficients satisfying certain perturbation property, and $\\partial\\Omega$ is $C^1$, we prove that for $p>2$ and $p\\in [n,\\infty)$, it holds $$ \\inf_{\\phi\\in\\mathcal{A}^p_0(\\Omega)}\\left\\|\\nabla f-\\nabla\\phi\\right\\|_{L^p(\\Omega)}\\le C\\left\\|\\mathcal{L}^{1/2}_D f\\right\\|_{L^p(\\Omega)} $$ for $f\\in \\dot{W}^{1,p}_0(\\Omega)$. Here $\\mathcal{A}^p_0(\\Omega)=\\{f\\in \\dot{W}^{1,p}_0(\\Omega):\\,\\mathcal{L}_Df=0\\}$ is a non-trivial subspace generated by harmonic function in $\\Omega$ with zero boundary value.",
        "citation_title": "Some inequalities related to Riesz transform on exterior Lipschitz domains",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "The foundations of Ringel duality for split quasi-hereditary algebras over commutative Noetherian rings are strengthened. Several descriptions and properties of the smallest resolving subcategory containing all standard modules over split quasi-hereditary algebras over commutative Noetherian rings are provided.\nIn particular, given two split quasi-hereditary algebras $A$ and $B$, we prove that any exact equivalence between the smallest resolving subcategory containing all standard modules over $A$ and the smallest resolving subcategory containing all standard modules over $B$ lifts to a Morita equivalence between $A$ and $B$ which preserves the quasi-hereditary structure.",
        "citation_title": "Characteristic tilting modules and Ringel duality in the Noetherian world",
        "date_delivered": "[Submitted on 27 Apr 2024]"
    },
    {
        "abstract": "We study a variational problem on $H^1({\\mathbb R})$ under an $L^\\infty$-constraint related to Sobolev-type inequalities for a class of generalized potentials, including $L^p$-potentials, non-positive potentials, and signed Radon measures. We establish various essential tools for this variational problem, including the decomposition principle, the comparison principle, and the perturbation theorem, those are the basis of the two-step minimization method. As for their applications, we present precise results for minimizers of minimization problems, such as the study of potentials of Dirac's delta measure type and the analysis of trapped modes in potential wells.",
        "citation_title": "Two-step minimization approach to an $L^\\infty$-constrained variational problem with a generalized potential",
        "date_delivered": "[Submitted on 28 Apr 2024]"
    },
    {
        "abstract": "We prove the $L^p-L^q$ $(1<p\\leqslant 2\\leqslant q<+\\infty)$ norm estimates for the solutions of heat and wave type equations on a locally compact separable unimodular group $G$ by using an integro-differential operator in time and any positive left invariant operator (maybe unbounded) on $G$. We complement our studies by giving asymptotic time estimates for the solutions, which in some cases are sharp.",
        "citation_title": "$L^p-L^q$ estimates for non-local heat and wave type equations on locally compact groups",
        "date_delivered": "[Submitted on 28 Apr 2024]"
    },
    {
        "abstract": "In 1993, the global stability of Minkowski spacetime has been proven in the celebrated work of Christodoulou and Klainerman \\cite{Ch-Kl}. In 2003, Klainerman and Nicol\u00f2 \\cite{Kl-Ni} revisited Minkowski stability in the exterior of an outgoing null cone. In \\cite{Shen23}, the author extended the results of \\cite{Ch-Kl} to minimal decay assumptions. In this paper, we prove that the exterior stability of Minkowski holds with decay which is borderline compared to the minimal decay considered in \\cite{Shen23}.",
        "citation_title": "Exterior stability of Minkowski spacetime with borderline decay",
        "date_delivered": "[Submitted on 29 Apr 2024]"
    },
    {
        "abstract": "This note is intended to explain the proof of two facts about quadrature domains: first, they are essentially unique if they exist; and second, they do exist for a large class of weight functions. The proofs roughly follow Sakai's \"Solutions to the obstacle problem as Green potentials,\" but are presented at an easier level.",
        "citation_title": "Note about the existence and essential uniqueness of quadrature domains",
        "date_delivered": "[Submitted on 29 Apr 2024]"
    },
    {
        "abstract": "We construct a new family of graded representations $\\widetilde{W}_{\\lambda}$ for the positive elliptic Hall algebra $\\mathcal{E}^{+}$ indexed by Young diagrams $\\lambda$ which generalize the standard $\\mathcal{E}^{+}$ action on symmetric functions. These representations have homogeneous bases of eigenvectors for the action of the Macdonald element $P_{0,1} \\in \\mathcal{E}^{+}$ with distinct $\\mathbb{Q}(q,t)$-rational spectrum generalizing the symmetric Macdonald functions. The analysis of the structure of these representations exhibits interesting combinatorics arising from the stable limits of periodic standard Young tableaux. We find an explicit combinatorial rule for the action of the multiplication operators $e_r[X]^{\\bullet}$ generalizing the Pieri rule for symmetric Macdonald functions. We will also naturally obtain a family of interesting $(q,t)$ product-series identities which come from keeping track of certain combinatorial statistics associated to periodic standard Young tableaux.",
        "citation_title": "Murnaghan-Type Representations for the Positive Elliptic Hall Algebra",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Courcelle's Theorem is an important result in graph theory, proving the existence of linear-time algorithms for many decision problems on graphs whose tree-width is bounded by a constant. The purpose of this text is twofold: to provide an explanation and step-by-step proof of Courcelle's Theorem as applied to graphs of tree-width bounded by a constant, and to show explicitly (on the example of path-width) how to apply the same principles to other graph classes. We present these topics in a way that does not assume any particular knowledge on the part of the reader except a basic understanding of mathematics and possibly the fundamentals of graph theory. Our hope is to make the topic accessible to a broader mathematical audience, to which end we have included extensive explanations and pretty pictures.",
        "citation_title": "Courcelle's Theorem: A Self-Contained Proof and a Path-Width Variant",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This thesis results from an intensive study on the canonical metrics on the Teichm\u00fcller spaces and the moduli spaces of Riemann surfaces. There are several renowned classical metrics on $T_g$ and $\\mathcal{M}_g$, including the Weil-Petersson metric, the Teichm\u00fcller metric, the Kobayashi metric, the Bergman metric, the Carath\u00e9odory metric and the K\u00e4hler-Einstein metric. The Teichm\u00fcller metric, the Kobayashi metric and the Carath\u00e9odory metric are only (complete) Finsler metrics, but they are effective tools in the study of hyperbolic property of $\\mathcal{M}_g$. The Weil-Petersson metric is an incomplete K\u00e4hler metric, while the Bergman metric and the K\u00e4hler-Einstein metric are complete K\u00e4hler metrics. However, McMullen introduced a new complete K\u00e4hler metric, called the McMullen metric, by perturbing the Weil-Petersson metric. This metric is indeed equivalent to the Teichm\u00fcller metric. Recently, Liu-Sun-Yau proved that the equivalence of the K\u00e4hler-Einstein metric to the Teichm\u00fcller metric, and hence gave a positive answer to a conjecture proposed by Yau. Their approach in the proof is to introduce two new complete K\u00e4hler metrics, namely, the Ricci metric and the perturbed Ricci metric, and then establish the equivalence of the Ricci metric to the K\u00e4hler-Einstein metric and the equivalence of the Ricci metric to the McMullen metric. The main purpose of this thesis is to survey the properties of these various metrics and the geometry of $T_g$ and $\\mathcal{M}_g$ induced by these metrics.",
        "citation_title": "Survey on the Canonical Metrics on the Teichm\u00fcller Spaces and the Moduli Spaces of Riemann Surfaces",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We establish a connection between problems studied in rigidity theory and matroids arising from linear algebraic constructions like tensor products and symmetric products. A special case of this correspondence identifies the problem of giving a description of the correctable erasure patterns in a maximally recoverable tensor code with the problem of describing bipartite rigid graphs or low-rank completable matrix patterns. Additionally, we relate dependencies among symmetric products of generic vectors to graph rigidity and symmetric matrix completion. With an eye toward applications to computer science, we study the dependency of these matroids on the characteristic by giving new combinatorial descriptions in several cases, including the first description of the correctable patterns in an (m, n, a=2, b=2) maximally recoverable tensor code.",
        "citation_title": "Rigidity matroids and linear algebraic matroids with applications to matrix completion and tensor codes",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We introduce the Rigged Dynamic Mode Decomposition (Rigged DMD) algorithm, which computes generalized eigenfunction decompositions of Koopman operators. By considering the evolution of observables, Koopman operators transform complex nonlinear dynamics into a linear framework suitable for spectral analysis. While powerful, traditional Dynamic Mode Decomposition (DMD) techniques often struggle with continuous spectra. Rigged DMD addresses these challenges with a data-driven methodology that approximates the Koopman operator's resolvent and its generalized eigenfunctions using snapshot data from the system's evolution. At its core, Rigged DMD builds wave-packet approximations for generalized Koopman eigenfunctions and modes by integrating Measure-Preserving Extended Dynamic Mode Decomposition with high-order kernels for smoothing. This provides a robust decomposition encompassing both discrete and continuous spectral elements. We derive explicit high-order convergence theorems for generalized eigenfunctions and spectral measures. Additionally, we propose a novel framework for constructing rigged Hilbert spaces using time-delay embedding, significantly extending the algorithm's applicability. We provide examples, including systems with a Lebesgue spectrum, integrable Hamiltonian systems, the Lorenz system, and a high-Reynolds number lid-driven flow in a two-dimensional square cavity, demonstrating Rigged DMD's convergence, efficiency, and versatility. This work paves the way for future research and applications of decompositions with continuous spectra.",
        "citation_title": "Rigged Dynamic Mode Decomposition: Data-Driven Generalized Eigenfunction Decompositions for Koopman Operators",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We prove a reverse Lieb--Thirring inequality with a sharp constant for the matrix Schr\u00f6dinger equation on the half-line.",
        "citation_title": "Reverse Lieb--Thirring inequality for the half-line matrix Schr\u00f6dinger operator",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Spectral estimation is a fundamental task in signal processing. Recent algorithms in quantum phase estimation are concerned with the large noise, large frequency regime of the spectral estimation problem. The recent work in Ding-Epperly-Lin-Zhang shows that the ESPRIT algorithm exhibits superconvergence behavior for the spike locations in terms of the maximum frequency. This note provides a perturbative analysis to explain this behavior. It also extends the discussion to the case where the noise grows with the sampling frequency.",
        "citation_title": "A perturbative analysis for noisy spectral estimation",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We consider the problem of the discrete-time approximation of the solution of a one-dimensional SDE with piecewise locally Lipschitz drift and continuous diffusion coefficients with polynomial growth. In this paper, we study the strong convergence of a (semi-explicit) exponential-Euler scheme previously introduced in Bossy et al. (2021). We show the usual 1/2 rate of convergence for the exponential-Euler scheme when the drift is continuous. When the drift is discontinuous, the convergence rate is penalised by a factor {$\\epsilon$} decreasing with the time-step. We examine the case of the diffusion coefficient vanishing at zero, which adds a positivity preservation condition and a convergence analysis that exploits the negative moments and exponential moments of the scheme with the help of change of time technique introduced in Berkaoui et al. (2008). Asymptotic behaviour and theoretical stability of the exponential scheme, as well as numerical experiments, are also presented.",
        "citation_title": "Strong convergence of the exponential Euler scheme for SDEs with superlinear growth coefficients and one-sided Lipschitz drift",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present extended Galerkin neural networks (xGNN), a variational framework for approximating general boundary value problems (BVPs) with error control. The main contributions of this work are (1) a rigorous theory guiding the construction of new weighted least squares variational formulations suitable for use in neural network approximation of general BVPs (2) an ``extended'' feedforward network architecture which incorporates and is even capable of learning singular solution structures, thus greatly improving approximability of singular solutions. Numerical results are presented for several problems including steady Stokes flow around re-entrant corners and in convex corners with Moffatt eddies in order to demonstrate efficacy of the method.",
        "citation_title": "Extended Galerkin neural network approximation of singular variational problems with error control",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Digital phased arrays have often been disregarded for millimeter-wave communications since the analog-to-digital converters (ADCs) are power-hungry. In this paper, we provide a different perspective on this matter by demonstrating analytically and numerically how the ADC resolution can be reduced when using digital phased arrays. We perform a theoretical analysis of the quantization noise characteristics for an OFDM signal received and processed by a digital phased array, using Gaussian approximation of the OFDM signal. In particular, we quantify the quantization noise suppression factor analytically and numerically. This factor describes how much the coherent combining reduces the quantization noise as a function of the number of antennas, which allows for reducing the ADC bit resolution. For instance in a 8-16 antenna digital phased array the ADC resolution can be reduced with 1-2 bits compared to the ADC required for an analog phased array.",
        "citation_title": "Analysis of Quantization Noise Suppression Gains in Digital Phased Arrays",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Although $S_\\omega$ (the group of all permutations of $\\mathbb{N}$) is size continuum, both it and its closed subgroups can be presented as the set of paths through a countable tree. The subgroups of $S_\\omega$ that can be presented this way with finite branching trees are exactly the profinite ones. We use these tree presentations to find upper bounds on the complexity of the existential theories of profinite subgroups of $S_\\omega$, as well as to prove sharpness for these bounds. These complexity results enable us to distinguish a simple subclass of profinite groups, those with \\emph{orbit independence}, for which we find an upper bound on the complexity of the entire first order theory.",
        "citation_title": "Complexities of Theories of Profinite Subgroups of $S_\u03c9$ via Tree Presentations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In the problem of quickest change detection (QCD), a change occurs at some unknown time in the distribution of a sequence of independent observations. This work studies a QCD problem where the change is either a bad change, which we aim to detect, or a confusing change, which is not of our interest. Our objective is to detect a bad change as quickly as possible while avoiding raising a false alarm for pre-change or a confusing change. We identify a specific set of pre-change, bad change, and confusing change distributions that pose challenges beyond the capabilities of standard Cumulative Sum (CuSum) procedures. Proposing novel CuSum-based detection procedures, S-CuSum and J-CuSum, leveraging two CuSum statistics, we offer solutions applicable across all kinds of pre-change, bad change, and confusing change distributions. For both S-CuSum and J-CuSum, we provide analytical performance guarantees and validate them by numerical results. Furthermore, both procedures are computationally efficient as they only require simple recursive updates.",
        "citation_title": "Quickest Change Detection with Confusing Change",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We prove two rigidity theorems for open (complete and noncompact) $n$-manifolds $M$ with nonnegative Ricci curvature and the infimum of volume growth order $<2$. The first theorem asserts that the Riemannian universal cover of $M$ has Euclidean volume growth if and only if $M$ is flat with an $n-1$ dimensional soul. The second theorem asserts that there exists a nonconstant linear growth harmonic function on $M$ if and only if $M$ is isometric to the metric product $\\mathbb{R}\\times N$ for some compact manifold $N$.",
        "citation_title": "On manifolds with nonnegative Ricci curvature and the infimum of volume growth order $<2$",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We prove that for any pair of Legendrian representatives of the Chekanov-Eliashberg twist knots with different LOSS invariants, any negative rational contact $r$-surgery with $r\\neq -1$ always gives rise to different contact 3-manifolds distinguished by their contact invariants. This gives the first examples of pairs ofLegendrian knots with the same classical invariants but distinct contact $r$-surgeries for all negative rational number $r$. We also generalize the statement from the twist knots to a certain families of two bridge knots.",
        "citation_title": "Negative contact surgery on Legendrian non-simple knots",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We consider the Euclidean vacuum for linearized gravity on the global de Sitter space, obtained from the Euclidean Green's function on the 4-sphere. We use the notion of Calder\u00f3n projectors to recover a quantum state for the Lorentzian theory on de Sitter space. We show that while the state is gauge invariant and Hadamard, it is not positive on the whole of the phase space. We show however that a suitable modification at low energies yields a well-defined Hadamard state on global de Sitter space.",
        "citation_title": "IR-fixed Euclidean vacuum for linearized gravity on de Sitter space",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We establish upper bounds on the size of the largest subset of $\\{1,2,\\dots,N\\}$ lacking nonzero differences of the form $h(p_1,\\dots,p_{\\ell})$, where $h\\in \\mathbb{Z}[x_1,\\dots,x_{\\ell}]$ is a fixed polynomial satisfying appropriate conditions and $p_1,\\dots,p_{\\ell}$ are prime. The bounds are of the same type as the best-known analogs for unrestricted integer inputs, due to Bloom-Maynard and Arala for $\\ell=1$, and to the authors for $\\ell \\geq 2$.",
        "citation_title": "The Furstenberg-S\u00e1rk\u00f6zy theorem for polynomials in one or more prime variables",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "On a K\u00e4hler manifold we consider the problems of maximizing/minimizing Monge--Amp\u00e8re energy over certain subsets of the space of K\u00e4hler potentials. Under suitable assumptions we prove that solutions to these variational problems exist, are unique, and have a simple characterization. We then use the extremals to construct hermitian metrics on holomorphic vector bundles, and investigate their curvature.",
        "citation_title": "Two variational problems in K\u00e4hler geometry",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We construct several families of functors on the homotopy category of singular Soergel bimodules that mimic cabling and insertion of column-colored projectors. We use these functors to identify the intrinsically-colored homology of Webster--Williamson and the projector-colored homology of Elias--Hogancamp for an arbitrary link, up to multiplication by a polynomial in the quantum degree $q$. Combined with the results of arXiv:2303.16271, this establishes parity results for the intrinsic column-colored homology of positive torus knots, partially resolving a conjecture of Hogancamp--Rose--Wedrich in arXiv:2107.09590.",
        "citation_title": "Fray functors and equivalence of colored HOMFLYPT homologies",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present both bijective and automated approaches to Abel-type sums, dear to Dominique Foata.",
        "citation_title": "Bijective and Automated Approaches to Abel Sums",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "When $p$ is an odd prime, we prove that the $\\mathbb F_p$-cohomology of $\\mathrm{BP}\\langle n\\rangle$ as a module over the Steenrod algebra determines the $p$-local spectrum $\\mathrm{BP}\\langle n\\rangle$. In particular, we prove that the $p$-local spectrum $\\mathrm{BP}\\langle n\\rangle$ only depends on its $p$-completion $\\mathrm{BP}\\langle n\\rangle_p^\\wedge$. As a corollary, this proves that the $p$-local homotopy type of $\\mathrm{BP}\\langle n\\rangle$ does not depend on the ideal by which we take the quotient of $\\mathrm{BP}$. In the course of the argument, we show that there is a vanishing line for odd degree classes in the Adams spectral sequence for endomorphisms of $\\mathrm{BP}\\langle n\\rangle$. We also prove that there are enough endomorphisms of $\\mathrm{BP}\\langle n\\rangle$ in a suitable sense. When $p=2$, we obtain the results for $n\\leq 3$.",
        "citation_title": "Uniqueness of $p$-local truncated Brown-Peterson spectra",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper presents a particle-based optimization method designed for addressing minimization problems with equality constraints, particularly in cases where the loss function exhibits non-differentiability or non-convexity. The proposed method combines components from consensus-based optimization algorithm with a newly introduced forcing term directed at the constraint set. A rigorous mean-field limit of the particle system is derived, and the convergence of the mean-field limit to the constrained minimizer is established. Additionally, we introduce a stable discretized algorithm and conduct various numerical experiments to demonstrate the performance of the proposed method.",
        "citation_title": "An interacting particle consensus method for constrained global optimization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We consider the asymptotic behavior of solutions to the convection-diffusion equation: \\[ \\partial_t u - \\mathrm{div}\\left(a(x)\\nabla u\\right) = d\\cdot\\nabla \\left(\\left\\lvert u\\right\\rvert ^{q-1}u\\right), \\ \\ x \\in \\mathbb{R}^n, \\ t>0 \\] with an integrable initial data $u_{0}(x)$, where $n\\ge1$, $q>1+\\frac{1}{n}$ and $d\\in \\mathbb{R}^{n}$. Moreover, we take $a(x)=1+b(x)>0$, where $b(x)$ is smooth and decays fast enough at spatial infinity. It is known that the asymptotic profile of the solution to this problem can be given by the heat kernel. Moreover, the second asymptotic profile of the solution have already been studied. In particular, the following three cases are distinguished: $1+\\frac{1}{n}<q<1+\\frac{2}{n}$; $q=1+\\frac{2}{n}$; $q>1+\\frac{2}{n}$. More precisely, the second asymptotic profile has different properties in each of these three cases. In this paper, we focus on the critical case of $q=1+\\frac{2}{n}$. By analyzing the corresponding integral equation in details, we have succeeded to give the more higher-order asymptotic expansion of the solution, which generalizes the previous works.",
        "citation_title": "Higher-order asymptotic profiles of solutions to the Cauchy problem for the convection-diffusion equation with variable diffusion",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Solutions to the governing partial differential equations obtained from a discrete numerical scheme can have significant errors, especially near shocks when the discrete representation of the solution cannot fully capture the discontinuity in the solution. A recent approach to shock tracking [1, 2] has been to implicitly align the faces of mesh elements with the shock, yielding accurate solutions on coarse meshes. In engineering applications, the solution field is often used to evaluate a scalar functional of interest, such as lift or drag over an airfoil. While functionals are sensitive to errors in the flow solution, certain regions in the domain are more important for accurate evaluation of the functional than the rest. Using this fact, we formulate a goal-oriented implicit shock tracking approach that captures a segment of the shock that is important for evaluating the functional. Shock tracking is achieved using Lagrange-Newton-Krylov-Schur (LNKS) full space optimizer, with the objective of minimizing the adjoint-weighted residual error indicator. We also present a method to evaluate the sensitivity and the Hessian of the functional error. Using available block preconditioners for LNKS [3, 4] makes the full space approach scalable. The method is applied to test cases of two-dimensional advection and inviscid compressible flows to demonstrate functional-dependent shock tracking. Tracking the entire shock without using artificial dissipation results in the error converging at the orders of $\\mathcal{O}(h^{p+1})$.",
        "citation_title": "Adjoint-based goal-oriented implicit shock tracking using full space mesh optimization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study the problem of stabilizing infinite-dimensional systems with input and output quantization. The closed-loop system we consider is subject to packet loss in the sensor-to-controller channels, whose duration is assumed to be averagely bounded. Given a bound on the initial state, we propose design methods for dynamic quantizers with zoom parameters. We show that the closed-loop state staring in a given region exponentially converges to zero if the bounds of quantization errors and packet-loss duration satisfy suitable conditions. Since the norms of the operators representing the system dynamics are used in the proposed quantization schemes, we also present methods for approximately computing the operator norms.",
        "citation_title": "Stabilization of infinite-dimensional systems under quantization and packet loss",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper presents a new algorithm member for accelerating first-order methods for bilevel optimization, namely the \\emph{(Perturbed) Restarted Accelerated Fully First-order methods for Bilevel Approximation}, abbreviated as \\texttt{(P)RAF${}^2$BA}. The algorithm leverages \\emph{fully} first-order oracles and seeks approximate stationary points in nonconvex-strongly-convex bilevel optimization, enhancing oracle complexity for efficient optimization. Theoretical guarantees for finding approximate first-order stationary points and second-order stationary points at the state-of-the-art query complexities are established, showcasing their effectiveness in solving complex optimization tasks. Empirical studies for real-world problems are provided to further validate the outperformance of our proposed algorithms. The significance of \\texttt{(P)RAF${}^2$BA} in optimizing nonconvex-strongly-convex bilevel optimization problems is underscored by its state-of-the-art convergence rates and computational efficiency.",
        "citation_title": "Accelerated Fully First-Order Methods for Bilevel and Minimax Optimization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The pro-$p$ Iwahori-Hecke $\\operatorname{Ext}$-algebra $E^\\ast$ is a graded algebra that has been introduced and studied by Ollivier-Schneider, with the long-term goal of investigating the category of smooth mod-$p$ representations of $p$-adic reductive groups and its derived category. Its $0$th graded piece is the pro-$p$ Iwahori-Hecke algebra studied by Vign\u00e9ras and others.\nIn the present article, we first show that the $\\operatorname{Ext}$-algebra $E^\\ast$ associated with the group $\\operatorname{SL}_2(\\mathfrak{F})$, $\\operatorname{PGL}_2(\\mathfrak{F})$ or $\\operatorname{GL}_2(\\mathfrak{F})$, where $\\mathfrak{F}$ is an unramified extension of $\\mathbb{Q}_p$ with $p \\neq 2,3$, is finitely generated as a (non-commutative) algebra.\nWe then specialize to the case of the group $\\operatorname{SL}_2(\\mathbb{Q}_p)$, with $p \\neq 2,3$, and we show that in this case the natural multiplication map from the tensor algebra $T^\\ast_{E^0} E^1$ to $E^\\ast$ is surjective and that its kernel is finitely generated as a two-sided ideal. Using this fact as main input, we then show that $E^\\ast$ is finitely presented as an algebra. We actually compute an explicit presentation.",
        "citation_title": "Finite generation properties of the pro-$p$ Iwahori-Hecke $\\operatorname{Ext}$-algebra",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Let $H_n^{(3)}$ be a 3-uniform linear hypergraph, i.e. any two edges have at most one vertex common. A special hypergraph, {\\em wicket}, is formed by three rows and two columns of a $3 \\times 3$ point matrix. In this note, we give a new lower bound on the Tur\u00e1n number of wickets using estimates on cap sets.",
        "citation_title": "Caps and Wickets",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Let $M$ be a complete Riemannian manifold satisfying a weighted Poincar\u00e9 inequality, and let $\\mathcal{E}$ be a Hermitian vector bundle over $M$ equipped with a metric covariant derivative $\\nabla$. We consider the operator $H_{X,V}=\\nabla^{\\dagger}\\nabla+\\nabla_{X}+ V$, where $\\nabla^{\\dagger}$ is the formal adjoint of $\\nabla$ with respect to the inner product in the space of square-integrable sections of $\\mathcal{E}$, $X$ is a smooth (real) vector field on $M$, and $V$ is a fiberwise self-adjoint, smooth section of the endomorphism bundle $\\textrm{End }\\mathcal{E}$. We give a sufficient condition for the triviality of the $L^2$-kernel of $H_{X,V}$. As a corollary, putting $X\\equiv 0$ and working in the setting of a Clifford bundle equipped with a Clifford connection $\\nabla$, we obtain the triviality of the $L^2$-kernel of $D^2$, where $D$ is the Dirac operator corresponding to $\\nabla$. In particular, when $\\mathcal{E}=\\Lambda^{k}T^*M$ and $D^2$ is the Hodge--deRham Laplacian on $k$-forms, we recover some recent vanishing results for $L^2$-harmonic $k$-forms.",
        "citation_title": "Covariant Schr\u00f6dinger Operator and $L^2$-Vanishing Property on Riemannian Manifolds",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Dedicated to the memory of Professor Tze Leung Lai, this paper introduces three multi-hypothesis sequential tests. These tests are derived from one-sided versions of the sequential probability ratio test and its modifications. They are proven to be first-order asymptotically optimal for testing simple and parametric composite hypotheses when error probabilities are small. These tests exhibit near optimality properties not only in classical i.i.d. observation models but also in general non-i.i.d. models, provided that the log-likelihood ratios between hypotheses converge r-completely to positive and finite numbers. These findings extend the seminal work of Lai (1981) on two hypotheses.",
        "citation_title": "Nearly Optimum Properties of Certain Multi-Decision Sequential Rules for General Non-i.i.d. Stochastic Models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "For a fixed integer $d\\geq 1$, we show that two quasitoric manifolds over a product of $d$-simplices are homotopy equivalent after appropriate localization, provided that their integral cohomology rings are isomorphic.",
        "citation_title": "Homotopy rigidity for quasitoric manifolds over a product of $d$-simplices",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "When solving systems of banded Toeplitz equations or calculating their inverses, it is necessary to determine the invertibility of the matrices beforehand. In this paper, we equate the invertibility of an $n$-order banded Toeplitz matrix with bandwidth $2k+1$ to that of a small $k*k$ matrix. By utilizing a specially designed algorithm, we compute the invertibility sequence of a class of banded Toeplitz matrices with a time complexity of $5k^2n/2+kn$ and a space complexity of $3k^2$ where $n$ is the size of the largest matrix. This enables efficient preprocessing when solving equation systems and inverses of banded Toeplitz matrices.",
        "citation_title": "Efficient Computation for Invertibility Sequence of Banded Toeplitz Matrices",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This study aims to examine the effect of L\u00e9vy noise on the solutions of the nonlinear Schr\u00f6dinger equation. An improved diversity of stochastic solutions is instinctively located discretely on certain conditions by applying the generalized Kudryashov method. Moreover, the dynamical behaviors of these exact results of the nonlinear Schr\u00f6dinger equation are interpreted in the context of the effect of L\u00e9vy noise. Even mathematical evaluations have been conducted and presented.",
        "citation_title": "On the solitary wave configurations of nonlinear Schr\u00f6dinger equation under the effect of L\u00e9vy noise",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The classical Gagliardo-Nirenberg inequality, known as an interpolation inequality, involves Lebesgue norms of functions and their derivatives. We established an interpolation lemma to connect Lebesgue and H\u00f6lder spaces, thus extending the Gagliardo-Nirenberg inequality. This extension involved substituting arbitrary Sobolev norms with appropriate H\u00f6lder norms, allowing for a wider range of applicable parameters in the inequality.",
        "citation_title": "Gagliardo-Nirenberg inequality with H\u00f6lder norms",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Motivated by the ideal peak-to-average-power ratio and radar sensing capability of traditional frequency-coded radar waveforms, this paper considers the frequency shift keying (FSK) based waveform for joint communications and radar (JCR). An analysis of the probability distributions of its ambiguity function (AF) sidelobe levels (SLs) and peak sidelobe level (PSL) is conducted to study the radar sensing capability of random FSK. Numerical results show that the independent frequency modulation introduces uncontrollable AF PSLs. In order to address this problem, the initial phases of waveform sub-pulses are designed by solving a min-max optimisation problem. Numerical results indicate that the optimisation-based phase design can effectively reduce the AF PSL to a level close to well-designed radar waveforms while having no impact on the data rate and the receiver complexity. For large numbers of waveform sub-pulses and modulation orders, the impact on the error probability is also insignificant.",
        "citation_title": "Can FSK Be Optimised for Integrated Sensing and Communications?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study how high charging rate demands from electric vehicles (EVs) in a power distribution grid may collectively cause its dynamic instability, and, accordingly, how a price incentivization strategy can be used to steer customers to settle for lesser charging rate demands so that these instabilities can be avoided. We pose the problem as a joint optimization and optimal control formulation. The optimization determines the optimal charging setpoints for EVs to minimize the $\\mathcal{H}_2$-norm of the transfer function of the grid model, while the optimal control simultaneously develops a linear quadratic regulator (LQR) based state-feedback control signal for the battery-currents of those EVs to jointly minimize the risk of grid instability. A subsequent algorithm is developed to determine how much customers may be willing to sacrifice their intended charging rate demands in return for financial incentives. Results are derived for both unidirectional and bidirectional charging, and validated using numerical simulations of multiple EV charging stations in the IEEE 33-bus power distribution model.",
        "citation_title": "Co-Optimization of EV Charging Control and Incentivization for Enhanced Power System Stability",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A $f\\colon\\mathbb{R}\\to\\mathbb{R}$ is called Hamel function if its graph is a Hamel basis of the linear space $\\mathbb{R}^2$ over rationals. We construct, assuming CH, a free group of the size $2^\\mathfrak{c}$ contained in the class of all Hamel functions, with the indentity function included. This answers, consistently, a question posed recently by M. Lichman, M. Pawlikowski, S. Smolarek, and J. Swaczyna.",
        "citation_title": "Free group of Hamel bijections of big size",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We construct a commutative orthogonal $C_2$-ring spectrum, $\\mathrm{MSpin}^c_{\\mathbb{R}}$, along with a $C_2$-$E_{\\infty}$-orientation $\\mathrm{MSpin}^c_{\\mathbb{R}} \\to \\mathrm{KU}_{\\mathbb{R}}$ of Atiyah's Real K-theory. Further, we define $E_{\\infty}$-maps $\\mathrm{MSpin} \\to (\\mathrm{MSpin}^c_{\\mathbb{R}})^{C_2}$ and $\\mathrm{MU}_{\\mathbb{R}} \\to \\mathrm{MSpin}^c_{\\mathbb{R}}$, which are used to recover the three well-known orientations of topological $\\mathrm{K}$-theory, $\\mathrm{MSpin}^c \\to \\mathrm{KU}$, $\\mathrm{MSpin} \\to \\mathrm{KO}$, and $\\mathrm{MU}_{\\mathbb{R}} \\to \\mathrm{KU}_{\\mathbb{R}}$, from the map $\\mathrm{MSpin}^c_{\\mathbb{R}} \\to \\mathrm{KU}_{\\mathbb{R}}$. We also show that the integrality of the $\\hat{A}$-genus on spin manifolds provides an obstruction for the fixed points $(\\mathrm{MSpin}^c_{\\mathbb{R}})^{C_2}$ to be equivalent to $\\mathrm{MSpin}$, using the Mackey functor structure of $\\underline{\\pi}_*\\mathrm{MSpin}^c_{\\mathbb{R}}$. In particular, the usual map $\\mathrm{MSpin} \\to \\mathrm{MSpin}^c$ does not arise as the inclusion of fixed points for any $C_2$-$E_{\\infty}$-ring spectrum.",
        "citation_title": "Real spin bordism and orientations of topological $\\mathrm{K}$-theory",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The links between the mean families of Lehmer and H\u00f6lder and the weighted maximum likelihood estimator have recently been established in the case of a regular univariate exponential family. In this article, we will extend the outcomes obtained to the multivariate case. This extension provides a probabilistic interpretation of these families of means and could therefore broaden their uses in various applications.",
        "citation_title": "Deriving Lehmer and H\u00f6lder means as maximum weighted likelihood estimates for the multivariate exponential family",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recently, deep neural networks have been found to nearly interpolate training data but still generalize well in various applications. To help understand such a phenomenon, it has been of interest to analyze the ridge estimator and its interpolation limit in high-dimensional regression models. For this motivation, we study the ridge estimator in a rotationally sparse setting of high-dimensional linear regression, where the signal of a response is aligned with a small number, $d$, of covariates with large or spiked variances, compared with the remaining covariates with small or tail variances, \\textit{after} an orthogonal transformation of the covariate vector. We establish high-probability upper and lower bounds on the out-sample and in-sample prediction errors in two distinct regimes depending on the ratio of the effective rank of tail variances over the sample size $n$. The separation of the two regimes enables us to exploit relevant concentration inequalities and derive concrete error bounds without making any oracle assumption or independent components assumption on covariate vectors. Moreover, we derive sufficient and necessary conditions which indicate that the prediction errors of ridge estimation can be of the order $O(\\frac{d}{n})$ if and only if the gap between the spiked and tail variances are sufficiently large. We also compare the orders of optimal out-sample and in-sample prediction errors and find that, remarkably, the optimal out-sample prediction error may be significantly smaller than the optimal in-sample one. Finally, we present numerical experiments which empirically confirm our theoretical findings.",
        "citation_title": "On Ridge Estimation in High-dimensional Rotationally Sparse Linear Regression",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A critical hindrance to realize frequency division duplex (FDD) massive multi-input multi-output (MIMO) systems is overhead associated with downlink channel state information at the transmitter (CSIT) acquisition. To address this challenge, we propose a novel framework that achieves robust performances while completely eliminating downlink CSIT training and feedback. Specifically, by exploiting partial frequency invariance of channel parameters between the uplink (UL) and downlink (DL), we adopt the 2D-Newtonized orthogonal matching pursuit (2D-NOMP) algorithm to reconstruct DL CSIT from UL training. Due to inherent discrepancies arising from a carrier frequency difference between two disjoint bands, however, the multi-user interference is inevitable. To overcome this, we propose a precoding method that employs rate-splitting multiple access (RSMA) and also develop an error covariance matrix (ECM) estimation method by using the observed Fisher information matrix (O-FIM). We find that this ECM estimation is crucial for our precoding design in maximizing the sum spectral efficiency (SE). Simulation results show that our method significantly improves the sum SE compared to other state-of-the-art approaches, underscoring the importance of our ECM estimation.",
        "citation_title": "Splitting Messages in the Dark- Rate-Splitting Multiple Access for FDD Massive MIMO Without CSI Feedback",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the double homology associated to triangulated spheres and present two results. First, we explicitly compute the double homology for minimum degree sphere triangulations. Using a spectral sequence argument, we compute the effect of removing a maximal simplex of a non-neighborly sphere triangulation. Using these results and computational aid we generate complexes with exotic double homology rank.",
        "citation_title": "Sphere Triangulations and their Double Homology",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We prove a descriptive version of Brooks's theorem for directed graphs. In particular, we show that, if $D$ is a Borel directed graph on a standard Borel space $X$ such that the maximum degree of each vertex is at most $d \\geq 3$, then unless $D$ contains the complete symmetric directed graph on $d + 1$ vertices, $D$ admits a $\\mu$-measurable $d$-dicoloring with respect to any Borel probability measure $\\mu$ on $X$, and $D$ admits a $\\tau$-Baire-measurable $d$-dicoloring with respect to any Polish topology $\\tau$ compatible with the Borel structure on $X$. We also prove a definable version of Gallai's theorem on list dicolorings for directed graphs by showing that any Borel directed graph of bounded degree whose connected components are not Gallai trees is Borel degree-list-dicolorable.",
        "citation_title": "Measurable Brooks's Theorem for Directed Graphs",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we compute the dual $F$-signatures of certain toric rings by using combinatorial techniques. Specifically, we calculate the dual $F$-signatures of Veronese subrings of polynomial rings. Moreover, we give an upper bound for the dual $F$-signatures of Segre products of polynomial rings and show that this upper bound is attained in some cases.",
        "citation_title": "Dual $F$-signatures of Veronese subrings and Segre products of polynomial rings",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we formulate a conjecture on joint distribution of Hecke--Maass cusp forms. To support our conjecture, we prove two conditional results on joint moments of two Hecke--Maass cusp forms, which confirms statistical independence of orthogonal cusp forms.",
        "citation_title": "Joint value distribution of Hecke--Maass forms",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Rapid advancement of antenna technology catalyses the popularization of extremely large-scale multiple-input multiple-output (XL-MIMO) antenna arrays, which pose unique challenges for localization with the inescapable near-field effect. In this paper, we propose an efficient near-field localization algorithm by leveraging a sectored uniform circular array (sUCA). In particular, we first customize a backprojection algorithm in the polar coordinate for sUCA-enabled near-field localization, which facilitates the target detection procedure. We then analyze the resolutions in both angular and distance domains via deriving the interval of zero-crossing points, and further unravel the minimum required number of antennas to eliminate grating lobes. The proposed localization method is finally implemented using fast Fourier transform (FFT) to reduce computational complexity. Simulation results verify the resolution analysis and demonstrate that the proposed method remarkably outperforms conventional localization algorithms in terms of localization accuracy. Moreover, the low-complexity FFT implementation achieves an average runtime that is hundreds of times faster when large numbers of antenna elements are employed.",
        "citation_title": "Low-Complexity Near-Field Localization with XL-MIMO Sectored Uniform Circular Arrays",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We investigate the continuous extension of discrete shift translations on one dimensional quantum lattice systems. We focus on a specific construction provided by a quasi-free C*-flow on the one-dimensional fermion lattice system. This quasi-free C*-flow is heuristically generated by a long-range Hamiltonian consisting of two-body translation invariant interactions with $\\frac{1}{r}$-decay. We explore statistical-mechanical interpretation of such a long-range Hamiltonian, which may be more naturally associated with a momentum operator rather than the Hamiltonian. Through its explicit dynamical formulas, wherein notably the sinc function appears, the continuous shift translations reveal violations of causality and locality. Furthermore, we demonstrate that this quasi-free C*-flow, implementing the shift translations, cannot be extended to the one-dimensional quantum spin system via the Jordan-Wigner transformation.",
        "citation_title": "Continuous extension of the discrete shift translations on one-dimensional quantum lattice systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The computational complexity of tiling finite simply connected regions with a fixed set of tiles is studied in this paper. We show that the problem of tiling simply connected regions with a fixed set of $23$ Wang tiles is NP-complete. As a consequence, the problem of tiling simply connected regions with a fixed set of $111$ rectangles is NP-complete. Our results improve that of Igor Pak and Jed Yang by using fewer numbers of tiles. Notably in the case of Wang tiles, the number has decreased by more than one third from $35$ to $23$.",
        "citation_title": "NP-completeness of Tiling Finite Simply Connected Regions with a Fixed Set of Wang Tiles",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We characterize those pairs $(\\psi,\\varphi)$ of smooth mappings $\\psi:\\mathbb{R}^d\\rightarrow\\mathbb{C},\\varphi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$ for which the corresponding weighted composition operator $C_{\\psi,\\varphi}f=\\psi\\cdot(f\\circ\\varphi)$ acts continuously on $\\mathscr{S}(\\mathbb{R}^d)$. Additionally, we give several easy-to-check necessary and sufficient conditions of this property for interesting special cases. Moreover, we characterize power boundedness and topologizablity of $C_{\\psi,\\varphi}$ on $\\mathscr{S}(\\mathbb{R}^d)$ in terms of $\\psi,\\varphi$. Among other things, as an application of our results we show that for a univariate polynomial $\\varphi$ with $\\text{deg}(\\varphi)\\geq 2$, power boundedness of $C_{\\psi,\\varphi}$ on $\\mathscr{S}(\\mathbb{R})$ for every $\\psi\\in\\mathscr{O}_M(\\mathbb{R})$ only depends on $\\varphi$ and that in this case power boundedness of $C_{\\psi,\\varphi}$ is equivalent to $(C_{\\psi,\\varphi}^n)_{n\\in\\mathbb{N}}$ converging to $0$ in $\\mathcal{L}_b(\\mathscr{S}(\\mathbb{R}))$ as well as to the uniform mean ergodicity of $C_{\\psi,\\varphi}$. Additionally, we give an example of a power bounded and uniformly mean ergodic weighted composition operator $C_{\\psi,\\varphi}$ on $\\mathscr{S}(\\mathbb{R})$ for which neither the multiplication operator $f\\mapsto \\psi f$ nor the composition operator $f\\mapsto f\\circ\\varphi$ acts on $\\mathscr{S}(\\mathbb{R})$. Our results complement and considerably extend various results of Fern\u00e1ndez, Galbis, and the second named author.",
        "citation_title": "Power boundedness and related properties for weighted composition operators on $\\mathscr{S}(\\mathbb{R}^d)$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The unitary Cayley graph has vertex set $\\Zl_n =\\{0,1, \\hdots ,n-1\\}$, where two vertices $u$ and $v$ are adjacent if $\\text{gcd}(u - v, n) = 1$. We focus on the periodicity and perfect state transfer of Grover walk on the unitary Cayley graphs. We completely characterize which unitary Cayley graphs are periodic. From our results, we find infinitely many new periodic graphs. We prove that periodicity is a necessary condition for perfect state transfer on an integral vertex-transitive graph, and we provide a simple criterion to characterize perfect state transfer on circulant graphs in terms of its adjacency spectrum. Using these, we prove only three graphs in a class of unitary Cayley graphs exhibit perfect state transfer. Also, we provide a spectral characterization of the periodicity of Grover walk on any integral regular graphs.",
        "citation_title": "Perfect state transfer of Grover walks on unitary Cayley graphs",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Generalized Linear Mixed Models (GLMMs) are widely used for analysing clustered data. One well-established method of overcoming the integral in the marginal likelihood function for GLMMs is penalized quasi-likelihood (PQL) estimation, although to date there are few asymptotic distribution results relating to PQL estimation for GLMMs in the literature. In this paper, we establish large sample results for PQL estimators of the parameters and random effects in independent-cluster GLMMs, when both the number of clusters and the cluster sizes go to infinity. This is done under two distinct regimes: conditional on the random effects (essentially treating them as fixed effects) and unconditionally (treating the random effects as random). Under the conditional regime, we show the PQL estimators are asymptotically normal around the true fixed and random effects. Unconditionally, we prove that while the estimator of the fixed effects is asymptotically normally distributed, the correct asymptotic distribution of the so-called prediction gap of the random effects may in fact be a normal scale-mixture distribution under certain relative rates of growth. A simulation study is used to verify the finite sample performance of our theoretical results.",
        "citation_title": "Asymptotic Results for Penalized Quasi-Likelihood Estimation in Generalized Linear Mixed Models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The enhanced power graph of a group $G$ is a graph with vertex set $G,$ where two distinct vertices $x$ and $y$ are adjacent if and only if there exists an element $w$ in $G$ such that both $x$ and $y$ are powers of $w.$ In this paper, we determine the vertex connectivity of the enhanced power graph of any finite nilpotent group.",
        "citation_title": "An exact enumeration of vertex connectivity of the enhanced power graphs of finite nilpotent groups",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we are concerned with normalized solutions in $H_{r}^{1}(\\mathbb{R}^{3}) \\times H_{r}^{1}(\\mathbb{R}^{3})$ for Hartree-Fock type systems with the form \\be\\lab{ Hartree-Fock} \\left\\{ \\begin{array}{ll} -\\Delta u +\\alpha \\phi _{u,v} u=\\lambda _{1} u+\\left | u \\right | ^{2q-2} u+\\beta \\left | v \\right | ^{q} \\left | u \\right | ^{q-2} u , \\\\ -\\Delta v +\\alpha \\phi _{u,v} v=\\lambda _{2} v+\\left | v\\right | ^{2q-2} v+\\beta \\left | u \\right | ^{q} \\left | v \\right | ^{q-2} v , \\\\ \\int_{\\mathbb{R}^{3}}\\left | u \\right | ^{2} {\\rm d}x=a_{1} , \\quad \\int_{\\mathbb{R}^{3}}\\left | v \\right | ^{2} {\\rm d}x=a_{2} , \\nonumber\\\\ \\end{array} where $$ \\phi_{u, v}\\left(x\\right):=\\int_{\\mathbb{R}^{3}} \\frac{u^{2}(y)+v^{2}(y)}{|x-y|} {\\rm d}y \\in D^{1,2}\\left(\\mathbb{R}^{3}\\right). $$ Here $\\alpha,\\beta>0, a_1,a_2>0$ and $1<q<\\frac{5}{3}$. By seeking the constrained global minimizers of the corresponding functional, we prove that the existence of normalized solutions to the system above for any $a_1,a_2>0$ when $1<q<\\frac{4}{3}$ and for $a_1,a_2>0$ small when $\\frac{4}{3}\\le q < \\frac{3}{2}$. The nonexistence of normalized solutions is also considered for $\\frac{3}{2}\\le q < \\frac{5}{3}$. Also, the orbital stability of standing waves is obtained under local well-posedness assumptions of the evolution problem.",
        "citation_title": "Existence of normalized solutions of a Hartree-Fock system with mass subcritical growth",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We investigate a system of geometric evolution equations describing a curvature and torsion driven motion of a family of 3D curves in the normal and binormal directions. We explore the direct Lagrangian approach for treating the geometric flow of such interacting curves. Using the abstract theory of nonlinear analytic semi-flows, we are able to prove local existence, uniqueness, and continuation of classical H\u00f6lder smooth solutions to the governing system of non-linear parabolic equations modelling $n$ evolving curves with mutual nonlocal interactions. We present several computational studies of the flow that combine the normal or binormal velocity and considering nonlocal interaction.",
        "citation_title": "Evolution of multiple closed knotted curves in space",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Reed-Solomon (RS) codes are constructed over a finite field that have been widely employed in storage and communication systems. Many fast encoding/decoding algorithms such as fast Fourier transform (FFT) and modular approach are designed for RS codes to reduce the encoding/decoding complexity defined as the number of XORs involved in the encoding/decoding procedure. In this paper, we present the construction of RS codes over the cyclic polynomial ring $ \\mathbb{F}_2[x]/(1+x+\\ldots+x^{p-1})$ and show that our codes are maximum distance separable (MDS) codes. Moreover, we propose the FFT and modular approach over the ring that can be employed in our codes for encoding/decoding complexity reduction. We show that our codes have 17.9\\% encoding complexity reduction and 7.5\\% decoding complexity reduction compared with RS codes over finite field, for $(n,k)=(2048,1984)$.",
        "citation_title": "Reed-Solomon Codes over Cyclic Polynomial Ring with Lower Encoding/Decoding Complexity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider stochastic mSQG (modified Surface Quasi-Geostrophic) equations with multiplicative transport noise of Kraichnan type, and $L^p$-initial conditions. Inspired by the recent work of Coghi and Maurelli [arXiv:2308.03216], we show weak existence and pathwise uniqueness of solutions to the equations for suitable choices of parameters in the nonlinearity, the noise and the integrability of initial data.",
        "citation_title": "Well-posedness of stochastic mSQG equations with Kraichnan noise and $L^p$ data",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper studies a class of network games with linear-quadratic payoffs and externalities exerted through a strictly concave interaction function. This class of game is motivated by the diminishing marginal effects with peer influences. We analyze the optimal pricing strategy for this class of network game. First, we prove the existence of a unique Nash Equilibrium (NE). Second, we study the optimal pricing strategy of a monopolist selling a divisible good to agents. We show that the optimal pricing strategy, found by solving a bilevel optimization problem, is strictly better when the monopolist knows the network structure as opposed to the best strategy agnostic to network structure. Numerical experiments demonstrate that in most cases, the maximum revenue is achieved with an asymmetric network. These results contrast with the previously studied case of linear interaction function, where a network-independent price is proven optimal with symmetric networks. Lastly, we describe an efficient algorithm to find the optimal pricing strategy.",
        "citation_title": "Optimal Pricing for Linear-Quadratic Games with Nonlinear Interaction Between Agents",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We introduce and study a generalization $s_{(\\mu|\\lambda)}$ of the Schur functions called the almost symmetric Schur functions. These functions simultaneously generalize the finite variable key polynomials and the infinite variable Schur functions. They form a homogeneous basis for the space of almost symmetric functions and are defined using a family of recurrences involving the isobaric divided difference operators and limits of Weyl symmetrization operators. The $s_{(\\mu|\\lambda)}$ are the $q=t=0$ specialization of the stable limit non-symmetric Macdonald functions $\\widetilde{E}_{(\\mu|\\lambda)}$ defined by the author in previous work. We find a combinatorial formula for these functions simultaneously generalizing well known formulas for the Schur functions and the key polynomials. Further, we prove positivity results for the coefficients of the almost symmetric Schur functions expanded into the monomial basis and into the monomial-Schur basis of the space of almost symmetric functions. The latter positivity result follows after realizing the almost symmetric Schur functions $s_{(\\mu|\\lambda)}$ as limits of characters of representations of parabolic subgroups in type $GL.$",
        "citation_title": "Almost Symmetric Schur Functions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "For $\\Gamma$ a Fuchsian Group of the first kind, we obtain large sieve inequalities with weights the hyperbolic periods of Maass forms of even weight. This is inspired by work of Chamizo, who proved a large sieve inequality with weights values of Maass forms of weight $0$. The motivation is applications in counting problems in $\\Gamma_1 \\backslash \\Gamma / \\Gamma_2$, where $\\Gamma_1$, $\\Gamma_2$ are hyperbolic subgroups of $\\Gamma$.",
        "citation_title": "Large Sieve Inequalities for periods of Maass forms",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We construct an $I$-family of ancient graphical mean curvature flows over a minimal hypersurface in $\\mathbb{R}^{n+1}$ of finite total curvature with the Morse index $I$ by establishing exponentially fast convergence in terms of $|x|^2-t$. As a corollary, we show that these ancient flows have finite total curvature and finite mass drop. Moreover, one family of these flows is mean convex by a pointwise estimate.",
        "citation_title": "Ancient mean curvature flows with finite total curvature",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We show that any graded digraph $D$ on $n$ vertices with maximum degree $\\Delta$ has an oriented Ramsey number of at most $C^\\Delta n$ for some absolute constant $C > 1$, improving upon a recent result of Fox, He, and Wigderson. In particular, this implies that oriented grids in any fixed dimension have linear oriented Ramsey numbers, and gives a polynomial bound on the oriented Ramsey number of the hypercube.\nWe also show that this result is essentially best possible, in that there exist graded digraphs on $n$ vertices with maximum degree $\\Delta$ such that their oriented Ramsey number is at least $c^\\Delta n$ for some absolute constant $c > 1$.",
        "citation_title": "Oriented Ramsey numbers of graded digraphs",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper involves a diffusive epidemic model whose domain has one free boundary with the Stefan boundary condition, and one fixed boundary subject to the usual homogeneous Dirichlet or Neumann condition. By using the standard upper and lower solutions method and the regularity theory, we first study some related steady state problems which help us obtain the exact longtime behaviors of solution component $(u,v)$. Then we prove there exists the unique classical solution whose longtime behaviors are governed by a spreading-vanishing dichotomy. Lastly, the criteria determining when spreading or vanishing happens are given with respect to the basic reproduction number $\\mathcal{R}_0$, the initial habitat $[0,h_0]$, the expanding rates $\\mu_1$ and $\\mu_2$ as well as the initial function $(u_0,v_0)$. The criteria reveal the effect of the cooperative behaviors of agents and humans on spreading and vanishing.",
        "citation_title": "Dynamics for a diffusive epidemic model with a free boundary: spreading-vanishing dichotomy",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider a wireless network with multiple single-antenna repeaters that amplify and instantaneously re-transmit the signals they receive to improve the channel rank and system coverage. Due to the positive feedback formed by inter-repeater interference, stability could become a critical issue. We investigate the problem of determining the maximum amplification gain that the repeaters can use without breaking the system stability. Specifically, we obtain a bound by using the Gershgorin disc theorem, which reveals that the maximum amplification gain is restricted by the sum of channel amplitude gains. We show by case studies the usefulness of the so-obtained bound and provide insights on how the repeaters should be deployed.",
        "citation_title": "Stability Analysis of Interacting Wireless Repeaters",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Atmospheric tomography, the problem of reconstructing atmospheric turbulence profiles from wavefront sensor measurements, is an integral part of many adaptive optics systems used for enhancing the image quality of ground-based telescopes. Singular-value and frame decompositions of the underlying atmospheric tomography operator can reveal useful analytical information on this inverse problem, as well as serve as the basis of efficient numerical reconstruction algorithms. In this paper, we extend existing singular value decompositions to more realistic Sobolev settings including weighted inner products, and derive an explicit representation of a frame-based (approximate) solution operator. These investigations form the basis of efficient numerical solution methods, which we analyze via numerical simulations for the challenging, real-world Adaptive Optics system of the Extremely Large Telescope using the entirely MATLAB-based simulation tool MOST.",
        "citation_title": "Singular Value and Frame Decomposition-based Reconstruction for Atmospheric Tomography",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Part of the intrinsic structure of singular integrals in the Bessel setting is captured by Muckenhoupt-type weights. Anderson--Kerman showed that the Bessel Riesz transform is bounded on weighted $L^p_w$ if and only if $w$ is in the class $A_{p,\\lambda}$. We introduce a new class of Muckenhoupt-type weights $\\widetilde A_{p,\\lambda}$ in the Bessel setting, which is different from $A_{p,\\lambda}$ but characterizes the weighted boundedness for the Hardy--Littlewood maximal operators. We also establish the weighted $L^p$ boundedness and compactness, as well as the endpoint weak type boundedness of Riesz commutators. The quantitative weighted bound is also established.",
        "citation_title": "Muckenhoupt-Type Weights and Quantitative Weighted Estimate in the Bessel Setting",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The numerical solution of problems in nonlinear magnetostatics is typically based on a variational formulation in terms of magnetic potentials, the discretization by finite elements, and iterative solvers like the Newton method. The vector potential approach aims at minimizing a certain energy functional and, in three dimensions, requires the use of edge elements and appropriate gauging conditions. The scalar potential approach, on the other hand, seeks to maximize the negative coenergy and can be realized by standard Lagrange finite elements, thus reducing the number of degrees of freedom and simplifying the implementation. The number of Newton iterations required to solve the governing nonlinear system, however, has been observed to be usually higher than for the vector potential formulation. In this paper, we propose a method that combines the advantages of both approaches, i.e., it requires as few Newton iterations as the vector potential formulation while involving the magnetic scalar potential as the primary unknown. We discuss the variational background of the method, its well-posedness, and its efficient implementation. Numerical examples are presented for illustration of the accuracy and the gain in efficiency compared to other approaches.",
        "citation_title": "A reduced scalar potential approach for magnetostatics avoiding the coenergy",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper investigates a Stacked Intelligent Metasurfaces (SIM)-assisted Integrated Sensing and Communications (ISAC) system. An extended target model is considered, where the BS aims to estimate the complete target response matrix relative to the SIM. Under the constraints of minimum Signal-to-Interference-plus-Noise Ratio (SINR) for the communication users (CUs) and maximum transmit power, we jointly optimize the transmit beamforming at the base station (BS) and the end-to-end transmission matrix of the SIM, to minimize the Cram\u00e9r-Rao Bound (CRB) for target estimation. Effective algorithms such as the alternating optimization (AO) and semidefinite relaxation (SDR) are employed to solve the non-convex SINR-constrained CRB minimization problem. Finally, we design and build an experimental platform for SIM, and evaluate the performance of the proposed algorithms for communication and sensing tasks.",
        "citation_title": "Multi-user ISAC through Stacked Intelligent Metasurfaces: New Algorithms and Experiments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Let $H: \\mathbb{R}^4 \\to \\mathbb{R}$ be any smooth function. This article introduces some arguments for extracting dynamical information about the Hamiltonian flow of $H$ from high-dimensional families of closed holomorphic curves. We work in a very general setting, without imposing convexity or contact-type assumptions. For any compact regular level set $Y$, we prove that the Hamiltonian flow admits an infinite family of pairwise distinct, proper, compact invariant subsets whose union is dense in $Y$. This is a generalization of the Fish-Hofer theorem, which showed that $Y$ has at least one proper compact invariant subset. We then establish a global Le Calvez-Yoccoz property for almost every compact regular level set $Y$: any compact invariant subset containing all closed orbits is either equal to $Y$ or is not locally maximal. Next, we prove quantitative versions, in four dimensions, of the celebrated almost-existence theorem for Hamiltonian systems; such questions have been open for general Hamiltonians since the late $1980$s. We prove that almost every compact regular level set of $H$ contains at least two closed orbits, a sharp lower bound. Under explicit and $C^\\infty$-generic conditions on $H$, we prove almost-existence of infinitely many closed orbits.",
        "citation_title": "High-dimensional families of holomorphic curves and three-dimensional energy surfaces",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "As a generalization of graphs, hypergraphs are widely used to model higher-order relations in data. This paper explores the benefit of the hypergraph structure for the interpolation of point cloud data that contain no explicit structural information. We define the $\\varepsilon_n$-ball hypergraph and the $k_n$-nearest neighbor hypergraph on a point cloud and study the $p$-Laplacian regularization on the hypergraphs. We prove the variational consistency between the hypergraph $p$-Laplacian regularization and the continuum $p$-Laplacian regularization in a semisupervised setting when the number of points $n$ goes to infinity while the number of labeled points remains fixed. A key improvement compared to the graph case is that the results rely on weaker assumptions on the upper bound of $\\varepsilon_n$ and $k_n$. To solve the convex but non-differentiable large-scale optimization problem, we utilize the stochastic primal-dual hybrid gradient algorithm. Numerical experiments on data interpolation verify that the hypergraph $p$-Laplacian regularization outperforms the graph $p$-Laplacian regularization in preventing the development of spikes at the labeled points.",
        "citation_title": "Hypergraph $p$-Laplacian regularization on point clouds for data interpolation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The present paper deals with the perturbation analysis of set-valued inclusion problems, a problem format whose relevance has recently emerged in such contexts as robust and vector optimization as well as in vector equilibrium theory. The set-valued inclusions here considered are parameterized by variables belonging to a topological space, with and without constraints. By proper techniques of variational analysis, some qualitative global implicit function theorems are established, which ensure global solvability of these problems and continuous dependence on the parameter of the related solutions. Applications to parametric vector optimization are discussed, aimed at deriving sufficient conditions for the existence of ideal efficient solutions that depend continuously on the parameter perturbations.",
        "citation_title": "On some global implicit function theorems for set-valued inclusions with applications to parametric vector optimization",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, a backward map is introduced for the purposes of analysis of the nonlinear (stochastic) filter stability. The backward map is important because the filter-stability in the sense of $\\chisq$-divergence follows from showing a certain variance decay property for the backward map. To show this property requires additional assumptions on the model properties of the hidden Markov model (HMM). The analysis in this paper is based on introducing a Poincar\u00e9 Inequality (PI) for HMMs with white noise observations. In finite state-space settings, PI is related to both the ergodicity of the Markov process as well as the observability of the HMM. It is shown that the Poincar\u00e9 constant is positive if and only if the HMM is detectable.",
        "citation_title": "Backward Map for Filter Stability Analysis",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We show that a very general Jacobian elliptic surface is determined by its polarized rational Hodge structure, subject to various constraints on the irregularity and the geometric genus.",
        "citation_title": "Generic Torelli with denominators for elliptic surfaces",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We answer in the affirmative the surprisingly difficult questions: If a complex Banach space possesses a real predual X, then is X a complex Banach space?\nIf a complex Banach space possesses a real predual, then does it have a complex predual? We also answer the analogous questions for operator spaces, that is spaces of operators on a Hilbert space, up to complete isometry. Indeed we use operator space methods to solve the Banach space question above.",
        "citation_title": "A missing theorem on dual spaces",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the point spectrum of a second order difference operator with complex potential on the half-line via Fredholm determinants of the corresponding Birman-Schwinger operator pencils, the Evans and the Jost functions. An application is given to instability of a generalization of the Kolmogorov flow for the Euler equation of ideal fluid on the two dimensional torus.",
        "citation_title": "Characteristic determinants for a second order difference equation on the half-line arising in hydrodynamics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "An intelligent omni-surface (IOS) assisted holographic multiple-input and multiple-output architecture is conceived for $360^\\circ$ full-space coverage at a low energy consumption. The theoretical ergodic rate lower bound of our non-orthogonal multiple access (NOMA) scheme is derived based on the moment matching approximation method, while considering the signal distortion at transceivers imposed by hardware impairments (HWIs). Furthermore, the asymptotically ergodic rate lower bound is derived both for an infinite number of IOS elements and for continuous aperture surfaces. Both the theoretical analysis and the simulation results show that the achievable rate of the NOMA scheme is higher than that of its orthogonal multiple access counterpart. Furthermore, owing to the HWIs at the transceivers, the achievable rate saturates at high signal-to-noise ratio region, instead of reaching its theoretical maximum.",
        "citation_title": "Achievable Rate Analysis of Intelligent Omni-Surface Assisted NOMA Holographic MIMO Systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Reconfigurable holographic surfaces (RHSs) constitute a promising technique of supporting energy-efficient communications. In this paper, we formulate the energy efficiency maximization problem of the switch-controlled RHS-aided beamforming architecture by alternately optimizing the holographic beamformer at the RHS, the digital beamformer, the total transmit power and the power sharing ratio of each user. Specifically, to deal with this challenging non-convex optimization problem, we decouple it into three sub-problems. Firstly, the coefficients of RHS elements responsible for the holographic beamformer are optimized to maximize the sum of the eigen-channel gains of all users by our proposed low-complexity eigen-decomposition (ED) method. Then, the digital beamformer is designed by the singular value decomposition (SVD) method to support multi-user information transfer. Finally, the total transmit power and the power sharing ratio are alternately optimized, while considering the effect of transceiver hardware impairments (HWI). We theoretically derive the spectral efficiency and energy efficiency performance upper bound for the RHS-based beamforming architectures in the presence of HWIs. Our simulation results show that the switch-controlled RHS-aided beamforming architecture achieves higher energy efficiency than the conventional fully digital beamformer and the hybrid beamformer based on phase shift arrays (PSA). Moreover, considering the effect of HWI in the beamforming design can bring about further energy efficiency enhancements.",
        "citation_title": "Energy-Efficient Reconfigurable Holographic Surfaces Operating in the Presence of Realistic Hardware Impairments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose a hybrid beamforming architecture for near-field reconfigurable holographic surfaces (RHS) harnessed in cell-free networks. Specifically, the holographic beamformer of each base station (BS) is designed for maximizing the channel gain based on the local channel state information (CSI). By contrast, the digital beamformer at the central processing unit is designed based on the minimum mean squared error criterion. Furthermore, the near-field spectral efficiency of the RHS in cell-free networks is derived theoretically by harnessing the popular stochastic geometry approach. We consider both the phase shift error (PSE) at the RHS elements and the hardware impairment (HWI) at the radio frequency (RF) chains of the transceivers. Furthermore, we theoretically derive the asymptotic capacity bound, when considering an infinite physical size for the RHS in the near-field channel model. The theoretical analysis and simulation results show that the PSE at the RHS elements and the HWI at the RF chains of transceivers limit the spectral efficiency in the high signal-to-noise ratio region. Moreover, we show that the PSE at the RHS elements and the HWI at the RF chains of BSs can be compensated by increasing the number of BSs. Finally, we also demonstrate that the ergodic spectral efficiency based on the near-field channel model is higher than that based on the far-field channel model assumption.",
        "citation_title": "Performance Analysis of Reconfigurable Holographic Surfaces in the Near-Field Scenario of Cell-Free Networks Under Hardware Impairments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Let $\\mathcal C$ be a Krull-Schmidt triangulated category with shift functor $[1]$ and $\\mathcal R$ be a rigid subcategory of $\\mathcal C$. We are concerned with the mutation of two-term weak $\\mathcal R[1]$-cluster tilting subcategories. We show that any almost complete two-term weak $\\mathcal R[1]$-cluster tilting subcategory has exactly two completions. Then we apply the results on relative cluster tilting subcategories to the domain of $\\tau$-tilting theory in functor categories and abelian categories.",
        "citation_title": "Relative cluster tilting theory and $\u03c4$-tilting theory",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We characterize the existence of an Ulrich vector bundle on a variety $X \\subset P^N$ in terms of the existence of a subvariety satisfying some precise conditions. Then we use this fact to prove that a complete intersection of dimension $n \\ge 4$, which if $n=4$ is very general and not of type $(2,2)$, does not carry any Ulrich bundles of rank $r \\le 3$ unless $n=4, r=2$ and $X$ is a quadric.",
        "citation_title": "Ulrich subvarieties and the non-existence of low rank Ulrich bundles on complete intersections",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Extending Sparks's theorem, we determine the cardinality of the lattice of $(C_1,C_2)$-clonoids of Boolean functions in the cases where the target clone $C_2$ is the clone of projections. Moreover, we explicitly describe the $(C_1,C_2)$-clonoids of Boolean functions in the cases where the source clone $C_1$ is one of the four clones of monotone functions or contains the discriminator function.",
        "citation_title": "Clonoids of Boolean functions with a monotone or discriminator source clone",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We establish the $e$-positivity of all cycle-chord graphs by using the composition method that is developed by Zhou and the author very recently. Our technique is not only applicable to all cycle-chords, but also much simpler than the $(e)$-positivity method that is used for handling cycle-chords with girth at most $4$.",
        "citation_title": "All cycle-chords are $e$-positive",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In contrast to the conventional reconfigurable intelligent surfaces (RIS), intelligent omni-surfaces (IOS) are capable of full-space coverage of smart radio environments by simultaneously transmitting and reflecting the incident signals. In this paper, we investigate the ergodic spectral efficiency of IOS-aided systems for transmission over random channel links, while considering both realistic imperfect channel state information (CSI) and transceiver hardware impairments (HWIs). Firstly, we formulate the linear minimum mean square error estimator of the equivalent channel spanning from the user equipments (UEs) to the access point (AP), where the transceiver HWIs are also considered. Then, we apply a two-timescale protocol for designing the beamformer of the IOS-aided system. Specifically, for the active AP beamformer, the minimum mean square error combining method is employed, which relies on the estimated equivalent channels, on the statistical information of the channel estimation error, on the inter-user interference as well as on the HWIs at the AP and UEs. By contrast, the passive IOS beamformer is designed based on the statistical CSI for maximizing the upper bound of the ergodic spectral efficiency. The theoretical analysis and simulation results show that the transceiver HWIs have a significant effect on the ergodic spectral efficiency, especially in the high transmit power region. Furthermore, we show that the HWIs at the AP can be effectively compensated by deploying more AP antennas.",
        "citation_title": "Ergodic Spectral Efficiency Analysis of Intelligent Omni-Surface Aided Systems Suffering From Imperfect CSI and Hardware Impairments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we describe the Fourier-Mukai locus of the derived category of a complex algebraic K3 surface of Picard number one. We also prove that the Fourier-Mukai locus of the derived category of a complex algebraic K3 surface of Picard number one is strictly smaller than it's Matsui spectrum.",
        "citation_title": "Fourier-Mukai loci of K3 surfaces of Picard number one",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Analog codes add redundancy by expanding the dimension using real/complex-valued operations. Frame theory provides a mathematical basis for constructing such codes, with diverse applications in non-orthogonal code-division multiple access (NOMA-CDMA), distributed computation, multiple description source coding, space-time coding (STC), and more. The channel model corresponding to these applications is a combination of noise and erasures. Recent analyses showed a useful connection between spectral random-matrix theory and large equiangular tight frames (ETFs) under random uniform erasures. In this work we generalize this model to a channel where the erasures come in blocks. This particularly fits NOMA-CDMA with multiple transmit antennas for each user and STC with known spatial grouping. We present a method to adjust ETF codes to suit block erasures, and find minimum intra-block-correlation frames which outperform ETFs in this setting.",
        "citation_title": "Frame Codes for the Block-Erasure Channel",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Answering a question of A. V. Vasil'ev, we show that each finite symmetric (or alternating) group $H$ is a retract of any group containing $H$ as a verbally closed subgroup.",
        "citation_title": "Finite symmetric groups are strongly verbally closed",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A new algebra, hitherto not encountered in the usual Lie algebraic varieties or supervarieties, is introduced. The paper explores the rich and novel structure of the algebra, and it compares it on the one hand with the Jordan-Lie Superalgebras studied by Okubo and Kamiya, and on the other, with the four usual Euclidean division rings of the reals, the complexes, the quaternions and the octonions that the algebra is seen to combine, extend and generalise. A potential physical application of the algebra is briefly alluded to at the end.",
        "citation_title": "Multiplicatively Ordered and Directed Hybrid Jordan-Lie Superalgebra",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the zeros of cusp forms in the Miller basis whose vanishing order at infinity is a fixed number $m$. We show that for sufficiently large weights, the finite zeros in the fundamental domain of such forms, all lie on the circular part of the boundary of the fundamental domain. We further show and quantify an effective bound for the weight, which is linear in terms of $m$.",
        "citation_title": "On the Zeros of the Miller basis of Cusp Forms",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper presents a novel mathematical framework based on stochastic geometry to investigate the electromagnetic field exposure of idle and active users in cellular networks implementing dynamic beamforming. Accurate modeling of antenna gain becomes crucial in this context, encompassing both the main and the side lobes. The marginal distribution of EMF exposure for each type of users is initially derived. Subsequently, network performance is scrutinized by introducing a new metric aimed at ensuring minimal downlink coverage while simultaneously maintaining EMF exposure below distinct thresholds for both idle and active users. The metrics exhibit a high dependency on various parameters, such as the distance between active and idle users and the number of antenna elements.",
        "citation_title": "Stochastic Geometry Analysis of EMF Exposure of Idle Users and Network Performance with Dynamic Beamforming",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we prove that a spirallike circularlike domain is Kobayashi hyperbolic if and only if its core is empty. In particular, we show that such a domain is Kobayashi hyperbolic if and only if it is (biholomorphic to) a bounded domain. We also propose a problem in this area.",
        "citation_title": "On Hyperbolicity of Spirallike Circularlike domain",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The $L_p$ versions of the support function and polar body are introduced by Berndtsson, Mastrantonis and Rubinstein in \\cite{Berndtsson-Mastrantonis-Rubinstein-2023} recently. In this paper, we prove that the $L_p$-support function of the shadow system $K_t$ introduced by Rogers and Shephard in \\cite{rogers-1958-02,shephard-1964} is convex and the volume of the section of $L_p$ polar bodies of $K_t$ is $\\frac{1}{n}$-concave with respect to parameter $t$, and obtain some related inequalities. Finally, we present the reverse Rogers-Shephard type inequality for $L_p$-polar bodies.",
        "citation_title": "The Lp Polar bodies of shadow system and related inequalities",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper we study properties of a variant of the $1/2$-caloric capacity, called $1/2$-symmetric caloric capacity. The latter is associated simultaneously with the $1/2$-fractional heat equation and its conjugate. We establish its semi-additivity in $\\mathbb{R}^2$ and, moreover, we compute explicitly the $1/2$-symmetric caloric capacity of rectangles, which illustrates its anisotropic behaviour.",
        "citation_title": "On the semi-additivity of the $1/2$-symmetric caloric capacity in the plane",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recent works have shown an interest in investigating the frequentist asymptotic properties of Bayesian procedures for high-dimensional linear models under sparsity constraints. However, there exists a gap in the literature regarding analogous theoretical findings for non-linear models within the high-dimensional setting. The current study provides a novel contribution, focusing specifically on a non-linear mixed-effects model. In this model, the residual variance is assumed to be known, while the covariance matrix of the random effects and the regression vector are unknown and must be estimated. The prior distribution for the sparse regression coefficients consists of a mixture of a point mass at zero and a Laplace distribution, while an Inverse-Wishart prior is employed for the covariance parameter of the random effects. First, the effective dimension of this model is bounded with high posterior probabilities. Subsequently, we derive posterior contraction rates for both the covariance parameter and the prediction term of the response vector. Finally, under additional assumptions, the posterior distribution is shown to contract for recovery of the unknown sparse regression vector at the same rate as observed in the linear case.",
        "citation_title": "Posterior contraction rates in a sparse non-linear mixed-effects model",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Positive semidefinite (PSD) cone is the cone of positive semidefinite matrices, and is the object of interest in semidefinite programming (SDP). A computational efficient approximation of the PSD cone is the $k$-PSD closure, $1 \\leq k < n$, cone of $n\\times n$ real symmetric matrices such that all of their $k\\times k$ principal submatrices are positive semidefinite. For $k=1$, one obtains a polyhedral approximation, while $k=2$ yields a second order conic (SOC) approximation of the PSD cone. These approximations of the PSD cone have been used extensively in real-world applications such as AC Optimal Power Flow (ACOPF) to address computational inefficiencies where SDP relaxations are utilized for convexification the non-convexities. However a theoretical discussion about the geometry of these conic approximations of the PSD cone is rather sparse. In this short communication, we attempt to provide a characterization of some family of generators of the aforementioned conic approximations.",
        "citation_title": "On generators of $k$-PSD closures of the positive semidefinite cone",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we are mainly concerned with the well-posed problem of the fractional Keller--Segel system in the framework of variable Lebesgue spaces. Based on carefully examining the algebraical structure of the system, we reduced the fractional Keller--Segel system into the generalized nonlinear heat equation to overcome the difficulties caused by the boundedness of the Riesz potential in a variable Lebesgue spaces, then by mixing some structural properties of the variable Lebesgue spaces with the optimal decay estimates of the fractional heat kernel, we were able to establish two well-posedness results of the fractional Keller--Segel system in this functional setting.",
        "citation_title": "On the Cauchy problem for the fractional Keller-Segel system in variable Lebesgue spaces",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This work investigates the intricate relationship between the q-boson model, a quantum integrable system, and classical integrable systems such as the Toda and KP hierarchies. Initially, we analyze scalar products of off-shell Bethe states and explore their connections to tau functions of integrable hierarchies. Furthermore, we discuss correlation functions within this formalism, examining their representations in terms of tau functions, as well as their Schur polynomial expansions.",
        "citation_title": "$Q$-Boson model and relations with integrable hierarchies",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The motivation of this paper is to recognize a geometric shape from a noisy sample in the form of a point cloud. Inspired by the HDBSCAN clustering algorithm and the multicover bifiltration, we introduce the core- and the alpha-core bifiltrations. The multicover-, core- and alpha-core bifiltrations are all interleaved, and they enjoy similar Prohorov stability properties. We have performed experiments with the core and the alpha-core bifiltrations where we have calculated their persistent homology along lines in the two-dimensional persistence parameter space.",
        "citation_title": "Core Bifiltration",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we propose a new wireless sensing system equipped with the movable-antenna (MA) array, which can flexibly adjust the positions of antenna elements for improving the sensing performance over conventional antenna arrays with fixed-position antennas (FPAs). First, we show that the angle estimation performance in wireless sensing is fundamentally determined by the array geometry, where the Cramer-Rao bound (CRB) of the mean square error (MSE) for angle of arrival (AoA) estimation is derived as a function of the antennas' positions for both one-dimensional (1D) and two-dimensional (2D) MA arrays. Then, for the case of 1D MA array, we obtain a globally optimal solution for the MAs' positions in closed form to minimize the CRB of AoA estimation MSE. While in the case of 2D MA array, we aim to achieve the minimum of maximum (min-max) CRBs of estimation MSE for the two AoAs with respect to the horizontal and vertical axes, respectively. In particular, for the special case of circular antenna movement region, an optimal solution for the MAs' positions is derived under certain numbers of MAs and circle radii. Thereby, both the lower- and upper-bounds of the min-max CRB are obtained for the antenna movement region with arbitrary shapes. Moreover, we develop an efficient alternating optimization algorithm to obtain a locally optimal solution for MAs' positions by iteratively optimizing one between their horizontal and vertical coordinates with the other being fixed. Numerical results demonstrate that our proposed 1D/2D MA arrays can significantly decrease the CRB of AoA estimation MSE as well as the actual MSE compared to conventional uniform linear arrays (ULAs)/uniform planar arrays (UPAs) with different values of uniform inter-antenna spacing.",
        "citation_title": "Movable Antenna Enhanced Wireless Sensing Via Antenna Position Optimization",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Gross, Kohnen and Zagier proved an averaged version of the algebraicity conjecture for special values of higher Green's functions on modular curves. In this work, we study an analogous problem for special values of Green's functions on hyperbolic $3$-space. We prove that their averages can be computed in terms of logarithms of primes and logarithms of units in real quadratic fields. Moreover, we study twisted averages of special values of Green's functions, which yield algebraic numbers instead of logarithms.",
        "citation_title": "Special values of Green's functions on hyperbolic $3$-space",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider generalizations of the classical inverse problem to Bayesien type estimators, where the result is not one optimal parameter but an optimal probability distribution in parameter space. The practical computational tool to compute these distributions is the Metropolis Monte Carlo algorithm. We derive kinetic theories for the Metropolis Monte Carlo method in different scaling regimes. The derived equations yield a different point of view on the classical algorithm. It further inspired modifications to exploit the difference scalings shown on an simulation example of the Lorenz system.",
        "citation_title": "Kinetic Theories for Metropolis Monte Carlo Methods",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We reobtain and often refine prior criteria due to Kaplansky, McGovern, Roitman, Shchedryk, Wiegand, and Zabavsky--Bilavska and obtain new criteria for a Hermite ring to be an \\textsl{EDR}. We mention three criteria: (1) a Hermite ring $R$ is an \\textsl{EDR} iff for all pairs $(a,c)\\in R^2$, the product homomorphism $U(R/Rac)\\times U\\bigl(R/Rc(1-a)\\bigr)\\to U(R/Rc)$ between groups of units is surjective; (2) a reduced Hermite ring is an \\textsl{EDR} iff it is a pre-Schreier ring and for each $a\\in R$, every zero determinant unimodular $2\\times 2$ matrix with entries in $R/Ra$ lifts to a zero determinant matrix with entries in $R$; (3) a B\u00e9zout domain $R$ is an \\textsl{EDD} iff for all triples $(a,b,c)\\in R^3$ there exists a unimodular pair $(e,f)\\in R^2$ such that $(a,e)$ and $(be+af,1-a-bc)$ are unimodular pairs. We use these criteria to show that each B\u00e9zout ring $R$ that is an $(SU)_2$ ring (as introduced by Lorenzini) such that for each nonzero $a\\in R$ there exists no nontrivial self-dual projective $R/Ra$-module of rank $1$ generated by $2$ elements (e.g., all its elements are squares), is an \\textsl{EDR}.",
        "citation_title": "Matrix invertible extensions over commutative rings. Part III: Hermite rings",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the distribution of fringe trees in Patricia tries and compressed binary search trees; both cases are random binary trees that have been compressed by deleting vertices of outdegree 1 so that they are random full binary trees. The main results are central limit theorems for the number of fringe trees of a given type, which imply quenched and annealed limit results for the fringe tree distribution; for Patricia tries, this is complicated by periodic oscillations in the usual manner. We also consider extended fringe trees. The results are derived from earlier results for uncompressed tries and binary search trees. In the case of compressed binary search trees, it seems difficult to give a closed formula for the asymptotic fringe tree distribution, but we provide a recursion and give examples.",
        "citation_title": "Fringe trees of Patricia tries and compressed binary search trees",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We extend the port-Hamiltonian framework defined with respect to a Lagrangian submanifold and a Dirac structure by augmenting the Lagrangian submanifold with the space of external variables. The new pair of conjugated variables is called energy port. We show that in the most general case, the extension describes constrained Hamiltonian systems whose Hamiltonian function depends on inputs.",
        "citation_title": "Port-Hamiltonian systems with energy and power ports",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Clustering methods must be tailored to the dataset it operates on, as there is no objective or universal definition of ``cluster,'' but nevertheless arbitrariness in the clustering method must be minimized. This paper develops a quantitative ``stability'' method of determining clusters, where stable or persistent clustering signals are used to indicate real structures have been identified in the underlying dataset. This method is based on modulating clustering methods by controlling a parameter -- through a thermodynamic analogy, the modulation parameter is considered ``time'' and the evolving clustering methodologies can be considered a ``heat flow.'' When the information entropy of the heat flow is stable over a wide range of times -- either globally or in the local sense which we define -- we interpret this stability as an indication that essential features of the data have been found, and create clusters on this basis.",
        "citation_title": "Stability of Information in the Heat Flow Clustering",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We establish the well/ill-posedness theories for the inviscid $\\alpha$-surface quasi-geostrophic ($\\alpha$-SQG) equations in H\u00f6lder spaces, where $\\alpha = 0$ and $\\alpha = 1$ correspond to the two-dimensional Euler equation in the vorticity formulation and SQG equation of geophysical significance, respectively. We first prove the local-in-time well-posedness of $\\alpha$-SQG equations in $C([0,T);C^{0,\\beta}(\\mathbb{R}^2))$ with $\\beta \\in (\\alpha,1)$ for some $T>0$. We then analyze the strong ill-posedness in $C^{0,\\alpha}(\\mathbb{R}^2)$ constructing smooth solutions to the $\\alpha$-SQG equations that exhibit $C^{0,\\alpha}$--norm growth in a short time. In particular, we develop the nonexistence theory for $\\alpha$-SQG equations in $C^{0,\\alpha}(\\mathbb{R}^2)$.",
        "citation_title": "On well/ill-posedness for the generalized surface quasi-geostrophic equations in H\u00f6lder spaces",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We prove global well-posedness for the cubic nonlinear Schr\u00f6dinger equation with nonlinearity concentrated on a homogeneous Poisson process.",
        "citation_title": "The nonlinear Schr\u00f6dinger equation with sprinkled nonlinearity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper we investigate the Cauchy problem of d-dimensional Euler-Poincar\u00e9 equations. By choosing a class of new and special initial data, we can transform this d-dimensional Euler-Poincar\u00e9 equations into the Camassa-Holm type equation in the real line. We first obtain some global existence results and then present a new blow-up result to the system under some different assumptions on this special class of initial data.",
        "citation_title": "Global existence and blow-up for the Euler-Poincar\u00e9 equations with a class of initial data",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Let $K$ be a convex body in ${\\mathbb R}^n$, and let $\\Pi_1({\\mathbb R}^n)$ be the space of polynomials in $n$ variables of degree at most $1$. Given an $(n+1)$-element set $Y\\subset K$ in general position, we let $P_Y$ denote the Lagrange interpolation projector $P_Y: C(K)\\to \\Pi_1({\\mathbb R}^n)$ with nodes in $Y$. In this paper, we study upper and lower bounds for the norm of the optimal Lagrange interpolation projector, i.e., the projector with minimal operator norm where the minimum is taken over all $(n+1)$-element sets of interpolation nodes in $K$. We denote this minimal norm by $\\theta_n(K)$. Our main result, Theorem 5.2, provides an explicit lower bound for the constant $\\theta_n(K)$ for an arbitrary convex body $K\\subset{\\mathbb R}^n$ and an arbitrary $n\\ge 1$. We prove that $\\theta_n(K)\\ge \\chi_n^{-1}\\left({{\\rm vol}(K)}/{{\\rm simp}(K)}\\right)$ where $\\chi_n$ is the Legendre polynomial of degree $n$ and ${\\rm simp}(K)$ is the maximum volume of a simplex contained in $K$. The proof of this result relies on a geometric characterization of the Legendre polynomials in terms of the volumes of certain convex polyhedra. More specifically, we show that for every $\\gamma\\ge 1$ the volume of the set $\\left\\{x=(x_1,...,x_n)\\in{\\mathbb R}^n : \\sum |x_j| +\\left|1- \\sum x_j\\right|\\le\\gamma\\right\\}$ is equal to ${\\chi_n(\\gamma)}/{n!}$. If $K$ is an $n$-dimensional ball, this approach leads us to the equivalence $\\theta_n(K) \\asymp\\sqrt{n}$ which is complemented by the exact formula for $\\theta_n(K)$. If $K$ is an $n$-dimensional cube, we obtain explicit efficient formulae for upper and lower bounds of the constant $\\theta_n(K)$; moreover, for small $n$, these estimates enable us to compute the exact values of this constant.",
        "citation_title": "Optimal Lagrange Interpolation Projectors and Legendre Polynomials",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We generalize Kracht's theory of internal describability from classical modal logic to the family of all logics canonically associated with varieties of normal lattice expansions (LE algebras). We work in the purely algebraic setting of perfect LEs; the formulas playing the role of Kracht's formulas in this generalized setting pertain to a first order language whose atoms are special inequalities between terms of perfect algebras. Via duality, formulas in this language can be equivalently translated into first order conditions in the frame correspondence languages of several types of relational semantics for LE-logics.",
        "citation_title": "Unified inverse correspondence for LE-logics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We introduce and study the generalized cyclotomic polynomials $\\Phi_{A,S,n}(x)$ associated with a regular system $A$ of divisors and an arbitrary set $S$ of positive integers. We show that all of these polynomials have integer coefficients, they can be expressed as the product of certain classical cyclotomic polynomials $\\Phi_d(x)$ with $d\\mid n$, and enjoy many other properties which are similar to the classical and unitary cases. We also point out some related Menon-type identities. One of them seems to be new even for the cyclotomic polynomials $\\Phi_n(x)$.",
        "citation_title": "Generalized cyclotomic polynomials associated with regular systems of divisors and arbitrary sets of positive integers",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We show that every component of the locus of smooth supersingular curves of genus $4$ in characteristic $p>2$ has a trivial generic automorphism group. As a result, we prove Oort's conjecture about automorphism groups of supersingular abelian fourfolds for $p>2$. Our main idea consists of estimating dimensions of the loci of smooth supersingular curves that admit an automorphism of prime order by considering possible choices of the corresponding quotient curves. This reasoning also results in a new proof of Oort's conjecture for $g = 3$ and $p>2$, previously proved by Karemaker, Yuboko, and Yu.",
        "citation_title": "Oort's conjecture and automorphisms of supersingular curves of genus four",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We aim to characterise boundedness of commutators $[b,T]$ of singular integrals $T$. Boundedness is studied between weighted Lebesgue spaces $L^p(X)$ and $L^q(X)$, $p\\leq q$, when the underlying space $X$ is a space of homogeneous type. Commutator theory in spaces of homogeneous type already exist in literature, in particular boundedness results in the setting $p=q$. The purpose here is to extend the earlier results to the setting of $p< q$. Our methods extend those of Duong et al. and Hyt\u00f6nen et al. A novelty here is that in order to show the lower bound of the commutator norm, we demonstrate that the approximate weak factorisation of Hyt\u00f6nen can be used when the underlying setting is a space of homogeneous type and not only in the Euclidean setting. The strength of the approximate weak factorisation is that (when compared to the so-called median method) it readily allows complex-valued $b$ in addition to real-valued ones. However, the median method has been previously successfully applied to iterated commutators and thus has its own strengths. We also present a proof based on that method.",
        "citation_title": "Fractional Bloom boundedness of commutators in spaces of homogeneous type",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Compact K\u00e4hler manifolds satisfy several nice Hodge-theoretic properties such as the Hodge symmetry, the Hard Lefschetz property and the Hodge--Riemann bilinear relations, etc. In this note, we investigate when such nice properties hold on compact complex manifolds with semistable degenerations.\nFor compact complex manifolds which can be obtained as smoothings of SNC varieties without triple intersection locus, we show the Hodge symmetry when the monodromy logarithm induces isomorphisms on the associated graded. We also show the Hodge--Riemann relations on $H^3$ of non-K\u00e4hler Calabi--Yau 3-folds with such semistable degenerations.",
        "citation_title": "On Hodge structures of compact complex manifolds with semistable degenerations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we consider the design of data-driven predictive controllers for nonlinear systems from input-output data via linear-in-control input Koopman lifted models. Instead of identifying and simulating a Koopman model to predict future outputs, we design a subspace predictive controller in the Koopman space. This allows us to learn the observables minimizing the multi-step output prediction error of the Koopman subspace predictor, preventing the propagation of prediction errors. To avoid losing feasibility of our predictive control scheme due to prediction errors, we compute a terminal cost and terminal set in the Koopman space and we obtain recursive feasibility guarantees through an interpolated initial state. As a third contribution, we introduce a novel regularization cost yielding input-to-state stability guarantees with respect to the prediction error for the resulting closed-loop system. The performance of the developed Koopman data-driven predictive control methodology is illustrated on a nonlinear benchmark example from the literature.",
        "citation_title": "Koopman Data-Driven Predictive Control with Robust Stability and Recursive Feasibility Guarantees",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The notions of N-hyperideals and J-hyperideals as two classes of hyperideals were recently defined in the context of Krasner (m,n)-hyperrings. These concepts are created on the basis of the intersection of all n-ary prime hyperideals and the intersection of all maximal hyperideals, respectively. Despite being vastly different in many aspects, they shar numerous similar properties. The aim of this research work is to merge them under one frame called n-ary delta(0)-hyperideals where the function delta assigns to each hyperideals of a Krasner (m,n)-hyperring a hyperideal of the same hyperring. We give various properties of n-ary delta(0)-hyperideals and use them to characerize certain classes of hyperring such as hyperintegral domains and local hyperrings. Moreover, we introduce the notions of (s,n)-absorbing delta(0)-hyperideals and weakly (s,n)-absorbing delta(0)-hyperideals.",
        "citation_title": "Merging N-hyperideals and J-hyperideals in one frame",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Block classical Gram-Schmidt (BCGS) is commonly used for orthogonalizing a set of vectors $X$ in distributed computing environments due to its favorable communication properties relative to other orthogonalization approaches, such as modified Gram-Schmidt or Householder. However, it is known that BCGS (as well as recently developed low-synchronization variants of BCGS) can suffer from a significant loss of orthogonality in finite-precision arithmetic, which can contribute to instability and inaccurate solutions in downstream applications such as $s$-step Krylov subspace methods. A common solution to improve the orthogonality among the vectors is reorthogonalization. Focusing on the \"Pythagorean\" variant of BCGS, introduced in [E. Carson, K. Lund, & M. Rozlo\u017en\u00edk. SIAM J. Matrix Anal. Appl. 42(3), pp. 1365--1380, 2021], which guarantees an $O(\\varepsilon)\\kappa^2(X)$ bound on the loss of orthogonality as long as $O(\\varepsilon)\\kappa^2(X)<1$, where $\\varepsilon$ denotes the unit roundoff, we introduce and analyze two reorthogonalized Pythagorean BCGS variants. These variants feature favorable communication properties, with asymptotically two synchronization points per block column, as well as an improved $O(\\varepsilon)$ bound on the loss of orthogonality. Our bounds are derived in a general fashion to additionally allow for the analysis of mixed-precision variants. We verify our theoretical results with a panel of test matrices and experiments from a new version of the \\texttt{BlockStab} toolbox.",
        "citation_title": "Reorthogonalized Pythagorean variants of block classical Gram-Schmidt",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the set $\\operatorname{MA}(X,Y)$ of operators between Banach spaces $X$ and $Y$ that attain their minimum norm, and the set $\\operatorname{QMA}(X,Y)$ of operators that quasi attain their minimum norm. We characterize the Radon-Nikodym property in terms of operators that attain their minimum norm and obtain some related results about the density of the sets $\\operatorname{MA}(X,Y)$ and $\\operatorname{QMA}(X,Y)$. We show that every infinite-dimensional Banach space $X$ has an isomorphic space $Y$ such that not every operator from $X$ to $Y$ quasi attains its minimum norm. We introduce and study Bishop-Phelps-Bollob\u00e1s type properties for the minimum norm, including the ones already considered in the literature, and we exhibit a wide variety of results and examples, as well as exploring the relations between them.",
        "citation_title": "On density and Bishop-Phelps-Bollob\u00e1s type properties for the minimum norm",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recently, there has been a significant focus on exploring the theoretical aspects of deep learning, especially regarding its performance in classification tasks. Bayesian deep learning has emerged as a unified probabilistic framework, seeking to integrate deep learning with Bayesian methodologies seamlessly. However, there exists a gap in the theoretical understanding of Bayesian approaches in deep learning for classification. This study presents an attempt to bridge that gap. By leveraging PAC-Bayes bounds techniques, we present theoretical results on the prediction or misclassification error of a probabilistic approach utilizing Spike-and-Slab priors for sparse deep learning in classification. We establish non-asymptotic results for the prediction error. Additionally, we demonstrate that, by considering different architectures, our results can achieve minimax optimal rates in both low and high-dimensional settings, up to a logarithmic factor. Moreover, our additional logarithmic term yields slight improvements over previous works. Additionally, we propose and analyze an automated model selection approach aimed at optimally choosing a network architecture with guaranteed optimality.",
        "citation_title": "Misclassification bounds for PAC-Bayesian sparse deep learning",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We generalize the method used by M\u00e6hlen & Seth [17] used to prove the existence of small-amplitude asymmetric solutions to the capillary-gravity Whiham equation, so that it can be applied directly to a class of similar equations. The purpose is to prove or disprove the existence of asymmetric waves for the water wave problem or other model equations for water waves. Our main result in this paper is a theorem that gives both necessary and sufficient conditions for the existence of small-amplitude periodic asymmetry solutions for this class of equations. The result is then applied to an infinite depth capillary-gravity Whitham equation and an infinite depth capillary-gravity Babenko equation to show the nonexistence of small-amplitude waves for these equations. This example also highlights the similarities between these equations suggesting the potential existence of small-amplitude asymmetric waves for the finite depth capillary-gravity Babenko equation.",
        "citation_title": "On small-amplitude asymmetric water waves",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper we study the weak convergence of self-normalized partial sum processes in the Skorokhod M1 topology for sequences of random variables which exhibit clustering of large values of the same sign. We show that for stationary regularly varying sequences with such properties, their corresponding properly centered self-normalized partial sums processes converge to a stable Levy process. The convergence is established in the space of cadlag functions endowed with Skorohod's M1 topology, which is more suitable especially for cases in which the standard J1 topology fails to induce weak convergence of joint stochastic functionals.",
        "citation_title": "Weak Convergence for Self-Normalized Partial Sum Processes in the Skorokhod M1 Topology with Applications to Regularly Varying Time Series",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose a notion of conditioned stochastic stability of invariant measures on repellers: we consider whether quasi-ergodic measures of absorbing Markov processes, generated by random perturbations of the deterministic dynamics and conditioned upon survival in a neighbourhood of a repeller, converge to an invariant measure in the zero-noise limit. Under suitable choices of the random perturbation, we find that equilibrium states on uniformly expanding repellers are conditioned stochastically stable. In the process, we establish a rigorous foundation for the existence of ``natural measures'', which were proposed by Kantz and Grassberger in 1984 to aid the understanding of chaotic transients.",
        "citation_title": "Conditioned stochastic stability of equilibrium states on uniformly expanding repellers",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the weak convergence behaviour of the Leimkuhler--Matthews method, a non-Markovian Euler-type scheme with the same computational cost as the Euler scheme, for the approximation of the stationary distribution of a one-dimensional McKean--Vlasov Stochastic Differential Equation (MV-SDE). The particular class under study is known as mean-field (overdamped) Langevin equations (MFL). We provide weak and strong error results for the scheme in both finite and infinite time. We work under a strong convexity assumption.\nBased on a careful analysis of the variation processes and the Kolmogorov backward equation for the particle system associated with the MV-SDE, we show that the method attains a higher-order approximation accuracy in the long-time limit (of weak order convergence rate $3/2$) than the standard Euler method (of weak order $1$). While we use an interacting particle system (IPS) to approximate the MV-SDE, we show the convergence rate is independent of the dimension of the IPS and this includes establishing uniform-in-time decay estimates for moments of the IPS, the Kolmogorov backward equation and their derivatives. The theoretical findings are supported by numerical tests.",
        "citation_title": "Improved weak convergence for the long time simulation of Mean-field Langevin equations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Hamming graph $H(n,q)$ is defined on the vertex set $[q]^n$ and two vertices are adjacent if and only if they differ in precisely one coordinate. Alon \\cite{Alon} proved that the burning number of $H(n,2)$ is $\\lceil\\frac n2\\rceil+1$. In this note we show that the burning number of $H(n,q)$ is $(1-\\frac 1q)n+O(\\sqrt{n\\log n})$ for fixed $q\\geq 2$ and $n\\to\\infty$.",
        "citation_title": "Burning Hamming graphs",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper introduces Waste Factor (W) and Waste Figure (WF) to assess power efficiency in any multiple-input multiple-output (MIMO) or single-input multiple-output (SIMO) or multiple-input single-output (MISO) cascaded communication system. This paper builds upon the new theory of Waste Factor, which systematically models added wasted power in any cascade for parallel systems such as MISO, SIMO, and MIMO systems, which are prevalent in current wireless networks. Here, we also show the advantage of W compared to conventional metrics for quantifying and analyzing energy efficiency. This work explores the utility of W in assessing energy efficiency in communication channels, within Radio Access Networks (RANs).",
        "citation_title": "Using Waste Factor to Optimize Energy Efficiency in Multiple-Input Single-Output (MISO) and Multiple-Input Multiple-Output (MIMO) Systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We use (versions of) the von Neumann inequality for Hilbert space contractions to prove several Schwarz-Pick inequalities. Specifically, we derive an alternate proof for a multi-point Schwarz-Pick inequality by Beardon and Minda, along with a generalized version for operators. Connections with model spaces and Peschl's invariant derivatives are established. Finally, Schwarz-Pick inequalities for analytic functions on polydisks and for higher order derivatives are discussed. An enhanced version of the Schwarz-Pick lemma, using the notion of distinguished variety, is obtained for the bidisk.",
        "citation_title": "Schwarz-Pick type inequalities from an operator theoretical point of view",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This study explores the relationship between hypergraph automorphisms and the spectral properties of matrices associated with hypergraphs. For an automorphism $f$, an \\( f \\)-compatible matrices capture aspects of the symmetry, represented by \\( f \\), within the hypergraph. First, we explore rotation, a specific kind of automorphism and find that the spectrum of any matrix compatible with a rotation can be decomposed into the spectra of smaller matrices associated with that rotation. We show that the spectrum of any \\(f\\)-compatible matrix can be decomposed into the spectra of smaller matrices associated with the component rotations comprising \\( f \\). Further, we study a hypergraph symmetry termed unit-automorphism, which induces bijections on the hyperedges, though not necessarily on the vertex set. We show that unit automorphisms also lead to the spectral decomposition of compatible matrices.",
        "citation_title": "Spectral decomposition of hypergraph automorphism compatible matrices",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We obtain several inequalities on the generalized means of dependent p-values. In particular, the weighted harmonic mean of p-values is strictly sub-uniform under several dependence assumptions of p-values, including independence, weak negative association, the class of extremal mixture copulas, and some Clayton copulas. Sub-uniformity of the harmonic mean of p-values has an important implication in multiple hypothesis testing: It is statistically invalid to merge p-values using the harmonic mean unless a proper threshold or multiplier adjustment is used, and this invalidity applies across all significance levels. The required multiplier adjustment on the harmonic mean explodes as the number of p-values increases, and hence there does not exist a constant multiplier that works for any number of p-values, even under independence.",
        "citation_title": "Sub-uniformity of harmonic mean p-values",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Starting from the study of pseudodifferential operators with completely periodic symbols, we obtain results of continuity and invertibility of a class of Gabor operators on time-frequency invariant Banach spaces. As an applications we find sufficient conditions for the existence of Gabor frames on the space of square integrable functions, associated to a general lattice.",
        "citation_title": "Pseudodifferential operators on time-frequency invariant Banach spaces and applications to Gabor Frames",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "An $\\mathbb{F}_q$- linear set $L=L_U$ of $\\Lambda=\\mathrm{PG}(V, \\mathbb{F}_{q^n}) \\cong \\mathrm{PG}(r-1,q^n)$ is a set of points defined by non-zero vectors of an $\\mathbb{F}_q$-subspace $U$ of $V$. The integer $\\dim_{\\mathbb{F}_q} U$ is called the rank of $L$. In [G. Lunardon, O. Polverino: Translation ovoids of orthogonal polar spaces. Forum Math. 16 (2004)], it was proven that any $\\mathbb{F}_q$-linear set $L$ of $\\Lambda$ of rank $u$ such that $\\langle L \\rangle=\\Lambda$ is either a canonical subgeometry of $\\Lambda$ or there are a $(u-r-1)$-dimensional subspace $\\Gamma$ of $\\mathrm{PG}(u-1,q^n) \\supset \\Lambda$ disjoint from $\\Lambda$ and a canonical subgeometry $\\Sigma \\cong \\mathrm{PG}(u-1,q)$ disjoint from $\\Gamma$ such that $L$ is the projection of $\\Sigma$ from $\\Gamma$ onto $\\Lambda$. The subspace $\\Gamma$ is called the vertex of the projection. In this article, we will show a method to reconstruct the vertex $\\Gamma$ for a peculiar class of linear sets of rank $u = n(r - 1)$ in $\\mathrm{PG}(r - 1, q^n)$ called evasive linear sets. Also, we will use this result to characterize some families of linear sets of the projective line $\\mathrm{PG}(1,q^n)$ introduced from 2018 onward, by means of certain properties of their projection vertices, as done in [B. Csajb\u00f3k, C. Zanella: On scattered linear sets of pseudoregulus type in $\\mathrm{PG}(1, q^t)$, Finite Fields Appl. 41 (2016)] and in [C. Zanella, F. Zullo: Vertex properties of maximum scattered linear sets of $\\mathrm{PG}(1, q^n)$. Discrete Math. 343(5) (2020)].",
        "citation_title": "A geometric characterization of known maximum scattered linear sets of $\\mathrm{PG}(1,q^n)$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We prove a rigorous lower bound on the correlation energy of interacting fermions in the mean-field regime for a wide class of singular interactions, including the Coulomb potential. Combined with the upper bound obtained in \\cite{ChrHaiNam-23b}, our result establishes an analogue of the Gell-Mann--Brueckner formula $c_{1}\\rho\\log\\left(\\rho\\right)+c_{2}\\rho$ for the correlation energy of the electron gas in the high-density limit. Moreover, our analysis allows us to go beyond mean-field scaling while still covering the same class of potentials.",
        "citation_title": "The Correlation Energy of the Electron Gas in the Mean-Field Regime",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A lexicographic maximum of a set $X \\subseteq \\mathbb{R}^n$ is a vector in $X$ whose smallest component is as large as possible, and subject to that requirement, whose second smallest component is as large as possible, and so on for the third smallest component, etc. Lexicographic maximization has numerous practical and theoretical applications, including fair resource allocation, analyzing the implicit regularization of learning algorithms, and characterizing refinements of game-theoretic equilibria. We prove that a minimizer in $X$ of the exponential loss function $L_c(\\mathbf{x}) = \\sum_i \\exp(-c x_i)$ converges to a lexicographic maximum of $X$ as $c \\rightarrow \\infty$, provided that $X$ is stable in the sense that a well-known iterative method for finding a lexicographic maximum of $X$ cannot be made to fail simply by reducing the required quality of each iterate by an arbitrarily tiny degree. Our result holds for both near and exact minimizers of the exponential loss, while earlier convergence results made much stronger assumptions about the set $X$ and only held for the exact minimizer. We are aware of no previous results showing a connection between the iterative method for computing a lexicographic maximum and exponential loss minimization. We show that every convex polytope is stable, but that there exist compact, convex sets that are not stable. We also provide the first analysis of the convergence rate of an exponential loss minimizer (near or exact) and discover a curious dichotomy: While the two smallest components of the vector converge to the lexicographically maximum values very quickly (at roughly the rate $\\frac{\\log n}{c}$), all other components can converge arbitrarily slowly.",
        "citation_title": "Lexicographic Optimization: Algorithms and Stability",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper explores stochastic modeling approaches to elucidate the intricate dynamics of stock prices and volatility in financial markets. Beginning with an overview of Brownian motion and its historical significance in finance, we delve into various stochastic models, including the classic Black-Scholes framework, the Heston model, fractional Brownian motion, GARCH models, and Levy processes. Through a thorough investigation, we analyze the strengths and limitations of each model in capturing key features of financial time series data. Our empirical analysis focuses on parameter estimation and model calibration using Levy processes, demonstrating their effectiveness in predicting stock returns. However, we acknowledge the need for further refinement and exploration, suggesting potential avenues for future research, such as hybrid modeling approaches. Overall, this study underscores the importance of stochastic modeling in understanding market dynamics and informs decision-making in the financial industry.",
        "citation_title": "A Basic Overview of Various Stochastic Approaches to Financial Modeling With Examples",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We characterise the model-theoretic algebraic closure in Zilber's exponential field. A key step involves showing that certain algebraic varieties have finite intersections with certain finite-rank subgroups of the graph of exponentiation. Mordell-Lang for algebraic tori (a theorem of Laurent) plays a central role in our proof.",
        "citation_title": "Algebraic types in Zilber's exponential field",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The real-time monitoring of the structural displacement of the Vacuum Vessel (VV) of thermonuclear fusion devices caused by electromagnetic (EM) loads is of great interest. In this paper, Model Order Reduction (MOR) is applied to the Integral Equation Methods (IEM) and the Finite Elements Method (FEM) to develop Electromagnetic and Structural Reduced Order Models (ROMs) compatible with real-time execution which allows for the real-time monitoring of strain and displacement in critical positions of Tokamaks machines. Low-rank compression techniques based on hierarchical matrices are applied to reduce the computational cost during the offline stage when the ROMs are constructed. Numerical results show the accuracy of the approach and demonstrate the compatibility with real-time execution in standard hardware.",
        "citation_title": "Reduced Order Modeling for Real-Time Monitoring of Structural Displacements due to Electromagnetic Forces in Large Scale Tokamaks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the periodic homogenization problem of state-constraint Hamilton--Jacobi equations on perforated domains in the convex setting and obtain the optimal convergence rate. We then consider a dilute situation in which the holes' diameter is much smaller than the microscopic scale. Finally, a homogenization problem with domain defects where some holes are missing is analyzed.",
        "citation_title": "Quantitative homogenization of state-constraint Hamilton--Jacobi equations on perforated domains and applications",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In autonomous mobility-on-demand systems, effectively managing vehicle flows to mitigate induced congestion and ensure efficient operations is imperative for system performance and positive customer experience. Against this background, we study the potential of staggered routing, i.e., purposely delaying trip departures from a system perspective, in order to reduce congestion and ensure efficient operations while still meeting customer time windows. We formalize the underlying planning problem and show how to efficiently model it as a mixed integer linear program. Moreover, we present a matheuristic that allows us to efficiently solve large-scale real-world instances both in an offline full-information setting and its online rolling horizon counterpart. We conduct a numerical study for Manhattan, New York City, focusing on low- and highly-congested scenarios. Our results show that in low-congestion scenarios, staggering trip departures allows mitigating, on average, 94% of the induced congestion in a full information setting. In a rolling horizon setting, our algorithm allows us to reduce 90% of the induced congestion. In high-congestion scenarios, we observe an average reduction of 66% as the full information bound and an average reduction of 56% in our online setting. Surprisingly, we show that these reductions can be reached by shifting trip departures by a maximum of six minutes in both the low and high-congestion scenarios.",
        "citation_title": "Staggered Routing in Autonomous Mobility-on-Demand Systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Let $P_{k,m}$ denote the Poincar\u00e9 series of weight $k$ and index $m$ for the full modular group $\\mathrm{SL}_2(\\mathbb{Z})$. Let $\\{P_{k,m}\\}$ be a sequence of Poincar\u00e9 series for which $m(k)$ satisfies $m(k) / k \\rightarrow\\infty$ and $m(k) \\ll k^{\\frac{2 + 2\\theta}{1 + 2\\theta} - \\epsilon}$ where $\\theta$ is an exponent towards the Ramanujan Petersson conjecture. We prove that the $L^2$ mass of such a sequence equidistributes on $\\mathrm{SL}_2(\\mathbb{Z}) \\backslash \\mathbb{H}$ with respect to the hyperbolic metric as $k$ goes to infinity. As a consequence, we deduce that the zeros of such a sequence $\\{P_{k,m}\\}$ become uniformly distributed in $\\mathrm{SL}_2(\\mathbb{Z}) \\backslash \\mathbb{H}$ with respect to the hyperbolic metric. Along the way we also improve a result of Rankin about the vanishing of Poincar\u00e9 series. We show that for sufficiently large $k$ and $1\\leq m\\ll k^2$, $P_{k,m}$ vanishes exactly once at the cusp, which also implies that $P_{k,m}\\not\\equiv 0$ in this range.",
        "citation_title": "Mass equidistribution for Poincar\u00e9 series of large index",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the $L^\\infty(\\mathbb{R}^d)$ boundedness for Riesz transforms of the form ${V^{a}(-\\frac12\\Delta+V)^{-a}},$ where $a > 0$ and $V$ is a non-negative potential with power growth acting independently on each coordinate. We factorize the semigroup $e^{-tL}$ into one-dimensional factors, estimate them separately and combine the results to estimate the original semigroup. Similar results with additional assumption $a \\leqslant 1$ are obtained on $L^1(\\mathbb{R}^d)$.",
        "citation_title": "Dimension-free estimates for positivity-preserving Riesz transforms related to Schr\u00f6dinger operators with certain potentials",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "One of the important applications of Golay complementary sets (GCSs) is the reduction of peak-to-mean envelope power ratio (PMEPR) in orthogonal frequency division multiplexing (OFDM) systems. OFDM has played a major role in modern wireless systems such as long-term-evolution (LTE), 5th generation (5G) wireless standards, etc. This paper searches for systematic constructions of GCSs of arbitrary lengths and alphabet sizes. The proposed constructions are based on extended Boolean functions (EBFs). For the first time, we can generate codes of independent parameter choices.",
        "citation_title": "Systematic Construction of Golay Complementary Sets of Arbitrary Lengths and Alphabet Sizes",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Zero forcing in graphs is a coloring process where a colored vertex can force its unique uncolored neighbor to be colored. A zero forcing set is a set of initially colored vertices capable of eventually coloring all vertices of the graph. In this paper, we focus on the numbers $z(G; i)$, which is the number of zero forcing sets of size $i$ of the graph $G$. These numbers were initially studied by Boyer et al. where they conjectured that for any graph $G$ on $n$ vertices, $z(G; i) \\leq z(P_n; i)$ for all $i \\geq 1$ where $P_n$ is the path graph on $n$ vertices. The main aim of this paper is to show that several classes of graphs, including outerplanar graphs and threshold graphs, satisfy this conjecture. We do this by studying various graph operations and examining how they affect the number of zero forcing sets.",
        "citation_title": "Exploring the Influence of Graph Operations on Zero Forcing Sets",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this article, we study a simplified version of a density-dependent first-order mean field game, in which the players face a penalization equal to the population density at their final position. We consider the problem of finding an equilibrium when the initial distribution is a discrete measure. We show that the problem becomes finite-dimensional: the final piecewise smooth density is completely determined by the weights and positions of the initial measure. We establish existence and uniqueness of a solution using classical fixed point theorems. Finally, we show that Newton's method provides an effective way to compute the solution. Our numerical simulations provide an illustration of how density penalization in a mean field game tends to the smoothen the initial distribution.",
        "citation_title": "A Model Problem for First Order Mean Field Games with Discrete Initial Data",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This is the first in a sequence of four papers, where we prove the arithmetic Siegel--Weil formula in co-rank $1$ for Kudla--Rapoport special cycles on exotic smooth integral models of unitary Shimura varieties of arbitrarily large even arithmetic dimension. Our arithmetic Siegel--Weil formula implies that degrees of Kudla--Rapoport arithmetic special $1$-cycles are encoded in the first derivatives of unitary Eisenstein series Fourier coefficients.\nThe crucial input is a new local limiting method at all places. In this paper, we formulate and prove the key local theorems at all non-Archimedean places. On the analytic side, the limit relates local Whittaker functions on different groups. On the geometric side at nonsplit non-Archimedean places, the limit relates degrees of $0$-cycles on Rapoport--Zink spaces and local contributions to heights of $1$-cycles in mixed characteristic.",
        "citation_title": "Co-rank $1$ Arithmetic Siegel--Weil I: Local non-Archimedean",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This is the second in a sequence of four papers, where we prove the arithmetic Siegel--Weil formula in co-rank $1$ for Kudla--Rapoport special cycles on exotic smooth integral models of unitary Shimura varieties of arbitrarily large even arithmetic dimension. Our arithmetic Siegel--Weil formula implies that degrees of Kudla--Rapoport arithmetic special $1$-cycles are encoded in the first derivatives of unitary Eisenstein series Fourier coefficients.\nIn this paper, we formulate and prove the key Archimedean local theorem. In the case of purely Archimedean intersection numbers, we also prove an Archimedean local arithmetic Siegel--Weil formula, relating Green currents of arbitrary degree and off-central derivatives of local Whittaker functions. The crucial input is a new limiting method, which is structurally parallel to our strategy at non-Archimedean places.",
        "citation_title": "Co-rank $1$ Arithmetic Siegel--Weil II: Local Archimedean",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This is the third in a sequence of four papers, where we prove the arithmetic Siegel--Weil formula in co-rank $1$ for Kudla--Rapoport special cycles on exotic smooth integral models of unitary Shimura varieties of arbitrarily large even arithmetic dimension. Our arithmetic Siegel--Weil formula implies that degrees of Kudla--Rapoport arithmetic special $1$-cycles are encoded in the first derivatives of unitary Eisenstein series Fourier coefficients.\nIn this paper, we finish the reduction process from global arithmetic intersection numbers for special cycles to the local geometric quantities in our companion papers.\nBuilding on our previous companion papers, we also propose a construction for arithmetic special cycle classes associated to possibly singular matrices of arbitrary co-rank.",
        "citation_title": "Co-rank $1$ Arithmetic Siegel--Weil III: Geometric local-to-global",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This is the fourth in a sequence of four papers, where we prove the arithmetic Siegel--Weil formula in co-rank $1$ for Kudla--Rapoport special cycles on exotic smooth integral models of unitary Shimura varieties of arbitrarily large even arithmetic dimension. Our arithmetic Siegel--Weil formula implies that degrees of Kudla--Rapoport arithmetic special $1$-cycles are encoded in the first derivatives of unitary Eisenstein series Fourier coefficients.\nIn this paper, we pin down precise normalizations for some $U(m,m)$ Siegel Eisenstein series, give local Siegel--Weil special value formulas with explicit constants, and record a geometric Siegel--Weil result for degrees of complex $0$-cycles. Using this, we complete the proof of our arithmetic Siegel--Weil results by patching together the local main theorems from our companion papers.",
        "citation_title": "Co-rank $1$ Arithmetic Siegel--Weil IV: Analytic local-to-global",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We prove a reverse Lieb--Thirring inequality with a sharp constant for the matrix Schr\u00f6dinger equation on the half-line.",
        "citation_title": "Reverse Lieb--Thirring inequality for the half-line matrix Schr\u00f6dinger operator",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We consider the Euclidean vacuum for linearized gravity on the global de Sitter space, obtained from the Euclidean Green's function on the 4-sphere. We use the notion of Calder\u00f3n projectors to recover a quantum state for the Lorentzian theory on de Sitter space. We show that while the state is gauge invariant and Hadamard, it is not positive on the whole of the phase space. We show however that a suitable modification at low energies yields a well-defined Hadamard state on global de Sitter space.",
        "citation_title": "IR-fixed Euclidean vacuum for linearized gravity on de Sitter space",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We investigate the continuous extension of discrete shift translations on one dimensional quantum lattice systems. We focus on a specific construction provided by a quasi-free C*-flow on the one-dimensional fermion lattice system. This quasi-free C*-flow is heuristically generated by a long-range Hamiltonian consisting of two-body translation invariant interactions with $\\frac{1}{r}$-decay. We explore statistical-mechanical interpretation of such a long-range Hamiltonian, which may be more naturally associated with a momentum operator rather than the Hamiltonian. Through its explicit dynamical formulas, wherein notably the sinc function appears, the continuous shift translations reveal violations of causality and locality. Furthermore, we demonstrate that this quasi-free C*-flow, implementing the shift translations, cannot be extended to the one-dimensional quantum spin system via the Jordan-Wigner transformation.",
        "citation_title": "Continuous extension of the discrete shift translations on one-dimensional quantum lattice systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A new algebra, hitherto not encountered in the usual Lie algebraic varieties or supervarieties, is introduced. The paper explores the rich and novel structure of the algebra, and it compares it on the one hand with the Jordan-Lie Superalgebras studied by Okubo and Kamiya, and on the other, with the four usual Euclidean division rings of the reals, the complexes, the quaternions and the octonions that the algebra is seen to combine, extend and generalise. A potential physical application of the algebra is briefly alluded to at the end.",
        "citation_title": "Multiplicatively Ordered and Directed Hybrid Jordan-Lie Superalgebra",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This work investigates the intricate relationship between the q-boson model, a quantum integrable system, and classical integrable systems such as the Toda and KP hierarchies. Initially, we analyze scalar products of off-shell Bethe states and explore their connections to tau functions of integrable hierarchies. Furthermore, we discuss correlation functions within this formalism, examining their representations in terms of tau functions, as well as their Schur polynomial expansions.",
        "citation_title": "$Q$-Boson model and relations with integrable hierarchies",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We prove a rigorous lower bound on the correlation energy of interacting fermions in the mean-field regime for a wide class of singular interactions, including the Coulomb potential. Combined with the upper bound obtained in \\cite{ChrHaiNam-23b}, our result establishes an analogue of the Gell-Mann--Brueckner formula $c_{1}\\rho\\log\\left(\\rho\\right)+c_{2}\\rho$ for the correlation energy of the electron gas in the high-density limit. Moreover, our analysis allows us to go beyond mean-field scaling while still covering the same class of potentials.",
        "citation_title": "The Correlation Energy of the Electron Gas in the Mean-Field Regime",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We develop a Schwinger-Keldysh field theory (SKFT) for open quantum systems interacting with a dissipative environment and apply it to the spin-boson model as an archetypical example where the environment is composed of a bosonic bath. Prior SKFT developments of this type have been confined to the Markovian regime, as an alternative to a conventional description by the Lindblad quantum master equation (QME) which is a time-local matrix differential equation. Here we combine SKFT with a two-particle irreducible (2PI) action that resums a class of Feynman diagrams to infinite order. We obtain the time-evolution of the spin density matrix in the form of a system of integro-differential equations applicable to both Markovian and non-Markovian regimes. The latter regime--where taking into account memory effects becomes essential--poses a challenge for standard methods when trying to incorporate arbitrary properties of the system, bath, and length of time evolution. The SKFT+2PI-computed time evolution of the spin expectation values in the Markovian regime reproduces the solution of the Lindblad QME, as long as the system-bath coupling in the latter is adjusted by increasing it. In the non-Markovian regime, SKFT+2PI yields a nonperturbative solution that mimics results from both hierarchical equations of motion and tensor networks methods that we employ as benchmarks. Our SKFT+2PI approach can also access challenging cases, such as zero-temperature and sub-Ohmic bath, as well as arbitrary long evolution times. Taking into account favorable numerical cost of solving the integro-differential equations with increasing number of spins, time steps or dimensionality the SKFT+2PI approach offers a promising route for simulation of driven-dissipative systems in quantum computing or quantum magnonics and spintronics in the presence of a variety of (single or multiple) dissipative environments.",
        "citation_title": "Schwinger-Keldysh nonequilibrium quantum field theory of open quantum systems beyond the Markovian regime: Application to the spin-boson model",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Finsler geometry is a natural generalization of (pseudo-)Riemannian geometry, where the line element is not the square root of a quadratic form but a more general homogeneous function. Parameterizing this in terms of symmetric tensors suggests a possible interpretation in terms of higher-spin fields. We will see here that, at linear level in these fields, the Finsler version of the Ricci tensor leads to the curved-space Fronsdal equation for all spins, plus a Stueckelberg-like coupling. Nonlinear terms can also be systematically analyzed, suggesting a possible interacting structure. No particular choice of spacetime dimension is needed. The Stueckelberg mechanism breaks gauge transformations to a redundancy that does not change the geometry. This is however not enough to eliminate non-transverse modes, at least for some versions of Finsler dynamics.",
        "citation_title": "Higher spins and Finsler geometry",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In this paper, we present a controllability analysis of the quantum Ising periodic chain of n spin 1/2 particles where the interpolating parameter between the two Hamiltonians plays the role of the control. A fundamental result in the control theory of quantum systems states that the set of achievable evolutions is (dense in) the Lie group corresponding to the Lie algebra generated by the Hamiltonians of the system. Such a dynamical Lie algebra therefore characterizes all the state transitions available for a given system. For the Ising spin periodic chain we characterize such a dynamical Lie algebra and therefore the set of all reachable states. In particular, we prove that the dynamical Lie algebra is a (3n-1)-dimensional Lie sub-algebra of su(2^n) which is a direct sum of a two dimensional center and a (3n-3)-dimensional semisimple Lie subalgebra. This in turn is the direct sum of n-1 Lie algebras isomorphic to su(2) parametrized by the eigenvalues of a fixed matrix. We display the basis for each of these Lie subalgebras. Therefore the problem of control for the Ising spin periodic chain is, modulo the two dimensional center, a problem of simultaneous control of n-1 spin 1/2 particles. In the process of proving this result, we develop some tools which are of general interest for the controllability analysis of quantum systems with symmetry.",
        "citation_title": "Controllability of the Periodic Quantum Ising Spin Chain",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study the point spectrum of a second order difference operator with complex potential on the half-line via Fredholm determinants of the corresponding Birman-Schwinger operator pencils, the Evans and the Jost functions. An application is given to instability of a generalization of the Kolmogorov flow for the Euler equation of ideal fluid on the two dimensional torus.",
        "citation_title": "Characteristic determinants for a second order difference equation on the half-line arising in hydrodynamics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We numerically analyse solutions of the spherically symmetric gravitational Vlasov-Poisson system close to compactly supported stable steady states. We observe either partially undamped oscillations or macroscopically damped solutions. We investigate for many steady states to which of these behaviours they correspond. A linear relation between the exponents of polytropic steady states and the qualitative behaviour close to them is identified. Undamped oscillations are also observed around not too concentrated King models and around all shells with a sufficiently large vacuum region. We analyse all solutions both at the non-linear and linearised level and find that the qualitative behaviours are identical at both. To relate the observed phenomena to theoretical results, we further include a comprehensive numerical study of the radial particle periods in the equilibria.",
        "citation_title": "Numerical experiments on stationary, oscillating, and damped spherical galaxy models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We extend the port-Hamiltonian framework defined with respect to a Lagrangian submanifold and a Dirac structure by augmenting the Lagrangian submanifold with the space of external variables. The new pair of conjugated variables is called energy port. We show that in the most general case, the extension describes constrained Hamiltonian systems whose Hamiltonian function depends on inputs.",
        "citation_title": "Port-Hamiltonian systems with energy and power ports",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Using an exact holographic duality formula between the inhomogeneous 2d Ising model and 3d quantum gravity, we provide a formula for \"real\" zeroes of the 2d Ising partition function on finite graphs in terms of the geometry of a 2d triangulation embedded in the three-dimensional Euclidean space. The complex phase of those zeroes is given by the dihedral angles of the triangulation, which reflect its extrinsic curvature within the ambient 3d space, while the modulus is given by the angles within the 2d triangles, thus encoding the intrinsic geometry of the triangulation. Our formula can not cover the whole set of Ising zeroes, but we conjecture that a suitable complexification of these \"real\" zeroes would provide a more thorough formula. Nevertheless, in the thermodynamic limit, in the case of flat planar 2d triangulations, our Ising zeroes formula gives the critical couplings for isoradial graphs, confirming its generality. This approach shows an intricate, but precise, new relation between statistical mechanics and quantum geometry.",
        "citation_title": "2d Ising Critical Couplings from Quantum Gravity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Surface diffusion and surface electromigration may lead to a morphological instability of thin solid films and nanowires. In this paper two nonlinear analyzes of a morphological instability are developed for a single-crystal cylindrical nanowire that is subjected to the axial current. These treatments extend the conventional linear stability analyzes without surface electromigration, that manifest a Rayleigh-Plateau instability. A weakly nonlinear analysis is done slightly above the Rayleigh-Plateau (longwave) instability threshold. It results in a one-dimensional Sivashinsky amplitude equation that describes a blow-up of a surface perturbation amplitude in a finite time. This is a signature of a formation of an axisymmetric spike singularity of a cylinder radius, which leads to a wire pinch-off and separation into a disjoint segments. The scaling analysis of the amplitude spike singularity is performed, and the time-and-electric field-dependent dimensions of the spike are characterized. A weakly nonlinear multi-scale analysis is done at the arbitrary distance above a longwave or a shortwave instability threshold. The time-and-electric field-dependent Fourier amplitudes of the major instability modes are derived and characterized.",
        "citation_title": "On Nanowire Morphological Instability and Pinch-Off by Surface Electromigration",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "It is commonly believed that logical states of quantum error-correcting codes have to be highly entangled such that codes capable of correcting more errors require more entanglement to encode a qubit. Here we show that this belief may or may not be true depending on a particular code. To this end, we characterize a tradeoff between the code distance $d$ quantifying the number of correctable errors, and geometric entanglement of logical states quantifying their maximal overlap with product states or more general \"topologically trivial\" states. The maximum overlap is shown to be exponentially small in $d$ for three families of codes: (1) low-density parity check (LDPC) codes with commuting check operators, (2) stabilizer codes, and (3) codes with a constant encoding rate. Equivalently, the geometric entanglement of any logical state of these codes grows at least linearly with $d$. On the opposite side, we also show that this distance-entanglement tradeoff does not hold in general. For any constant $d$ and $k$ (number of logical qubits), we show there exists a family of codes such that the geometric entanglement of some logical states approaches zero in the limit of large code length.",
        "citation_title": "How much entanglement is needed for quantum error correction?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A formalism of classical mechanics is given for time-dependent many-body states of quantum mechanics, describing both fluid flow and point mass trajectories. The familiar equations of energy, motion, and those of Lagrangian mechanics are obtained. An energy and continuity equation is demonstrated to be equivalent to the real and imaginary parts of the time dependent Schroedinger equation, respectively, where the Schroedinger equation is in density matrix form. For certain stationary states, using Lagrangian mechanics and a Hamiltonian function for quantum mechanics, equations for point-mass trajectories are obtained. For 1-body states and fluid flows, the energy equation and equations of motion are the Bernoulli and Euler equations of fluid mechanics, respectively. Generalizations of the energy and Euler equations are derived to obtain equations that are in the same form as they are in classical mechanics. The fluid flow type is compressible, inviscid, irrotational, with the nonclassical element of local variable mass. Over all space mass is conserved. The variable mass is a necessary condition for the fluid flow to agree with the zero orbital angular momentum for s states of hydrogen. Cross flows are examined, where velocity directions are changed without changing the kinetic energy. For one-electron atoms, the velocity modification gives closed orbits for trajectories, and mass conservation, vortexes, and density stratification for fluid flows. For many body states, Under certain conditions, and by hypotheses, Euler equations of orbital-flows are obtained. One-body Schroedinger equations that are a generalization of the Hartree-Fock equations are also obtained. These equations contain a quantum Coulomb's law, involving the 2-body pair function of reduced density matrix theory that replace the charge densities.",
        "citation_title": "A Formulation of Quantum Fluid Mechanics and Trajectories",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We expound upon our (polarization-free) definition of the quantization map in geometric quantization, which is justified using the Poisson sigma model and pieces together most known quantization schemes. We use it to obtain the noncommutative torus and a finite dimensional irreducible representation. We discuss invariance of polarization using Schur's lemma.",
        "citation_title": "Geometric Quantization Without Polarizations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Given a quantum channel and a state which satisfy a fixed point equation approximately (say, up to an error $\\varepsilon$), can one find a new channel and a state, which are respectively close to the original ones, such that they satisfy an exact fixed point equation? It is interesting to ask this question for different choices of constraints on the structures of the original channel and state, and requiring that these are also satisfied by the new channel and state. We affirmatively answer the above question, under fairly general assumptions on these structures, through a compactness argument. Additionally, for channels and states satisfying certain specific structures, we find explicit upper bounds on the distances between the pairs of channels (and states) in question. When these distances decay quickly (in a particular, desirable manner) as $\\varepsilon\\to 0$, we say that the original approximate fixed point equation is rapidly fixable. We establish rapid fixability, not only for general quantum channels, but also when the original and new channels are both required to be unitary, mixed unitary or unital. In contrast, for the case of bipartite quantum systems with channels acting trivially on one subsystem, we prove that approximate fixed point equations are not rapidly fixable. In this case, the distance to the closest channel (and state) which satisfy an exact fixed point equation can depend on the dimension of the quantum system in an undesirable way. We apply our results on approximate fixed point equations to the question of robustness of quantum Markov chains (QMC) and establish the following: For any tripartite quantum state, there exists a dimension-dependent upper bound on its distance to the set of QMCs, which decays to zero as the conditional mutual information of the state vanishes.",
        "citation_title": "Robustness of Fixed Points of Quantum Channels and Application to Approximate Quantum Markov Chains",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A $p$-adic Brownian motion is a continuous time stochastic process in a $p$-adic state space that has a Vladimirov operator as its infinitesimal generator. The current work shows that any such process is the scaling limit of a discrete time random walk on a discrete group. Earlier work required the exponent of the Vladimirov operator to be in $(1, \\infty)$, and the convergence was the weak convergence of probability measures on the Skorohod space of paths on a compact time interval. The current approach simplifies the earlier approach, allows for any positive exponent, eliminates the restriction to compact time intervals, and establishes some moment estimates for the discrete time processes that are of independent interest.",
        "citation_title": "$p$-Adic Brownian Motion is a Scaling Limit",
        "date_delivered": "[Submitted on 12 Oct 2020 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We examine the validity of the kinetic description of wave turbulence for a model quadratic equation. We focus on the space-inhomogeneous case, which had not been treated earlier; the space-homogeneous case is a simple variant. We determine nonlinearities for which the kinetic description holds, or might fail, up to an arbitrarily small polynomial loss of the kinetic time scale. More precisely, we focus on the convergence of the Dyson series, which is an expansion of the solution in terms of the random data.",
        "citation_title": "Derivation of the kinetic wave equation for quadratic dispersive problems in the inhomogeneous setting",
        "date_delivered": "[Submitted on 25 Jul 2021 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Results concerning the existence and spectral stability and instability of multiple periodic wave solutions for the nonlinear Schr\u00f6dinger system with \\textit{dnoidal} and \\textit{cnoidal} profile will be determined in this manuscript. The spectral analysis for the corresponding linearized operator is established by using the comparison theorem and tools of Floquet theory. The main results are determined by applying the spectral stability theory in \\cite{KapitulaKevrekidisSandstedeI} and \\cite{KapitulaKevrekidisSandstedeII} via Krein signature.",
        "citation_title": "Spectral stability of multiple periodic waves for the Schrodinger system with cubic nonlinearity",
        "date_delivered": "[Submitted on 15 Dec 2022 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The purpose of unitary synthesis is to find a gate sequence that optimally approximates a target unitary transformation. A new synthesis approach, called probabilistic synthesis, has been introduced, and its superiority has been demonstrated over traditional deterministic approaches with respect to approximation error and gate length. However, the optimality of current probabilistic synthesis algorithms is unknown. We obtain the tight lower bound on the approximation error obtained by the optimal probabilistic synthesis, which guarantees the sub-optimality of current algorithms. We also show its tight upper bound, which improves and unifies current upper bounds depending on the class of target unitaries. These two bounds reveal the fundamental relationship of approximation error between probabilistic approximation and deterministic approximation of unitary transformations. From a computational point of view, we show that the optimal probability distribution can be computed by the semidefinite program (SDP) we construct. We also construct an efficient probabilistic synthesis algorithm for single-qubit unitaries, rigorously estimate its time complexity, and show that it reduces the approximation error quadratically compared with deterministic algorithms.",
        "citation_title": "Probabilistic unitary synthesis with optimal accuracy",
        "date_delivered": "[Submitted on 16 Jan 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We verify that a large portion of the theory of complex operator spaces and operator algebras (as represented by the 2004 book by the author and Le Merdy for specificity) transfers to the real case. We point out some of the results that do not work in the real case. We also discuss how the theory and standard constructions interact with the complexification. For example, we develop the real case of the theory of operator space multipliers and the operator space centralizer algebra, and discuss how these topics connect with the complexification. This turns out to differ in some important details from the complex case. We also characterize real structure in complex operator spaces; and give `real' characterizations of some of the most important objects in the subject.",
        "citation_title": "Real operator spaces and operator algebras",
        "date_delivered": "[Submitted on 29 Mar 2023 (v1), last revised 1 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We study Anderson localization in disordered tight-binding models on hyperbolic lattices. Such lattices are geometries intermediate between ordinary two-dimensional crystalline lattices, which localize at infinitesimal disorder, and Bethe lattices, which localize at strong disorder. Using state-of-the-art computational group theory methods to create large systems, we approximate the thermodynamic limit through appropriate periodic boundary conditions and numerically demonstrate the existence of an Anderson localization transition on the $\\{8,3\\}$ and $\\{8,8\\}$ lattices. We find unusually large critical disorder strengths, determine critical exponents, and observe a strong finite-size effect in the level statistics.",
        "citation_title": "Anderson localization transition in disordered hyperbolic lattices",
        "date_delivered": "[Submitted on 12 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Two-pointed quantum disks with a weight parameter $W>0$ is a canonical family of finite-volume random surfaces in Liouville quantum gravity. We extend the conformal welding of quantum disks in [AHS23] to the non-simple regime, and give a construction of the multiple SLE associated with any given link pattern for $\\kappa\\in(4,8)$. Our proof is based on connections between SLE and Liouville conformal field theory (LCFT), where we show that in the conformal welding of multiple forested quantum disks, the surface after welding can be described in terms of LCFT, and the random conformal moduli contains the SLE partition function for the interfaces as a multiplicative factor. As a corollary, for $\\kappa\\in(4,8)$, we prove the existence of the multiple SLE partition functions, which are smooth functions satisfying a system of PDEs and conformal covariance.",
        "citation_title": "Conformal welding of quantum disks and multiple SLE: the non-simple case",
        "date_delivered": "[Submitted on 31 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The Parisi formula for the free energy is among the crown jewels in the theory of spin glasses. We present a simpler proof of the lower bound in the case of the spherical mean-field model. Our method follows the TAP approach developed recently in e.g. (Subag, 2018): we obtain an ultrametric tree of pure states, each with approximately the same free energy as the entire model, which are hierarchically arranged in accordance with the Parisi ansatz. We construct this tree ``layer by layer'' given the minimizer to Parisi's variational problem. On overlap intervals with full RSB, the tree is built by an optimization algorithm due to Subag. On overlap intervals with finite RSB, the tree is constructed by a new truncated second moment argument; a similar argument also characterizes the free energy of the resulting pure states. Notably we do not use the Aizenman--Sims--Starr scheme, and require interpolation bounds only up to the 1RSB level. Our methods also yield results for large deviations of the ground state, including the entire upper tail rate function for all 1RSB models without external field.",
        "citation_title": "A Constructive Proof of the Spherical Parisi Formula",
        "date_delivered": "[Submitted on 27 Nov 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We classify connected \u00e9tale algebras $A$'s in pre-modular fusion categories $\\mathcal B$ with $\\text{rank}(\\mathcal B)\\le3$ including degenerate and non-(pseudo-)unitary ones. We comment on Lagrangian algebras and physical applications to ground state degeneracy and proof of spontaneous $\\mathcal B$-symmetry breaking.",
        "citation_title": "Classification of connected \u00e9tale algebras in pre-modular fusion categories up to rank three",
        "date_delivered": "[Submitted on 27 Nov 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We classify connected \u00e9tale algebras in (possibly non-unitary) modular fusion categories $\\mathcal B$'s with $\\text{rank}(\\mathcal B)\\le5$. We also comment on Lagrangian algebra, anyon condensation, and physical applications. Concretely, we prove certain spontaneous $\\mathcal B$-symmetry breaking and predict ground state degeneracies in massive renormalization group flows from non-unitary minimal models.",
        "citation_title": "Classification of connected \u00e9tale algebras in modular fusion categories up to rank five",
        "date_delivered": "[Submitted on 20 Dec 2023 (v1), last revised 2 May 2024 (this version, v5)]"
    },
    {
        "abstract": "These lecture notes, adapted from the habilitation thesis of the author, survey in a first part various exact results obtained in the past few decades about KPZ fluctuations in one dimension, with a special focus on finite volume effects describing the relaxation to its stationary state of a finite system starting from a given initial condition. The second part is more specifically devoted to an approach allowing to express in a simple way the statistics of the current in the totally asymmetric simple exclusion process in terms of a contour integral on a compact Riemann surface, whose infinite genus limit leads to KPZ fluctuations in finite volume.",
        "citation_title": "KPZ fluctuations in finite volume",
        "date_delivered": "[Submitted on 26 Jan 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "This paper is devoted to the study of existence and properties of solitary waves of the Benjamin equation. The studied equation includes a parameter $\\gamma$ in front of the Benjamin-Ono term. We show the existence, uniqueness, decay and orbital stability of solitary wave solutions obtained as a solution to a certain minimization problem, associated either with high speeds without a sign condition on the parameter $\\gamma$ or with low speeds for the appropriate sign.",
        "citation_title": "On the Uniqueness and Orbital Stability of Slow and Fast Solitary Wave Solutions of the Benjamin Equation",
        "date_delivered": "[Submitted on 6 Apr 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "In this paper, we propose a recipe for B-model computation of genus 1 Gromov-Witten invariants of Calabi-Yau and Fano Projective Hypersurfaces. Our formalism can be applied equally to both Calabi-Yau and Fano cases. In Calabi-Yau case, drastic cancellation of terms used in our formalism occurs and it results in another representation of BCOV-Zinger formula for projective Calabi-Yau hypersurfaces.",
        "citation_title": "Elliptic Virtual Structure Constants and Generalizations of BCOV-Zinger Formula to Projective Fano Hypersurfaces",
        "date_delivered": "[Submitted on 11 Apr 2024 (v1), last revised 2 May 2024 (this version, v5)]"
    },
    {
        "abstract": "The Kerr nonlinearity allows for exact analytic soliton solutions in 1+1D. While nothing excludes that these solitons form in naturally-occurring real-world 3D settings as solitary walls or stripes, their observation has previously been considered unfeasible because of the strong transverse instability intrinsic to the extended nonlinear perturbation. We report the observation of solitons that are fully compatible with the 1+1D Kerr paradigm limit hosted in a 2+1D system. The waves are stripe spatial solitons in bulk copper doped potassium-lithium-tantalate-niobate (KLTN) supported by the unsaturated photorefractive screening nonlinearity. The parameters of the stripe solitons fit well, in the whole existence domain, with the 1+1D existence curve that we derive for the first time in closed form starting from the saturable model of propagation. Transverse instability, that accompanies the solitons embedded in the 3D system, is found to have a gain length much longer than the crystal. Findings establish our system as a versatile platform for investigating exact soliton solutions in bulk settings and in exploring the role of dimensionality at the transition from integrable to non-integrable regimes of propagation.",
        "citation_title": "Evidence of 1+1D photorefractive stripe solitons deep in the Kerr limit",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present a tunable magnetoelastic lattice with a multistable onsite potential, focusing on a tristable potential. Through experimental and numerical analysis, we verify the existence of three types of transition waves with distinct amplitudes and velocities. Additionally, we establish the presence of a scaling law that elucidates various characteristics of these transition waves. By manipulating the onsite potential, we investigate the collision dynamics of two transition waves within the system. In chains featuring an asymmetric potential well, the collision of similar transition waves leads to the remote nucleation of a new phase. In chains with a symmetric potential well, the collision of dissimilar transition waves results in the formation of a stationary domain wall",
        "citation_title": "Remote Nucleation and Stationary Domain Walls via Transition Waves in Tristable Magnetoelastic Lattices",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Understanding pedestrian dynamics is crucial for appropriately designing pedestrian spaces. The pedestrian fundamental diagram (FD), which describes the relationship between pedestrian flow and density within a given space, characterizes these dynamics. Pedestrian FDs are significantly influenced by the flow type, such as uni-directional, bi-directional, and crossing flows. However, to the authors' knowledge, generalized pedestrian FDs that are applicable to various flow types have not been proposed. This may be due to the difficulty of using statistical methods to characterize the flow types. The flow types significantly depend on the angles of pedestrian movement; however, these angles cannot be processed by standard statistics due to their periodicity. In this study, we propose a comprehensive model for pedestrian FDs that can describe the pedestrian dynamics for various flow types by applying Directional Statistics. First, we develop a novel statistic describing the pedestrian flow type solely from pedestrian trajectory data using Directional Statistics. Then, we formulate a comprehensive pedestrian FD model that can be applied to various flow types by incorporating the proposed statistics into a traditional pedestrian FD model. The proposed model was validated using actual pedestrian trajectory data. The results confirmed that the model effectively represents the essential nature of pedestrian dynamics, such as the capacity reduction due to conflict of crossing flows and the capacity improvement due to the lane formation in bi-directional flows.",
        "citation_title": "Modeling pedestrian fundamental diagram based on Directional Statistics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work we investigate the set of cubic Hamiltonian vector fields for which their associated Kahan-Hirota-Kimura maps preserve the original Hamiltonian function. We analyze these fields in $\\mathbb{R}^2$ and $\\mathbb{R}^4$. We also study a family of fields in $\\mathbb{R}^6$. Additionally, we explore several properties like the existence of additional first integrals of specific type, the possibility that the initial Hamiltonian vector field is a Lie Symmetry of the corresponding map, or the symplecticity of the considered maps.",
        "citation_title": "Kahan-Hirota-Kimura maps preserving original cubic hamiltonians",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Neural networks have become a widely adopted tool for tackling a variety of problems in machine learning and artificial intelligence. In this contribution we use the mathematical framework of local stability analysis to gain a deeper understanding of the learning dynamics of feed forward neural networks. Therefore, we derive equations for the tangent operator of the learning dynamics of three-layer networks learning regression tasks. The results are valid for an arbitrary numbers of nodes and arbitrary choices of activation functions. Applying the results to a network learning a regression task, we investigate numerically, how stability indicators relate to the final training-loss. Although the specific results vary with different choices of initial conditions and activation functions, we demonstrate that it is possible to predict the final training loss, by monitoring finite-time Lyapunov exponents or covariant Lyapunov vectors during the training process.",
        "citation_title": "On the weight dynamics of learning networks",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "In this work, we investigated the spatial evolution of optical power in a closed-form optical waveguide configuration consisting of six passive waveguides and each of the waveguides exhibits equal strength of Kerr nonlinearity. We considered only nearest neighbor interaction between the waveguides. We found that in the case of low Kerr nonlinearity, evolution of optical power shows synchronization behavior. But when we increased the strength of Kerr nonlinearity, we discovered that spatial evolution of optical power in all waveguides shows independent characteristics. On the other hand, we have studied the impact of the coupling constant on the synchronization dynamics of our system. Our findings showed us that strong coupling can strengthen the collective dynamics in the presence of strong Kerr nonlinearity. From our results, we can conclude that Kerr nonlinearity in our system plays the role of disorder parameter that destroys as well as alters the synchronization behavior of evolution of optical power in the waveguides and coupling constant plays the role of an antagonist and restores synchronization in the model.",
        "citation_title": "Synchronization Dynamics in the Spatial Evolution of Optical Power in Optical Oligomer",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Li\u00e9nard-type nonlinear oscillators with linear and nonlinear damping terms exhibit diverse dynamical behavior in both the classical and quantum regimes. In this paper, we consider examples of various one-dimensional Li\u00e9nard type-I and type-II oscillators. The associated Euler-Lagrange equations are divided into groups based on the characteristics of the damping and forcing terms. The Li\u00e9nard type-I oscillators often display localized solutions, isochronous and non-isochronous oscillations and are also precisely solvable in quantum mechanics in general, where the ordering parameters play an important role. These include Mathews-Lakshmanan and Higgs oscillators. However, the classical solutions of some of the nonlinear oscillators are expressed in terms of elliptic functions and have been found to be quasi-exactly solvable in the quantum region. The three-dimensional generalizations of these classical systems add more degrees of freedom, which show complex dynamics. Their quantum equivalents are also explored in this article. The isotonic generalizations of the non-isochronous nonlinear oscillators have also been solved both classically and quantum mechanically to advance the studies. The modified Emden equation categorized as Li\u00e9nard type-II exhibits isochronous oscillations at the classical level. This property makes it a valuable tool for studying the underlying nonlinear dynamics. The study on the quantum counterpart of the system provides a deeper understanding of the behavior in the quantum realm as a typical PT-symmetric system.",
        "citation_title": "Li\u00e9nard Type Nonlinear Oscillators and Quantum Solvability",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This work investigates the intricate relationship between the q-boson model, a quantum integrable system, and classical integrable systems such as the Toda and KP hierarchies. Initially, we analyze scalar products of off-shell Bethe states and explore their connections to tau functions of integrable hierarchies. Furthermore, we discuss correlation functions within this formalism, examining their representations in terms of tau functions, as well as their Schur polynomial expansions.",
        "citation_title": "$Q$-Boson model and relations with integrable hierarchies",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The management of common-pool resources is a complex challenge due to the risk of overexploitation and the tragedy of the commons. A novel framework has been introduced to address this issue, focusing on the coevolutionary relationship between human behavior and common-pool resources within a human-environment system. However, the impact of the Allee effect on the coevolution and its resource sustainability is still unexplored. The Allee effect, a biological phenomenon characterized by a correlation between resource availability and growth rate, is a fundamental attribute of numerous natural resources. In this paper, we introduce two coevolutionary models of resource and strategy under replicator dynamics and knowledge feedback by applying the Allee effect to the common-pool resources within human-environment system. These models encapsulate various facets of resource dynamics and the players' behavior, such as resource growth function, the extraction rates, and the strategy update rules. We find that the Allee effect can induce bi-stability and critical transition, leading to either sustainable or unsustainable outcomes depending on the initial condition and parameter configuration. We demonstrate that knowledge feedback enhances the resilience and sustainability of the coevolving system, and these results advances the understanding of human-environment system and management of common-pool resources.",
        "citation_title": "The role of the Allee effect in common-pool resource and its sustainability",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In Gale Crater near Mars' equator, dunes and ripples of sand stand out from the general orderless, rocky terrain. In addition, images from Curiosity, the Mars Science Laboratory rover, reveal more subtle orderly forms: widespread, meter-scale domains of evenly spaced, pebble-size rocks (termed clasts) on wind-blown sand in scattered locations. Here, we examine quantitatively several clast domains on both Mars and Earth, and compare their geometry with that of random points. The clast distributions are more orderly than expected by chance; they differ significantlty from those associated with uniform (Poisson) random processes. Moreover, they are hyperuniform, a self-organized state recently recognized in diverse active materials and biological systems but that appears novel for planetary surfaces. These patches are often surrounded by recent wind-borne ripples, suggesting an interplay between sand transport, ripple activity and clasts. Using numerical simulations, we show that clast displacements induced by gravity, combined with the evolution of the sand surface caused by aeolian sand transport and ripple migration, can produce realistic hyperuniform and random clast distributions, as well as distinct clast alignements. Our findings highlight the existence of easily overlooked disordered hyperuniform states on ground surfaces, suggesting novel self-organized states beyond distinct geometric patterns.",
        "citation_title": "Hyperuniformity on Mars: Pebbles scattered on sand",
        "date_delivered": "[Submitted on 21 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In the recent decades of high-energy physics research, it was demonstrated that strongly interacting quark-gluon plasma (sQGP) is created in ultra-relativistic nucleus-nucleus collisions. Investigation and understanding of the properties of the hadronic matter are among the most important goals of the NA61/SHINE collaboration at CERN SPS. Mapping of the phase diagram is achieved by varying the collision energy (5 GeV $\\sqrt{s_{NN}}<17$ GeV) and by changing the collision system ($p$+$p$, $p$+Pb, Be+Be, Ar+Sc, Xe+La, Pb+Pb). Femtoscopic correlations reveal the space-time structure of the hadron emitting source.\nIn this article, we report on the measurement of femtoscopic correlations in small to intermediate systems. Comparing the measurements to calculations based on symmetric L\u00e9vy sources, we discuss the results on L\u00e9vy source parameters as a function of average pair transverse mass. One of the physical parameters is of particular importance, the L\u00e9vy exponent $\\alpha$, which describes the shape of the source and may be related to the critical exponent $\\eta$ in the proximity of the critical point. Therefore, measuring it may shed light on the location of the critical endpoint of the QCD phase diagram.",
        "citation_title": "Femtoscopy with L\u00e9vy sources at NA61/SHINE",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Beta-delayed proton and gamma emission in the decay of $^{21}$Mg has been measured at ISOLDE, CERN with the ISOLDE Decay Station (IDS) set-up. The existing decay scheme is updated, in particular what concerns proton transitions to excited states in $^{20}$Ne. Signatures of interference in several parts of the spectrum are used to settle spin and parity assignments to highly excited states in $^{21}$Na. The previously reported $\\beta$p$\\alpha$ branch is confirmed. A half-life of 120.5(4) ms is extracted for $^{21}$Mg. The revised decay scheme is employed to test mirror symmetry in the decay and to extract the beta strength distribution of $^{21}$Mg that is compared with theory.",
        "citation_title": "Detailed study of the decay of $^{21}$Mg",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose to measure the weak decay constant $\\alpha_-$ for the decay $\\Lambda\\rightarrow p\\pi^-$ using a both circularly and linearly polarized photon beam with the GlueX spectrometer in Hall D. The measurement will take advantage of the fact that a measurement with both linear and circular photon beam polarization results in an over-constrained set of amplitudes which can be fitted to data and used to extract $\\alpha_-$ which will be left as a free parameter in the fit. We expect to determine $\\alpha_-$ with statistical uncertainties comparable to existing measurements and independent systematic uncertainties. This measurement can be performed alongside GlueX-II running and requires no new hardware or new beam time. The measurement requires that a sufficient fraction of the electron beam polarization be longitudinal in the Hall D tagger.",
        "citation_title": "Proposal for PAC 52: Measurement of $\u03b1_-$ for $\u039b\\rightarrow p\u03c0^-$",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Heavy flavour production in proton-proton (pp) collisions provides insights into the fundamental properties of Quantum Chromodynamics (QCD). Beauty hadron production measurements are widely performed through indirect approaches based on their inclusive decay modes. A Bayesian unfolding data-driven analysis of the ALICE and LHCb data was performed in this study, which recovers the full kinematic information of the beauty hadrons via different inclusive decay channels. The corresponding beauty hadron production cross sections obtained after the Bayesian unfolding are found to be consistent within their uncertainties. The weighted average open beauty production cross sections are presented as a function of the transverse momentum and rapidity in pp collisions at $\\sqrt{s}$ = 5.02 TeV and $\\sqrt{s}$ = 13 TeV, respectively. The $p_T$-integrated open beauty production $\\mathrm{d}\\sigma/\\mathrm{d}y$ and the total $\\mathrm{b}\\rm\\overline{b}$ cross section $\\sigma_{\\rm \\mathrm{b}\\rm\\overline{b}}$ are also reported. The precision of these results significantly improves upon worldwide measurements, providing valuable validation and constraints on mechanisms of heavy flavour production in pp collisions at the LHC energies.",
        "citation_title": "Data-driven analysis of the beauty hadron production in p+p collisions at the LHC with Bayesian unfolding",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the production of $X(3872)$ mesons in photon-induced nuclear reactions near the threshold within the collision model based on the nuclear spectral function. The model accounts for direct photon-nucleon $X(3872)$ production processes as well as five different scenarios for their internal structure. We calculate the absolute and relative excitation functions for $X(3872)$ production off $^{12}$C and $^{184}$W target nuclei at near-threshold incident photon energies of 8--16 GeV, the absolute differential cross sections for their production off these target nuclei at laboratory angles of 0$^{\\circ}$--10$^{\\circ}$ and for incident photon energy of 13 GeV as well as the A dependences of the relative (transparency ratios) cross sections for $X(3872)$ production from ${\\gamma}A$ collisions at photon energies around 13 GeV within the adopted scenarios for the $X(3872)$ meson internal structure. We show that the absolute and relative observables considered reveal distinct sensitivity to these scenarios. Therefore, the measurement of such observables in a dedicated experiment at the CEBAF facility in the near-threshold energy range will allow us to get valuable information on the $X(3872)$ inner structure.",
        "citation_title": "Probing the structure of $X(3872)$ in photoproduction",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper presents a novel method for low maintenance, low ambiguity in-situ drift velocity monitoring in large volume Time Projection Chambers (TPCs). The method was developed and deployed for the 40m^3 TPC tracker system of the NA61/SHINE experiment at CERN, which has a one meter of drift length. The method relies on a low-cost multi-wire proportional chamber (MWPC) placed downstream of the TPCs to be monitored. The drift velocity is then determined by matching the reconstructed tracks in the TPC to the hits of the pertinent monitoring chamber, called Geometry Reference Chamber (GRC), which is then used as a differential length scale. An important design requirement on the GRC was minimal added complexity to the existing system, in particular, compatibility with Front-End Electronics (FEE) cards already used to read out the TPCs. Moreover, the GRC system was designed to operate both in large and small particle flux. The system is capable of monitoring the evolution of the in-situ drift velocity down to a one permil precision, with a few minutes of time sampling.",
        "citation_title": "Novel method for in-situ drift velocity measurement in large volume TPCs: the Geometry Reference Chamber of the NA61/SHINE experiment at CERN",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The predictive power of the deformed relativistic Hartree-Bogoliubov theory in continuum (DRHBc) with density functional PC-PK1 is demonstrated for superheavy region ($101 \\leqslant Z \\leqslant 120$) by comparing with available experimental and empirical mass data in the AME2020. The DRHBc theory predicts 93 bound nuclei beyond the drip line $N = 258$ in the region of $106 \\leqslant Z \\leqslant 112$, which form a stability peninsula. The odd-even differences between odd-$N$ and even-$N$ nuclei are remarkable in the stability peninsula; the number of bound odd-$N$ nuclei is less than that of bound even-$N$ nuclei, and the one-neutron separation energy of an odd-$N$ nucleus is smaller than those of its neighboring even-$N$ nuclei due to the blocking effect. The deformation effect is indispensable for the reentrant stability beyond the drip line by significantly affecting the structure of single-particle levels around the Fermi energy. \\textbf{\\textcolor{red}{The interplay between deformation and pairing effects affects the position where the odd-$N$ nucleus becomes bound in the stability peninsula.}} By examining the deformation effect at different orders, it is found that \\textbf{\\textcolor{red}{quadrupole deformation makes leading contribution to the appearance of stability peninsula and the effects of hexadecapole and hexacontatetrapole deformations are nonnegligible.",
        "citation_title": "The odd-even differences in stability peninsula for $106 \\leqslant Z \\leqslant 112$ region with the deformed relativistic Hartree-Bogoliubov theory in continuum",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Low-Gain Avalanche Diode (LGAD) is a new silicon detector and holds wide application prospects in particle physics experiments due to its excellent timing resolution.\nThe LGAD with a pixel size of 1.3 mm $\\times$ 1.3 mm was used to construct a High Granularity Timing Detector (HGTD) in ATLAS experiments to solve the pile-up problem.\nMeanwhile, the Circular Electron Positron Collider (CEPC) also proposes detectors using the LGAD. However, pixel LGAD exhibits higher readout electronics density and cost, which somewhat limits the application of LGADs. To decrease the readout electronics density, the Institute of High Energy Physics (IHEP) of the Chinese Academy of Sciences has designed strip LGADs with larger areas. These strip LGADs are all 19 mm in length but with different widths of 1.0 mm, 0.5 mm, and 0.3 mm.\nThis article provides a detailed introduction to the design parameters of these strip LGADs and tests their electrical characteristics, including leakage current, break-down voltage, depletion capacitance, etc.\nThe timing resolution and signal-to-noise ratio of the three strip LGAD sensors were investigated using a beta source test system.\nThe position resolution parallel to the strip direction was tested and analyzed for the first time using a pico-second laser test system.\nTests have demonstrated that the timing resolution of strip LGADs can reach about 37.5 ps, and position resolution parallel to the strip direction is better than 1 mm.",
        "citation_title": "Development of the strip LGAD detector with double-end readout for future colliders",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present an extensive study of hexadecapole correlations in the rare-earth region near $N=90$ and the effects these correlations have on various nuclear properties, such as the low-energy spectra, as well as quadrupole, hexadecapole, and monopole transition strengths. In order to examine hexadecapole correlations, we employ a mapped $sdg$ interacting boson model, with parameters derived from a self-consistent mean-field calculations with a relativistic energy density functional. We apply this model to even-even isotopes of Nd, Sm, Gd, Dy, and Er ($Z=60 - 68$) with neutron numbers $N=84-96$. The obtained results show a good agreement with the experiment. By comparing the results with the ones obtained from a simpler mapped $sd$ interacting boson model, we show that the inclusion of the hexadecapole degree of freedom via $g$ boson is necessary to improve the results of the $J^{\\pi} \\geq 6^{+}$ yrast energies in the nuclei with $N=84$ and 86, being near the neutron shell closure. The $sdg$ interacting boson model increases the quadrupole transition strengths between yrast states in the $N=90$ and 92 well deformed nuclei, which is in good agreement with the experiment for most of those isotopes. The presence of $g$ bosons does have an important effect on hexadecapole transition strengths, although experimental data for such transitions are limited. The obtained monopole transition strengths do not differ significantly from the ones obtained from the simpler $sd$ model.",
        "citation_title": "Microscopic description of hexadecapole collectivity in even-even rare-earth nuclei near $N=90$",
        "date_delivered": "[Submitted on 6 Mar 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Despite their high relative abundance in our Universe, neutrinos are the least understood fundamental particles of nature. They also provide a unique system to study quantum coherence and the wavelike nature of particles in fundamental systems due to their extremely weak interaction probabilities. In fact, the quantum properties of neutrinos emitted in experimentally relevant sources are virtually unknown and the spatial extent of the neutrino wavepacket is only loosely constrained by reactor neutrino oscillation data with a spread of 13 orders of magnitude. Here, we present the first direct limits of this quantity through a new experimental concept to extract the energy width, $\\sigma_{\\textrm{N},E}$, of the recoil daughter nucleus emitted in the nuclear electron capture (EC) decay of $^7$Be. The final state in the EC decay process contains a recoiling $^7$Li nucleus and an electron neutrino ($\\nu_e$) which are entangled at their creation. The $^7$Li energy spectrum is measured to high precision by directly embedding $^7$Be radioisotopes into a high resolution superconducting tunnel junction that is operated as a cryogenic sensor. The lower limit on the spatial uncertainty of the recoil daughter was found to be $\\sigma_{\\textrm{N}, x} \\geq 6.2$\\,pm, which implies the final-state system is localized at a scale more than a thousand times larger than the nucleus itself. From this measurement, the first direct lower limits on the spatial extent of the neutrino wavepacket were extracted using two different theoretical methods. These results have wide-reaching implications in several areas including the nature of spatial localization at sub-atomic scales, interpretation of neutrino physics data, and the potential reach of future large-scale experiments.",
        "citation_title": "Direct Experimental Constraints on the Spatial Extent of a Neutrino Wavepacket",
        "date_delivered": "[Submitted on 3 Apr 2024 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The AC-coupled Strip LGAD (Strip AC-LGAD) is a novel LGAD design that diminishes the density of readout electronics through the use of strip electrodes, enabling the simultaneous measurement of time and spatial information. The Institute of High Energy Physics has designed a long Strip AC-LGAD prototype with a strip electrode length of 5.7 mm and pitches of 150 $\\mu m$, 200 $\\mu m$, and 250 $\\mu m$. Spatial and timing resolutions of the long Strip AC-LGAD are studied by pico-second laser test and beta source tests. The laser test demonstrates that spatial resolution improves as the pitch size decreases, with an optimal resolution achieved at 8.3 $\\mu$m. Furthermore, the Beta source test yields a timing resolution of 37.6 ps.",
        "citation_title": "The Performance of AC-coupled Strip LGAD developed by IHEP",
        "date_delivered": "[Submitted on 8 Jul 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Anisotropy scaling functions derived from comprehensive measurements of transverse momentum- and centrality-dependent anisotropy coefficients $v_2(p_T,\\text{cent})$ and $v_3(p_T,\\text{cent})$ in Pb+Pb collisions at 5.02 and 2.76 TeV, and Xe+Xe collisions at 5.44 TeV at the LHC, offer new insights into the `ultra-central flow puzzle'. These functions integrate diverse measurements into a single curve, clarifying anisotropy attenuation throughout the entire $p_T$ and centrality range. They reveal the influence of initial-state eccentricities ($\\varepsilon_{n}$), dimensionless size ($\\mathbb{R}$), radial flow, viscous correction to the thermal distribution function ($\\delta_f$), the medium's stopping power ($\\hat{q}$), and specific shear viscosity ($\\eta/s$) on the observed anisotropies. This analysis not only enhances understanding of transport coefficients but also provides crucial constraints on nuclear deformation.",
        "citation_title": "Anisotropy Scaling Functions in Heavy-Ion Collisions: Insights into the `Ultra-Central Flow Puzzle' and Constraints on Transport Coefficients and Nuclear Deformation",
        "date_delivered": "[Submitted on 14 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The magnetic moment yields an excellent framework to explore the inner structure of particles determined by the quark-gluon dynamics of QCD, as it is the leading-order response of a bound system to a weak external magnetic field. Motivated by this, in this study, the magnetic moments of possible axial-vector $T_{bc\\bar u \\bar u}$, $T_{bc\\bar d \\bar d}$, and $T_{bc\\bar u \\bar d}$ tetraquarks are obtained with the help of light-cone QCD sum rules. For this purpose, we assume that these states are represented as a diquark-antidiquark picture with different structures and interpolating currents. The magnetic moment results derived using different diquark-antidiquark configurations differ substantially from each other. This can be translated into more than one tetraquark state with the same quantum number and quark content yet possessing different magnetic moments. From the numerical results obtained, we have concluded that the magnetic moments of the $T_{bc}$ states can project their inner structure, which can be used for their quantum numbers and quark-gluon organization. The contribution of individual quarks to the magnetic moments is also analyzed for completeness. We hope that our predictions of the magnetic moments of the $T_{bc}$ tetraquarks, together with the results of other theoretical investigations of the spectroscopic parameters and decay widths of these interesting tetraquarks, may be valuable in the search for these states in future experiments and in unraveling the internal structure of these tetraquarks.",
        "citation_title": "Unveiling the underlying structure of axial-vector bottom-charm tetraquarks in the light of their magnetic moments",
        "date_delivered": "[Submitted on 24 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The predictive power of the deformed relativistic Hartree-Bogoliubov theory in continuum (DRHBc) with density functional PC-PK1 is demonstrated for superheavy region ($101 \\leqslant Z \\leqslant 120$) by comparing with available experimental and empirical mass data in the AME2020. The DRHBc theory predicts 93 bound nuclei beyond the drip line $N = 258$ in the region of $106 \\leqslant Z \\leqslant 112$, which form a stability peninsula. The odd-even differences between odd-$N$ and even-$N$ nuclei are remarkable in the stability peninsula; the number of bound odd-$N$ nuclei is less than that of bound even-$N$ nuclei, and the one-neutron separation energy of an odd-$N$ nucleus is smaller than those of its neighboring even-$N$ nuclei due to the blocking effect. The deformation effect is indispensable for the reentrant stability beyond the drip line by significantly affecting the structure of single-particle levels around the Fermi energy. \\textbf{\\textcolor{red}{The interplay between deformation and pairing effects affects the position where the odd-$N$ nucleus becomes bound in the stability peninsula.}} By examining the deformation effect at different orders, it is found that \\textbf{\\textcolor{red}{quadrupole deformation makes leading contribution to the appearance of stability peninsula and the effects of hexadecapole and hexacontatetrapole deformations are nonnegligible.",
        "citation_title": "The odd-even differences in stability peninsula for $106 \\leqslant Z \\leqslant 112$ region with the deformed relativistic Hartree-Bogoliubov theory in continuum",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "By considering the one loop background field method for a quark-antiquark interaction mediated by one (non perturbative) gluon exchange, sixth order quark effective interactions are derived and investigated in the limit of zero momentum transfer. They extend fourth order quark interactions worked out in previous works of the author. These interactions break $U_A(1)$ symmetry and may be either momentum independent or dependent. Part of these $U_A(1)$ breaking interactions vanish in the limit of massless quarks and several other - involving vector and/or axial quark currents - survive even in the absence of Dynamical Symmetry Breaking. By means of the auxiliary field method, these interactions give rise to three meson interactions whose values are compared to phenomenological values found in the literature. By restricting to the up and down quark sector, some three-meson couplings correspond to the strong decay of specific light axial-vector mesons. These (strong) decays are shown to yield an asymmetry in the production rate of positive and negative pions by means of interference effects that arise due to neutral meson mixings.",
        "citation_title": "$U_A(1)$ symmetry breaking quark interactions from vacuum polarization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Understanding the properties and physical phase of the dense strongly interacting matter present in the cores of neutron stars or created in their binary mergers remains one of the most prominent open problems in nuclear astrophysics. While most microscopic analyses have historically relied on solvable phenomenological models of nuclear and quark matter, in recent years a model-independent approach utilizing only controlled ab-initio calculations and astrophysical observations has emerged as a viable alternative.\nIn these lecture notes, I review recent progress in first-principles weak-coupling calculations within high-density quark matter, shedding light on its thermodynamic and transport properties. I cover the most important technical tools used in such calculations, introduce selected highlight results, and explain how this information can be used in phenomenological studies of neutron-star physics. The notes do not offer a self-consistent treatment of the topics covered, but rather aim at filling gaps in existing textbooks on thermal field theory and at connecting the dots in a story developed in several recent research articles, to which the interested reader is directed for further technical details.",
        "citation_title": "Particle-theory input for neutron-star physics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Anisotropy scaling functions derived from comprehensive measurements of transverse momentum- and centrality-dependent anisotropy coefficients $v_2(p_T,\\text{cent})$ and $v_3(p_T,\\text{cent})$ in Pb+Pb collisions at 5.02 and 2.76 TeV, and Xe+Xe collisions at 5.44 TeV at the LHC, offer new insights into the `ultra-central flow puzzle'. These functions integrate diverse measurements into a single curve, clarifying anisotropy attenuation throughout the entire $p_T$ and centrality range. They reveal the influence of initial-state eccentricities ($\\varepsilon_{n}$), dimensionless size ($\\mathbb{R}$), radial flow, viscous correction to the thermal distribution function ($\\delta_f$), the medium's stopping power ($\\hat{q}$), and specific shear viscosity ($\\eta/s$) on the observed anisotropies. This analysis not only enhances understanding of transport coefficients but also provides crucial constraints on nuclear deformation.",
        "citation_title": "Anisotropy Scaling Functions in Heavy-Ion Collisions: Insights into the `Ultra-Central Flow Puzzle' and Constraints on Transport Coefficients and Nuclear Deformation",
        "date_delivered": "[Submitted on 14 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The high temperatures reached in a proto-neutron star or during the post-merger phase of a binary neutron star coalescence lead to non-negligible thermal effects on the equation of state (EOS) of dense nuclear matter. Here we study these effects within the covariant density functional theory employing the posteriors of a Bayesian inference, which encompasses a large sample of EOS models. Different densities and temperatures are considered. We find that for a number of quantities thermal effects are strongly correlated with the Dirac effective mass ($m^*$) of the nucleons and/or its logarithmic derivative as a function of density. These results can be explained within the low temperature approximation though they survive beyond this limit.",
        "citation_title": "Bayesian inference of thermal effects in dense matter within the covariant density functional theory",
        "date_delivered": "[Submitted on 22 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "To assess the potential of cold-fusion for synthesizing superheavy nuclei (SHN) with proton numbers 104-113, we systematically calculated 145 naturally occurring projectile-target combinations within the DNS model. Reactions predominantly show maximum cross-sections in the 1n to 2n channels, peaking near the Coulomb barrier with a sum of barrier and Q-value within 30 MeV. The maximum cross-section occurs below the Bass barrier, suggesting either the Bass model's limitation or significant deformation reducing the effective Coulomb barrier. Our calculations align well with experimental data, revealing that more neutron-rich projectiles slightly enhance fusion, though the effect is minor. For fixed targets (Pb, Bi), evaporation residue cross-sections decrease linearly with increasing projectile proton number, attributed to reduced fusion probability and lower fission barriers in heavier SHN. The touching potential $V_{\\rm in}$ shows a linear trend with the product of projectile-target proton numbers, with neutron-rich systems exhibiting lower $V_{\\rm in}$. Some reactions with $V_{\\rm in} < V_{\\rm S}$ may involve nucleon transfer before capture. Based on the DNS model, we identified optimal combinations and collision energies for synthesizing SHN with significant cross-sections. Collectively, our findings indicate that cold fusion is a promising avenue for creating proton-rich SHN around the drip line in the Z=104-113 region, offering distinct advantages over alternative mechanisms.",
        "citation_title": "Exploring the potential of synthesizing unknown superheavy isotopes via cold-fusion reactions based on the dinuclear system model",
        "date_delivered": "[Submitted on 30 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this article, a quantum variant of chess is introduced, which can be played on a traditional board, without using computers or other electronic devices. The rules of the game arise naturally by combining the rules of conventional chess with key quantum-physical effects such as superposition and entanglement. Niel's Chess is recommended for ages 10 and above, to everyone who wishes to play a creative game with historical roots and at the same time gain intuition about the foundational quantum effects that power cutting-edge technologies like quantum computing and quantum communication, which are poised to revolutionize our society in the coming decades. Takeaways from a pilot educational session that was carried out with 10-to-12-year-old children are also presented.",
        "citation_title": "Niel's Chess: A Quantum Game for Schools and the General Public",
        "date_delivered": "[Submitted on 4 Jan 2024]"
    },
    {
        "abstract": "Interest in science fiction's (SF's) potential science communication use is hindered by concerns about SF misrepresenting science. This study addresses these concerns by asking how SF media reflects scientific findings in exoplanet science. A database of SF exoplanets was analysed using a Bayesian network to find interconnected interactions between planetary characterisation features and literary data. Results reveal SF exoplanets designed after the discovery of real exoplanets are less Earth-like, providing statistical evidence that SF incorporates rapidly-evolving science. Understanding SF's portrayal of science is crucial for its potential use in science communication.",
        "citation_title": "Science Fiction Media Representations of Exoplanets: Portrayals of Changing Astronomical Discoveries",
        "date_delivered": "[Submitted on 4 Mar 2024]"
    },
    {
        "abstract": "Data are given, commentary is supplied and explanations are provided with regard to the technical, the organizational and, of course, the human history connected to the time of research, which resulted to the paper entitled \"Soil sampling and Cs-137 analysis of the Chernobyl fallout in Greece\", written by late Professor S.E. Simopoulos. This paper has been provided in Greek translation within an issued honorary volume (ISBN 978-960-254-714-4). Reasonably, the narration starts with the review of the political, the financial and the social situation of Greece around 1986. Subsequently, an analysis is given on the then available means, the persons involved, the methods used, the lessons learned and any other connection with the oral history of the NTUA's Nuclear Engineering Laboratory and other relevant Greek Laboratories. For this history, written proof is now scarce and the persons available to pass it on are growing less and less. N.P. Petropoulos, now Laboratory member and then student of Professor S.E. Simopoulos was in charge of preparation of this text.",
        "citation_title": "Technical, organizational and oral history regarding the soil samples measurements for Cs-137 because of the Chernobyl accident fallout",
        "date_delivered": "[Submitted on 24 Mar 2024]"
    },
    {
        "abstract": "The formation of protein precursors, due to the condensation of atomic carbon under the low-temperature conditions of the molecular phases of the interstellar medium, opens alternative pathways for the origin of life. We perform peptide synthesis under conditions prevailing in space and provide a comprehensive analytic characterization of its products. The application of 13C allowed us to confirm the suggested pathway of peptide formation that proceeds due to the polymerization of aminoketene molecules that are formed in the C + CO + NH3 reaction. Here, we address the question of how the efficiency of peptide production is modified by the presence of water molecules. We demonstrate that although water slightly reduces the efficiency of polymerization of aminoketene, it does not prevent the formation of peptides.",
        "citation_title": "Formation of extraterrestrial peptides and their derivatives",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Compact autonomous marine vehicles, both surface and submersible, are now commonly used to conduct observations of ocean velocities using Acoustic Doppler Current Profilers (ADCPs). However, in the inevitable presence of surface waves, ADCP measurements conducted by these platforms are susceptible to biases stemming from wave-coherent orbital motion and platform tilting. In typical ocean conditions, the magnitude of the bias can reach tens of centimeters per second. This paper presents analytical derivation of the depth-dependent bias formulas for a variety of scenarios, encompassing surface and subsurface platforms, upward- and downward-looking ADCPs, free-drifting and self-propelled vehicles. The bias is shown to be a function of the wave field properties, platform response dynamics, and the ADCP configuration (particularly, orientation and beam angle). In all cases, the wave-induced biases show parametric scaling similar to that of the Stokes drift, albeit with a number of critical nuances. Analytical derivations are validated with a semi-analytical model, which can also be used to estimate the biases for more complex measurement configurations. Further analysis reveals unexpected fundamental differences between the upward- and downward-looking ADCP configurations, offering insights for experimental design aimed at minimizing and mitigating wave-induced biases in autonomous oceanographic observations.",
        "citation_title": "Wave-induced biases in ADCP measurements from quasi Lagrangian platforms",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The ability to control the behavior of fluid instabilities at material interfaces, such as the shock-driven Richtmyer--Meshkov instability, is a grand technological challenge with a broad number of applications ranging from inertial confinement fusion experiments to explosively driven shaped charges. In this work, we use a linear-geometry shaped charge as a means of studying methods for controlling material jetting that results from the Richtmyer--Meshkov instability. A shaped charge produces a high-velocity jet by focusing the energy from the detonation of high explosives. The interaction of the resulting detonation wave with a hollowed cavity lined with a thin metal layer produces the unstable jetting effect. By modifying characteristics of the detonation wave prior to striking the lined cavity, the kinetic energy of the jet can be enhanced or reduced. Modifying the geometry of the liner material can also be used to alter jetting properties. We apply optimization methods to investigate several design parameterizations for both enhancing or suppressing the shaped-charge jet. This is accomplished using 2D and 3D hydrodynamic simulations to investigate the design space that we consider. We also apply new additive manufacturing methods for producing the shaped-charge assemblies, which allow for experimental testing of complicated design geometries obtained through computational optimization. We present a direct comparison of our optimized designs with experimental results carried out at the High Explosives Application Facility at Lawrence Livermore National Laboratory.",
        "citation_title": "Explosively driven Richtmyer--Meshkov instability jet suppression and enhancement via coupling machine learning and additive manufacturing",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The concept of Hydrodynamic Cavitation (HC) has emerged as a promising method for wastewater treatment, bio-diesel production and multiple other environmental processes with Venturi-type cavitation reactors showing particular advantages. However, numerical simulations of a venturi-type reactor with an elucidated explanation of the underlying flow physics remain inadequate. The present study numerically investigates and analyzes the flow inside a venturi-type reactor from both global cavity dynamics and localized turbulence statistics perspectives. Some models in the Detached Eddy Simulation (DES) family are employed to model the turbulence with the study initially comparing 2D simulations before extending the analysis to 3D simulations. The results show that while URANS models show significantly different dynamics as a result of grid refinement, the DES models show standard flow dynamics associated with cavitating flows. Nevertheless, signifi- cant discrepancies continue to exist when comparing the turbulence statistics on the local scale. As the discussion extends to 3D calculations, the DES models are able to well predict the turbulence phenomena at the local scale and reveal some new insights regarding the role of baroclinic torque into the cavitation-vortex interaction.The findings of this study thus contribute to the fundamental understandings of the venturi-type reactor.",
        "citation_title": "Numerical investigation of three-dimensional effects of cavitating flow in a venturi-type hydrodynamic cavitation reactor",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Understanding the dynamic nature of the semiconductor-water interface is crucial for developing efficient photoelectrochemical water splitting catalysts, as it governs reactivity through charge and mass transport. In this study, we employ ab initio molecular dynamics simulations to investigate the structural and dynamical properties of water at the $\\beta$-TaON (100) surface. We observed that a well-defined interface is established through the spontaneous dissociation of water and the reorganization of surface chemical bonds. This leads to the formation of a partially hydroxylated surface, accompanied by a strong network of hydrogen bonds at the TaON-water interface. Consequently, various proton transport routes, including the proton transfer through \"low-barrier hydrogen bond\" path, become active across the interface, dramatically increasing the overall rate of the proton hopping at the interface. Based on our findings, we propose that the observed high photocatalytic activity of TaON-based semiconductors could be attributed to the spontaneous water dissociation and the resulting high proton transfer rate at the interface.",
        "citation_title": "Molecular Insights into the Water Dissociation and Proton Dynamics at the $\u03b2$-TaON (100)/Water Interface",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Starting from the assumption that saturation of plasma turbulence driven by temperature-gradient instabilities in fusion plasmas is achieved by a local energy cascade between a long-wavelength outer scale, where energy is injected into the fluctuations, and a small-wavelength dissipation scale, where fluctuation energy is thermalized by particle collisions, we formulate a detailed phenomenological theory for the influence of perpendicular flow shear on magnetized-plasma turbulence. Our theory introduces two distinct regimes, called the weak-shear and strong-shear regimes, each with its own set of scaling laws for the scale and amplitude of the fluctuations and for the level of turbulent heat transport. We discover that the ratio of the typical radial and poloidal wavenumbers of the fluctuations (i.e., their aspect ratio) at the outer scale plays a central role in determining the dependence of the turbulent transport on the imposed flow shear. Our theoretical predictions are found to be in excellent agreement with numerical simulations of two paradigmatic models of fusion-relevant plasma turbulence: (i) an electrostatic fluid model of slab electron-scale turbulence, and (ii) Cyclone-base-case gyrokinetic ion-scale turbulence. Additionally, our theory envisions a potential mechanism for the suppression of electron-scale turbulence by perpendicular ion-scale flows based on the role of the aforementioned aspect ratio of the electron-scale fluctuations.",
        "citation_title": "Suppression of temperature-gradient-driven turbulence by sheared flows in fusion plasmas",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The verification of whether small-scale turbulence is isotropic remains a grand challenge. The difficulty arises because the presence of small-scale anisotropy is tied to the dissipation tensor, whose components require the full three-dimensional information of the flow field in both high spatial and temporal resolution, a condition rarely satisfied in turbulence experiments, especially during field scale measurement of atmospheric turbulence. To circumvent this issue, an \\emph{intermittency-anisotropy} framework is proposed through which we successfully extract the features of small-scale anisotropy from single-point measurements of turbulent time series by exploiting the properties of small-scale intermittency. Specifically, this framework quantifies anisotropy by studying the contrasting effects of burst-like activities on the scale-wise production of turbulence kinetic energy between the horizontal and vertical directions. The veracity of this approach is tested by applying it over a range of datasets covering an unprecedented range in the Reynolds numbers ($Re \\approx 10^{3}$ to $10^{6}$), sampling frequencies (10 kHz to 10 Hz), surface conditions (aerodynamically smooth surfaces to typical grasslands to forest canopies), and flow types (channel flows, boundary layer flows, atmospheric flows, and flows over forest canopies). For these diverse datasets, the findings indicate that the effects of small-scale anisotropy persists up to the integral scales of the streamwise velocity fluctuations and there exists a universal relationship to predict this anisotropy from the two-component state of the Reynolds stress tensor. This relationship is important towards the development of next-generation closure models of wall-turbulence by incorporating the effects of anisotropy at smaller scales of the flow.",
        "citation_title": "Quantifying small-scale anisotropy in turbulent flows",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Plasma-based acceleration (PBA) has emerged as a promising candidate for the accelerator technology used to build a future linear collider and/or an advanced light source. In PBA, a trailing or witness particle beam is accelerated in the plasma wave wakefield (WF) created by a laser or particle beam driver. The distance over which the drive beam evolves is several orders of magnitude larger than the wake wavelength. This large disparity in length scales is amenable to the quasi-static approach. Three-dimensional (3D), quasi-static (QS), particle-in-cell (PIC) codes, e.g., QuickPIC, have been shown to provide high fidelity simulation capability with 2-4 orders of magnitude speedup over 3D fully explicit PIC codes. We describe a mesh refinement scheme that has been implemented into the 3D QS PIC code, QuickPIC. We use a very fine (high) resolution in a small spatial region that includes the witness beam and progressively coarser resolutions in the rest of the simulation domain. A fast multigrid Poisson solver has been implemented for the field solve on the refined meshes and a Fast Fourier Transform (FFT) based Poisson solver is used for the coarse mesh. The code has been parallelized with both MPI and OpenMP, and the parallel scalability has also been improved by using pipelining. A preliminary adaptive mesh refinement technique is described to optimize the computational time for simulations with an evolving witness beam size. Several test problems are used to verify that the mesh refinement algorithm provides accurate results. The results are also compared to highly resolved simulations with near azimuthal symmetry using a new hybrid QS PIC code QPAD that uses a PIC description in the coordinates ($r$, $ct-z$) and a gridless description in the azimuthal angle, $\\phi$.",
        "citation_title": "Implementation of a Mesh refinement algorithm into the quasi-static PIC code QuickPIC",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The visible spectrum of Mo$^{15+}$ ions was measured using a high-temperature superconducting electron-beam ion trap at the Shanghai EBIT Laboratory, with an electron beam energy $E_{e}$=400 eV, significantly lower than the ionization potential (IP=544.0 eV) of Mo$^{14+}$ ions in the ground state. To expound on the experiment, the energy level structure, radiative transition properties, electron-impact excitation, and electron-impact ionization cross section for both the ground state and low-lying excited state of the Mo$^{14+}$ ions were calculated using Dirac-Fock-Slater method with a local central potential and distorted wave approximation. The results demonstrated reasonable agreement with both available experimental and theoretical data. Through an analysis of the related atomic processes of Mo$^{14+}$ ion, a scenario involving the stepwise ionization of the metastable state 3p$^{6}$3d$^{9}$4s was proposed to explain the presence of the Mo$^{15+}$ ions with a lower energy of the incident electron. Finally, the significance of the metastable levels in ionizing Mo$^{14+}$ ions is highlighted.",
        "citation_title": "Stepwise ionization of Mo$^{14+}$ ions in EBIT: The importance of the metastable level",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Accurate prediction of the dynamics and deformation of freely moving drops is crucial for numerous droplet applications. When the Weber number is finite but below a critical value, the drop deviates from its spherical shape and deforms as it is accelerated by the gas stream. Since aerodynamic drag on the drop depends on its shape oscillation, accurately modeling the drop shape evolution is essential for predicting the drop's velocity and position. In this study, 2D axisymmetric interface-resolved simulations were performed to provide a comprehensive dataset for developing a data-driven model. Parametric simulations were conducted by systematically varying the drop diameter and free-stream velocity, achieving wide ranges of Weber and Reynolds numbers. The instantaneous drop shapes obtained in simulations are characterized by spherical harmonics. Temporal data of the drag and modal coefficients are collected from the simulation data to train a Nonlinear Auto-Regressive models with eXogenous inputs (NARX) neural network model. The overall model consists of two multi-layer perceptron networks, which predict the modal coefficients and the drop drag, respectively. The drop shape can be reconstructed with the predicted modal coefficients. The model predictions are validated against the simulation data in the testing set, showing excellent agreement for the evolutions of both the drop shape and drag.",
        "citation_title": "Data-driven modeling of the aerodynamic deformation and drag for a freely moving drop in the sub-critical Weber number regime",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The problem of light waves interaction with charged particles becomes more and more complex starting with the case of plane waves, where the analytical solution is well known, to more natural, though more complicated situations which include focused or structured laser beams. Internal structure may introduce a new degree of freedom and qualitatively change the dynamics of interacting particles. For certain conditions, namely for the dilute plasma, description of single-particle dynamics in the focused structured laser beams is the first step and may serve as a good approximation on the way of understanding the global plasma response. Moreover, the general problem of integrability in complex systems starts from consideration of the integrals of motion for a single particle. The primary goal of this work is an understanding of the physics of the orbital angular momentum (OAM) absorption by a single particle in a focused structured light. A theoretical model of the process, including solutions of Maxwell equations with the required accuracy and a high-order perturbative approach to electron motion in external electromagnetic fields, is developed and its predictions are examined with numerical simulations for several exemplary electromagnetic field configurations. In particular, it was found that for the particles distributed initially with the azimuthal symmetry around the beam propagation direction, the transferred OAM has a smallness of the fourth order of the applied field amplitude, and requires an accurate consideration of the temporal laser pulse envelope.",
        "citation_title": "Angular momentum gain by electrons under action of intense structured light",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In this study, we investigate second- and third-harmonic generation processes in Au nanorod systems using the real-time time-dependent density functional tight binding (RT-TDDFTB) method. Our study focuses on computation of nonlinear signals based on the time dependent dipole response induced by linearly polarized laser pulses interacting with nanoparticles. We systematically explore the influence of various laser parameters, including pump intensity, duration, frequency, and polarization directions, on the harmonic generation. We demonstrate all the results using Au nanorod dimer systems arranged in end-to-end configurations, and disrupting the spatial symmetry of regular single nanorod systems crucial for second harmonic generation processes. Furthermore, we study the impact of nanorod lengths, which lead to variable plasmon energies, on the harmonic generation, and estimates of polarizabilities and hyper-polarizabilities are provided.",
        "citation_title": "Laser Pulse Induced Second- and Third-Harmonic Generation of Gold Nanorods with Real-Time Time-Dependent Density Functional Tight Binding (RT-TDDFTB) Method",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Ultrafast electron diffraction (UED) instruments typically operate at kHz or lower repetition rates and rely on indirect detection of electrons. However, these experiments encounter limitations because they are required to use electron beams containing a relatively large number of electrons (>>100 electrons/pulse), leading to severe space-charge effects. Consequently, electron pulses with long durations and large transverse diameters are used to interrogate the sample. Here, we introduce a novel UED instrument operating at a high repetition rate and employing direct electron detection. We operate significantly below the severe space-charge regime by using electron beams containing 55 to 140 electrons per pulse at 30-kHz. We demonstrate the ability to detect time-resolved signals from thin film solid samples with a difference contrast signal, {\\Delta}I/I0, and an instrument response function as low as 10-5 and 243-fs (FWHM), respectively, without temporal compression. Overall, our findings underscore the importance of increasing the repetition rate of UED experiments and adopting a direct electron detection scheme. Our newly developed scheme enables more efficient and sensitive investigations of ultrafast dynamics in photoexcited samples using ultrashort electron beams.",
        "citation_title": "High repetition rate ultrafast electron diffraction with direct electron detection",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present a comprehensive set of theoretical results for differential, integrated, and momentum transfer cross sections for the elastic scattering of electrons by beryllium, magnesium and calcium, at energies below 1 keV. In addition, we provide Sherman function values for elastic electron scattering from calcium in the same energy range. This study extends the application of our method of calculations, already employed for barium and strontium, to all stable alkaline-earth-metal atoms. Our semi-empirical approach to treating target polarization has produced in our earlier work a satisfactory agreement with experimental values and precise theoretical results such as convergent close-coupling calculations for barium. The present data are expected to be of similar high accuracy, based on our previous success in similar calculations for barium and all inert gases.",
        "citation_title": "Elastic electron scattering from Be, Mg, and Ca",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recently, two-dimensional terahertz spectroscopy (2DTS) has attracted increasing attention for studying complex solids. A number of recent studies have applied 2DTS either with long pulses or away from any material resonances, situations that yield unconventional 2DTS spectra that are often difficult to interpret. Here, we clarify the generic origins of observed spectral features by examining 2DTS spectra of ZnTe, a model system with a featureless optical susceptibility at low terahertz frequencies. These results also reveal possible artifacts that may arise from electro-optic sampling in collinear 2DTS experiments, including the observation of spurious rectified or second harmonic signals.",
        "citation_title": "Excitation-Dependent Features and Artifacts in 2-D Terahertz Spectroscopy",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we investigated the spatial evolution of optical power in a closed-form optical waveguide configuration consisting of six passive waveguides and each of the waveguides exhibits equal strength of Kerr nonlinearity. We considered only nearest neighbor interaction between the waveguides. We found that in the case of low Kerr nonlinearity, evolution of optical power shows synchronization behavior. But when we increased the strength of Kerr nonlinearity, we discovered that spatial evolution of optical power in all waveguides shows independent characteristics. On the other hand, we have studied the impact of the coupling constant on the synchronization dynamics of our system. Our findings showed us that strong coupling can strengthen the collective dynamics in the presence of strong Kerr nonlinearity. From our results, we can conclude that Kerr nonlinearity in our system plays the role of disorder parameter that destroys as well as alters the synchronization behavior of evolution of optical power in the waveguides and coupling constant plays the role of an antagonist and restores synchronization in the model.",
        "citation_title": "Synchronization Dynamics in the Spatial Evolution of Optical Power in Optical Oligomer",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study high quality-factor (high Q) resonances supported by periodic arrays of Mie resonators from the perspectives of both Bloch wave theory and multiple scattering theory. We reveal that, unlike a common belief, the bound states in the continuum (BICs) derived by the Bloch-wave theory do not directly determine the resonance with the highest Q value in large but finite arrays. Higher Q factors appear to be associated with collective resonances formed by nominally guided modes below the light line associated with strong effect of both electric and magnetic multipoles. Our findings offer valuable insights into accessing the modes with higher Q resonances via bonding modes within finite metastructures. Our results underpin the pivotal significance of magnetic and electric multipoles in the design of resonant metadevices and nonlocal flat-band optics. Moreover, our demonstrations reveal that coupled arrays of high-Q microcavities do not inherently result in a stronger light-matter interaction when compared to coupled low-Q nanoresonators. This result emphasizes the critical importance of the study of multiple light-scattering effects in cavity-based systems.",
        "citation_title": "Collective nature of high-Q resonances in finite-size photonic metastructures",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Cross-sections for dissociative recombination and electron-impact vibrational excitation of the BF$^+_2$ molecular ion are computed using a theoretical approach that combines the normal modes approximation for the vibrational states of the target ion and use of the UK R-matrix code to evaluate electron-ion scattering matrices for fixed geometries of the ion. Thermally-averaged rate coefficients are obtained from the cross-sections for temperatures in the 10-3000 K range.",
        "citation_title": "Theoretical study of dissociative recombination and vibrational excitation of the BF$_2^+$ ion by an electron impact",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The numerical simulation of rarefied gas mixtures with disparate mass and concentration is a huge research challenge. Based on our recent kinetic modelling for monatomic gas mixture flows, this problem is tackled by the general synthetic iterative scheme (GSIS), where the mesoscopic kinetic and macroscopic synthetic equations are alternately solved by the finite-volume discrete velocity method. Three important features of GSIS are highlighted. First, the synthetic equations are precisely derived from the kinetic equation, naturally reducing to the Navier-Stokes equations in the continuum flow regime; in other flow regimes, the kinetic equation provides high-order closure of the constitutive relations to capture the rarefaction effects. Second, these synthetic equations, which can be solved quickly, help to adjust the kinetic system to relax rapidly toward the steady state. Furthermore, in such a two-way coupling, the constraint on the spatial cell size is relieved. Third, the linear Fourier stability analysis demonstrates that the error decay rate in GSIS is smaller than 0.5 for various combinations of mass, concentration and viscosity ratios, such that the error can be reduced by three orders of magnitude after 10 iterations. The efficiency and accuracy of GSIS are demonstrated through several challenging cases covering a wide range of mass ratio, species concentration, and flow speed.",
        "citation_title": "General synthetic iterative scheme for rarefied gas mixture flows",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "One of the most promising approaches for the next generation of neutrino experiments is the realization of large hybrid Cherenkov/scintillation detectors made possible by recent innovations in photodetection technology and liquid scintillator chemistry. The development of a potentially suitable future detector liquid with particularly slow light emission is discussed in the present publication. This cocktail is compared with respect to its fundamental characteristics (scintillation efficiency, transparency, and time profile of light emission) with liquid scintillators currently used in large-scale neutrino detectors. In addition, the optimization of the admixture of wavelength shifters for a scintillator with particularly high light emission is presented. Furthermore, the pulse-shape discrimination capabilities of the novel medium was studied using a pulsed particle accelerator driven neutron source. Beyond that, purification methods based on column chromatography and fractional vacuum distillation for the co-solvent DIN (Diisopropylnaphthalene) are discussed.",
        "citation_title": "Development of a Bi-solvent Liquid Scintillator with Slow Light Emission",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Liquid argon detectors are ubiquitous in particle, astroparticle, and applied physics. They reached an unprecedented level of maturity thanks to more than 20 years of R&D and the operation of large-scale facilities at CERN, Fermilab, and the Gran Sasso laboratories. This article reviews such an impressive advance - from the grounding of the experimental technique up to cutting-edge applications. We commence the review by describing the physical and chemical properties of liquid argon as an active and target medium for particle detection, together with advantages and limitations compared with other liquefied noble gases. We examine the opportunities and challenges of liquid argon detectors operated as calorimeters, scintillators, and time projection chambers. We then delve into the core applications of liquid argon detectors at colliders (ATLAS), accelerator neutrino beams (SBN, DUNE), and underground laboratories (DarkSide, DEAP, ICARUS) for the observation of rare events. We complete the review by looking at unconventional developments (pixelization, combined light-charge readout, Xe-doped devices, all-optical readout) and applications in medical and applied physics to extend this technology's scope toward novel research fields.",
        "citation_title": "The science and technology of liquid argon detectors",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We apply the political stress index as introduced by Goldstone (1991) and implemented by Turchin (2013), to the case study of Poland. The approach quantifies political and social unrest as a single quantity based on a multitude of economic and demographic variables. The present-day data allow us to directly apply index without the need of simulating the elite component, as was done previously. Neither model version shows appreciable unrest levels for the present, while the simulated model applied to partial historical data yields the index in remarkable agreement with the fall of communism in Poland. We next analyze the model's sensitive dependence on its parameters (the hallmark of chaos), which limits its utility and application to other countries. The original equations cannot, by construction, describe the elite fraction for longer time-periods; and we propose a modification to remedy this problem. The model still holds some predictive power, but we argue that some components should be reinterpreted if one wants to keep its dynamical equations.",
        "citation_title": "Political Stress Index of Poland",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper discusses some features of the spectral line profile theory used in the treatment of measured atomic transitions. It is shown that going beyond the established linear approximation for the spectral line contour in the case of its nonresonant extension, the potential for a more accurate extraction of atomic characteristics from experimental data arises. Using the example of the Lyman-$\\alpha$ (Ly$_\\alpha$) transition in hydrogen, a simple analysis of the observed spectral line distorted by a possible interfering transitions is given. In particular, the results obtained in the present work clearly demonstrate that the processing of the same experimental data at different settings can provide an accurate determination of the transition frequency, the centre of gravity as well as the hyperfine splitting of the ground state in hydrogen-like atomic systems. The latter is especially important for setting up precision spectroscopic experiments on the antihydrogen atom.",
        "citation_title": "Handling the asymmetric spectral line profile",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Walking is the most sustainable form of urban mobility, but is compromised by uncomfortable or unhealthy sun exposure, which is an increasing problem due to global warming. Shade from buildings can provide cooling and protection for pedestrians, but the extent of this potential benefit is unknown. Here we explore the potential for shaded walking, using building footprints and street networks from both synthetic and real cities. We introduce a route choice model with a sun avoidance parameter $\\alpha$ and define the CoolWalkability metric to measure opportunities for walking in shade. We derive analytically that on a regular grid with constant building heights, CoolWalkability is independent of $\\alpha$, and that the grid provides no CoolWalkability benefit for shade-seeking individuals compared to the shortest path. However, variations in street geometry and building heights create such benefits. We further uncover that the potential for shaded routing differs between grid-like and irregular street networks, forms local clusters, and is sensitive to the mapped network geometry. Our research identifies the limitations and potential of shade for cool, active travel, and is a first step towards a rigorous understanding of shade provision for sustainable mobility in cities.",
        "citation_title": "CoolWalks: Assessing the potential of shaded routing for active mobility in urban street networks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Integrated frequency combs promise transformation of lab-based metrology into disruptive real-world applications. These microcombs are, however, sensitive to stochastic thermal fluctuations of the integrated cavity refractive index, with its impact becoming more significant as the cavity size becomes smaller. This tradeoff between microcomb noise performance and footprint stands as a prominent obstacle to realizing applications beyond a controlled lab environment. Here, we demonstrate that small footprint and low noise become compatible through the all-optical Kerr-induced synchronization (KIS) method. Our study unveils that the phase-locking nature of the synchronization between the cavity soliton and the injected reference pump laser enables the microcomb to no longer be limited by internal noise sources. Instead, the microcomb noise is mostly limited by external sources, namely, the frequency noise of the two pumps that doubly pin the microcomb. First, we theoretically and experimentally show that the individual comb tooth linewidths of an octave-spanning microcomb remain within the same order-of-magnitude as the pump lasers, contrary to the single-pumped case that exhibits a more than two order-of-magnitude increase from the pump to the comb edge. Second, we theoretically show that intrinsic noise sources such as thermorefractive noise in KIS are quenched at the cavity decay rate, greatly decreasing its impact. Experimentally, we show that even with free-running lasers, the KIS microcomb can exhibit better repetition rate noise performance than the predicted thermorefractive noise limitation in absence of KIS.",
        "citation_title": "All-Optical Noise Quenching of An Integrated Frequency Comb",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We implemented the Bayesian analysis to the polarised neutron reflectivity data. Reflectivity data from a magnetic TbCo thin film structure was studied using the bundle of a Monte-Carlo Markov-chain algorithm, likelihood estimation, and error modeling. By utilizing the Bayesian analysis, we were able to investigate the uniqueness of the solution beyond reconstructing the magnetic and structure parameters. This approach has demonstrated its expedience as several probable reconstructions were found (the multimodality case) concerning the isotopic composition of the surface cover layer. Such multimodal reconstruction emphasizes the importance of rigorous data analysis instead of the direct data fitting approach, especially in the case of poor statistically conditioned data, typical for neutron reflectivity experiments. The analysis details and the discussion on multimodality are in this article.",
        "citation_title": "Multimodal reconstruction of TbCo thin film structure with Basyeian analysis of polarised neutron reflectivity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Dirac exchange interaction is derived from recent quantum kinetic theory for collisionless plasmas. For this purpose, the kinetic equation is written in the semiclassical and long wavelength approximations. The validity of the model for real systems is worked out, in terms of temperature and density parameters. Within the region of applicability, the correlation potential energy is shown to be always smaller than the exchange contribution. From the moments of the quantum kinetic equations, macroscopic, hydrodynamic equations are found, for an electron-ion plasma. The Dirac exchange term is explicitly derived, in the case of a completely degenerate electron gas. These results show, within quantum kinetic theory for charged particle systems, a new view of the Dirac exchange interaction frequently used in density functional theory parametrization. Finally, a simpler form of the quantum plasma exchange kinetic theory is also found.",
        "citation_title": "Derivation of Dirac Exchange Interaction Potential from Quantum Plasma Kinetic Theory",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Solving the wave equation is essential to seismic imaging and inversion. The numerical solution of the Helmholtz equation, fundamental to this process, often encounters significant computational and memory challenges. We propose an innovative frequency-domain scattered wavefield modeling method employing neural operators adaptable to diverse seismic velocities. The source location and frequency information are embedded within the input background wavefield, enhancing the neural operator's ability to process source configurations effectively. In addition, we utilize a single reference frequency, which enables scaling from larger-domain forward modeling to higher-frequency scenarios, thereby improving our method's accuracy and generalization capabilities for larger-domain applications. Several tests on the OpenFWI datasets and realistic velocity models validate the accuracy and efficacy of our method as a surrogate model, demonstrating its potential to address the computational and memory limitations of numerical methods.",
        "citation_title": "Learned frequency-domain scattered wavefield solutions using neural operators",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper presents a novel method for low maintenance, low ambiguity in-situ drift velocity monitoring in large volume Time Projection Chambers (TPCs). The method was developed and deployed for the 40m^3 TPC tracker system of the NA61/SHINE experiment at CERN, which has a one meter of drift length. The method relies on a low-cost multi-wire proportional chamber (MWPC) placed downstream of the TPCs to be monitored. The drift velocity is then determined by matching the reconstructed tracks in the TPC to the hits of the pertinent monitoring chamber, called Geometry Reference Chamber (GRC), which is then used as a differential length scale. An important design requirement on the GRC was minimal added complexity to the existing system, in particular, compatibility with Front-End Electronics (FEE) cards already used to read out the TPCs. Moreover, the GRC system was designed to operate both in large and small particle flux. The system is capable of monitoring the evolution of the in-situ drift velocity down to a one permil precision, with a few minutes of time sampling.",
        "citation_title": "Novel method for in-situ drift velocity measurement in large volume TPCs: the Geometry Reference Chamber of the NA61/SHINE experiment at CERN",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The land-atmosphere coupling strength has been defined as the percentage of precipitation variability explained by the variation of soil moisture in the Global Land-Atmosphere Coupling Experiment (GLACE). While it is useful to identify global hotspots of land-atmosphere interaction, this coupling strength is different from coupling sensitivity, which directly quantifies how precipitation generation responds to the perturbation of soil moisture and is essential for our understanding of the global water cycle. To disentangle these two quantities, here we theoretically explore the relationships among coupling strength, sensitivity, and soil moisture variances. We use climate model outputs to show that the largest soil moisture variances are located in the transitional climate zones and the variations of soil moisture largely account for the geographical patterns of coupling hotspots. The coupling sensitivity is not necessarily low in non-hotspot regions, which could impose great impacts on the development of extreme climate events. We therefore call for more research attention on coupling sensitivity to improve our understanding of the climate system.",
        "citation_title": "Strength and Sensitivity of Land-Atmosphere Interaction",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The fusion research facility ITER is currently being assembled to demonstrate that fusion can be used for industrial energy production, while several other programmes across the world are also moving forward, such as EU-DEMO, CFETR, SPARC and STEP. The high engineering complexity of a tokamak makes it an extremely challenging device to optimise, and test-based optimisation would be too slow and too costly. Instead, digital design and optimisation must be favored, which requires strongly-coupled suites of High-Performance Computing calculations. In this context, having surrogate models to provide quick estimates with uncertainty quantification is essential to explore and optimise new design options. Furthermore, these surrogates can in turn be used to accelerate simulations in the first place. This is the case of Parareal, a time-parallelisation method that can speed-up large HPC simulations, where the coarse-solver can be replaced by a surrogate. A novel framework, Neural-Parareal, is developed to integrate the training of neural operators dynamically as more data becomes available. For a given input-parameter domain, as more simulations are being run with Parareal, the large amount of data generated by the algorithm is used to train new surrogate models to be used as coarse-solvers for future Parareal simulations, leading to progressively more accurate coarse-solvers, and thus higher speed-up. It is found that such neural network surrogates can be much more effective than traditional coarse-solver in providing a speed-up with Parareal. This study is a demonstration of the convergence of HPC and AI which simply has to become common practice in the world of digital engineering design.",
        "citation_title": "Neural-Parareal: Dynamically Training Neural Operators as Coarse Solvers for Time-Parallelisation of Fusion MHD Simulations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study the classical dynamics of a system of a pair of Kerr-Duffing nonlinear oscillators coupled by a nonlinear interaction and subject to a parametric drive. Within a rotating wave approximation (RWA), we analyze the steady-state solutions for the oscillation amplitude of the two oscillators. In the most relevant case of identical oscillators, we separately investigate configurations in which only one oscillator is parametrically driven, or both of them are simultaneously driven. In the latter regime, special attention is paid to the symmetric case where the parametric drives acting on the two oscillators are equal: for an increasing value of the detuning of the parametric drive, a transition to a multi-stable, symmetry-breaking regime is found, where the two oscillators display different oscillation amplitudes and phases.",
        "citation_title": "Nonlinearity-induced symmetry breaking in a system of two parametrically driven Kerr-Duffing oscillators",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recent developments in numerically exact quantum dynamics methods have brought the dream of calculating the dynamics of chemically complex open systems closer to reality. Path-integral-based methods, hierarchical equations of motion (HEOM) and quantum analog simulators all require the spectral density of the environment to describe its effect on the system. Here we find that the rate of slow population relaxation is sensitive to the precise functional form used to describe the spectral density peaks. This finding highlights yet another challenge to obtaining accurate spectral densities. In the context of quantum information science, we give a simple recipe to adjust the results of analog simulation for this difference assuming both the simulator and the target spectral densities are known.",
        "citation_title": "High-frequency tails in spectral densities",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Intermolecular charge-transfer is a highly important process in biology and energy-conversion applications where generated charges need to be transported over several moieties. However, its theoretical description is challenging since the high accuracy required to describe these excited states must be accessible for calculations on large molecular systems. In this benchmark study, we identify reliable low-scaling computational methods for this task. Our reference results were obtained from highly accurate wavefunction calculations that restrict the size of the benchmark systems. However, the density-functional theory based methods that we identify as accurate can be applied to much larger systems. Since targeting charge-transfer states requires the unambiguous classification of an excited state, we first analyze several charge-transfer descriptors for their reliability concerning intermolecular charge-transfer and single out DCT as an optimal choice for our purposes. In general, best results are obtained for orbital-optimized methods - and among those, IMOM proved to be the most numerically stable variant - but optimally-tuned range-separated hybrid functionals combined with rather small basis sets proved to yield surprisingly good results. This makes these fast calculations attractive for high-throughput screening applications.",
        "citation_title": "Benchmarking DFT-based excited-state methods for intermolecular charge-transfer excitations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Low-Gain Avalanche Diode (LGAD) is a new silicon detector and holds wide application prospects in particle physics experiments due to its excellent timing resolution.\nThe LGAD with a pixel size of 1.3 mm $\\times$ 1.3 mm was used to construct a High Granularity Timing Detector (HGTD) in ATLAS experiments to solve the pile-up problem.\nMeanwhile, the Circular Electron Positron Collider (CEPC) also proposes detectors using the LGAD. However, pixel LGAD exhibits higher readout electronics density and cost, which somewhat limits the application of LGADs. To decrease the readout electronics density, the Institute of High Energy Physics (IHEP) of the Chinese Academy of Sciences has designed strip LGADs with larger areas. These strip LGADs are all 19 mm in length but with different widths of 1.0 mm, 0.5 mm, and 0.3 mm.\nThis article provides a detailed introduction to the design parameters of these strip LGADs and tests their electrical characteristics, including leakage current, break-down voltage, depletion capacitance, etc.\nThe timing resolution and signal-to-noise ratio of the three strip LGAD sensors were investigated using a beta source test system.\nThe position resolution parallel to the strip direction was tested and analyzed for the first time using a pico-second laser test system.\nTests have demonstrated that the timing resolution of strip LGADs can reach about 37.5 ps, and position resolution parallel to the strip direction is better than 1 mm.",
        "citation_title": "Development of the strip LGAD detector with double-end readout for future colliders",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This study evaluates the precision of widely recognized quantum chemical methodologies, CCSD(T), DLPNO-CCSD(T) and localized ph-AFQMC, for determining the thermochemistry of main group elements. DLPNO-CCSD(T) and localized ph-AFQMC, which offer greater scalability compared to canonical CCSD(T), have emerged over the last decade as pivotal in producing precise benchmark chemical data. Our investigation includes closed-shell, neutral molecules, focusing on their heat of formation and atomization energy sourced from four specific small molecule datasets. Firstly, we selected molecules from the G2 and G3 datasets, noted for their reliable experimental heat of formation data. Additionally, we incorporate molecules from the W4-11 and W4-17 sets, which provide high-level theoretical reference values for atomization energy at 0 K. Our findings reveal that both DLPNO-CCSD(T) and ph-AFQMC methods are capable of achieving a root-mean-square deviation (RMSD) of less than 1 kcal/mol across the combined dataset, aligning with the threshold for chemical accuracy. Moreover, we make efforts to confine the maximum deviations within 2 kcal/mol, a degree of precision that significantly broadens the applicability of these methods in fields such as biology and materials science.",
        "citation_title": "Scalable Ab Initio Electronic Structure Methods with Near Chemical Accuracy for Main Group Chemistry",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present theoretical and experimental evidence of high-gain far-detuned nonlinear frequency conversion, extending towards both the visible and the mid-infrared, in a few-mode graded-index silica fiber pumped at 1.064 $\\mu$m, and more specifically achieving gains of hundreds of dB per meter below 0.65 $\\mu$m and beyond 3.5 $\\mu$m. Our findings highlight the potential of graded-index fibers in terms of strong modal confinement over an ultrabroad spectral range for enabling high-gain wavelength conversion. Such advancements require an accurate interpretation of intramodal and intermodal four-wave mixing processes.",
        "citation_title": "High-gain far-detuned nonlinear frequency conversion in optical fibers: intramodal vs. intermodal processes",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The reconstruction of photon conversions is importantin order to improve the reconstruction efficiency of the physics measurements involving photons. However, there are significant number of conversions in which only one of the two tracks emitted electrons is reconstructed in the detector due to very asymmetric energy sharing between the electron-positron pair. The momentum determination of the parent photon can be improved by estimating the missing energy in such conversions. In this study, we propose a simple statistical method that can be used to determine the mean value of the missing energy. By using simulated minimum bias events at LHC conditions and a toy detector simulation, the performance of the method is tested for several decay channels commonly used in particle physics analyses. A considerable improvement in the mass reconstruction precision is obtained when reconstructing particles decaying to photons whose energies are less than 20 GeV.",
        "citation_title": "A Statistical Method for Improving Momentum Measurement of Photon Conversions Reconstructed from Single Electrons",
        "date_delivered": "[Submitted on 18 Mar 2024]"
    },
    {
        "abstract": "Ground-based laser interferometric gravitational wave detectors consist of complex multiple optical cavity systems. An arm-length stabilization (ALS) system has played an important role in bringing such complex detector into operational state and enhance the duty cycle. The sensitivity of these detectors can be improved if the thermal noise of their test mass mirror coatings is reduced. Crystalline AlGaAs coatings are a promising candidate for this. However, traditional ALS system with frequency-doubled 532 nm light is no longer an option with AlGaAs coatings due to the narrow bandgap of GaAs, thus alternative locking schemes must be developed. In this letter, we describe an experimental demonstration of a novel ALS scheme which is compatible with AlGaAs coatings. This ALS scheme will enable the use of AlGaAs coatings and contribute to improved sensitivity of future detectors.",
        "citation_title": "Experimental demonstration of frequency downconverted arm length stabilization for a future upgraded gravitational wave detector",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Neural network interatomic potentials (NNPs) have recently proven to be powerful tools to accurately model complex molecular systems while bypassing the high numerical cost of ab-initio molecular dynamics simulations. In recent years, numerous advances in model architectures as well as the development of hybrid models combining machine-learning (ML) with more traditional, physically-motivated, force-field interactions have considerably increased the design space of ML potentials. In this paper, we present FeNNol, a new library for building, training and running force-field-enhanced neural network potentials. It provides a flexible and modular system for building hybrid models, allowing to easily combine state-of-the-art embeddings with ML-parameterized physical interaction terms without the need for explicit programming. Furthermore, FeNNol leverages the automatic differentiation and just-in-time compilation features of the Jax Python library to enable fast evaluation of NNPs, shrinking the performance gap between ML potentials and standard force-fields. This is demonstrated with the popular ANI-2x model reaching simulation speeds nearly on par with the AMOEBA polarizable force-field on commodity GPUs (GPU=Graphics processing unit). We hope that FeNNol will facilitate the development and application of new hybrid NNP architectures for a wide range of molecular simulation problems.",
        "citation_title": "FeNNol: an Efficient and Flexible Library for Building Force-field-enhanced Neural Network Potentials",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Scheduled maintenance is likely to be lengthy and therefore consequential for the economics of fusion power plants. The maintenance strategy that maximizes the economic value of a plant depends on internal factors such as the cost and durability of the replaceable components, the frequency and duration of the maintenance blocks, and the external factors of the electricity system in which the plant operates. This paper examines the value of fusion power plants with various maintenance properties in a decarbonized United States Eastern Interconnection circa 2050. Seasonal variations in electricity supply and demand mean that certain times of year, particularly spring to early summer, are best for scheduled maintenance. Seasonality has two important consequences. First, the value of a plant can be 15% higher than what one would naively expect if value were directly proportional to its availability. Second, in some cases, replacing fractions of a component in shorter maintenance blocks spread over multiple years is better than replacing it all at once during a longer outage, even through the overall availability of the plant is lower in the former scenario.",
        "citation_title": "Valuing maintenance strategies for fusion plants as part of a future electricity grid",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Time-dependent Hartree-Fock (TDHF) is one of the fundamental post-Hartree-Fock (HF) methods to describe excited states. In its Tamm-Dancoff form, equivalent to Configuration Interaction Singles, it is still widely used and particularly applicable to big molecules where more accurate methods may be unfeasibly expensive. However, it is rarely implemented in real space, mostly because of the expensive nature of the exact-exchange potential in real space. Compared to widely used Gaussian-type orbitals (GTO) basis sets, real space often offers easier implementation of equations and more systematic convergence of Rydberg states, as well as favorable scaling, effective domain parallelization, flexible boundary conditions, and ability to treat model systems. We implemented TDHF in the Octopus real-space code as a step toward linear-response hybrid time-dependent density-functional theory (TDDFT), other post-HF methods, and ensemble density-functional theory methods involving exact exchange. Calculation of HF's non-local exact exchange is very expensive in real space. We overcome this limitation with Octopus' implementation of Adaptively Compressed Exchange (ACE), and find the appropriate mixing and starting point to complete the ground-state calculation in a practical amount of time, to enable TDHF. We compared our results to those from GTOs on a set of small molecules and confirmed close agreement of results, though with larger deviations than in the case of semi-local TDDFT. We find that convergence of TDHF demands a finer real-space grid than semi-local TDDFT. We also present the subtleties in benchmarking a real-space calculation against GTOs, relating to Rydberg and vacuum states.",
        "citation_title": "Implementation of time-dependent Hartree Fock in real space",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose a theoretical scheme for a non-Hermitian atomic grating within an ultra-cold rubidium-87 ($^{87}Rb$) atomic ensemble. The grating's diffraction properties depend on the polarization states of incident photons and are controlled non-locally through Rydberg interactions. Multiple types of polarization-dependent diffraction modes are generated, benefiting from no crosstalk atomic transition channels based on transition selection rules. Those polarization-dependent diffraction modes can be switched using dynamic optical pulse trains, exploiting the Rydberg blockade effect, and are tunable by non-Hermitian optical modulation. Our work will advance the application of asymmetric optical scattering by utilizing the polarization degree of freedom within continuous media and benefit the application of versatile non-Hermitian/asymmetric optical devices.",
        "citation_title": "Polarization dependent non-Hermitian atomic grating controlled by dipole blockade effect",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The detection of individual photons at cryogenic temperatures is of interest to many experiments searching for physics beyond the Standard Model. Silicon photomultipliers are often deployed in liquid argon or liquid xenon to detect scintillation light by either directly detecting the vacuum ultra-violet scintillation or by detecting light from fluorescent compounds that are used to shift the wavelength. Here we present results from an experimental setup that measures the photon detection efficiencies of silicon photomultipliers, sensitive to the visible spectrum, at liquid nitrogen temperature, 77 K. Results from a KETEK PM3325-WB-D0 and a Hamamatsu S13360-3050CS silicon photomultiplier exhibit a decrease in photon detection efficiency greater than 20% at liquid nitrogen temperature relative to room temperature for 562.5 nm light.",
        "citation_title": "Characterization of Silicon Photomultiplier Photon Detection Efficiency at Liquid Nitrogen Temperature",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Experiments violating Bell's inequality appear to indicate deterministic models do not correspond to a realistic theory of quantum mechanics. The theory of pilot waves seemingly overcomes this hurdle via nonlocality and statistical dependence, however it necessitates the existence of \"ghost waves\". This manuscript develops a deterministic dynamical system with local interactions. The aggregate behavior of the trajectories are reminiscent of a quantum particle evolving under the Schr\u00f6dinger equation and reminiscent of Feynman's path integral interpretation in three canonical examples: motion in free space, double slit diffraction, and superluminal barrier traversal. Moreover, the system bifurcates into various dynamical regimes including a classical limit. These results illustrate a deterministic alternative to probabilistic interpretations and aims to shed light on the transition from quantum to classical mechanics.",
        "citation_title": "Towards a Deterministic Interpretation of Quantum Mechanics: Insights from Dynamical Systems",
        "date_delivered": "[Submitted on 23 Apr 2024]"
    },
    {
        "abstract": "Second-order topological insulators can be characterized by their bulk polarization, which is believed to be intrinsically connected to the center of the Wannier function. In this study, we demonstrate the existence of second-order topological insulators that feature a pair of partially degenerate photonic bands. These arise from the nonsymmorphic glide symmetry in an all-dielectric photonic crystal. The center of the maximally localized Wannier function (MLWF) is consistently located at the origin but is not equivalent with respect to the sum of constituent polarizations. As a result, topological corner modes can be identified by the distinctly hybridized MLWFs that truncate at the sample boundary. Through full-wave numerical simulations paired with microwave experiments, the second-order topology is clearly confirmed and characterized. These topological corner states exhibit notably unique modal symmetries, which are made possible by the inversion of the Wannier bands. Our results provide an alternative approach to explore higher-order topological physics with significant potential for applications in integrated and quantum photonics.",
        "citation_title": "Topological Corner Modes by Composite Wannier States in Glide-Symmetric Photonic Crystal",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Using molecular simulation and continuum dielectric theory, we consider how electrochemical kinetics are modulated as a function of twist angle in bilayer graphene electrodes. By establishing an effective connection between twist angle and the screening length of charge carriers within the electrode, we investigate how tunable metallicity can result in modified statistics of the electron transfer energy gap. Constant potential molecular simulations show that the activation free energy for electron transfer is an increasing function of the screening length, or decreasing function the density of states at the Fermi energy in the electrode, and subsequently a non-monotonic function of twist angle. We find the twist angle alters the density of states, which tunes the number of thermally-accessible channels for electron transfer, as well as the reorganization energy by altering the stability of the vertically excited state through attenuated image charge interactions. Understanding these effects allows us to cast the Marcus rate of interfacial electron transfer as a function of twist angle, in a manner consistent with a growing body of experimental observations.",
        "citation_title": "Microscopic origin of twist-dependent electron transfer rate in bilayer graphene",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Computational electromagnetics (CEM) is employed to numerically solve Maxwell's equations, and it has very important and practical applications across a broad range of disciplines, including biomedical engineering, nanophotonics, wireless communications, and electrodynamics. The main limitation of existing CEM methods is that they are computationally demanding. Our work introduces a leap forward in scientific computing and CEM by proposing an original solution of Maxwell's equations that is grounded on graph neural networks (GNNs) and enables the high-performance numerical resolution of these fundamental mathematical expressions. Specifically, we demonstrate that the update equations derived by discretizing Maxwell's partial differential equations can be innately expressed as a two-layer GNN with static and pre-determined edge weights. Given this intuition, a straightforward way to numerically solve Maxwell's equations entails simple message passing between such a GNN's nodes, yielding a significant computational time gain, while preserving the same accuracy as conventional transient CEM methods. Ultimately, our work supports the efficient and precise emulation of electromagnetic wave propagation with GNNs, and more importantly, we anticipate that applying a similar treatment to systems of partial differential equations arising in other scientific disciplines, e.g., computational fluid dynamics, can benefit computational sciences",
        "citation_title": "Solving Maxwell's equations with Non-Trainable Graph Neural Network Message Passing",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantification of chaos is a challenging issue in complex dynamical systems. In this paper, we discuss the chaotic properties of generalized Lotka-Volterra and May-Leonard models of biodiversity, via the Hamming distance density. We identified chaotic behavior for different scenarios via the specific features of the Hamming distance and the method of q-exponential fitting. We also investigated the spatial autocorrelation length to find the corresponding characteristic length in terms of the number of species in each system. In particular, the results concerning the characteristic length are in good accordance with the study of the chaotic behavior implemented in this work.",
        "citation_title": "Chaotic behavior in Lotka-Volterra and May-Leonard models of biodiversity",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Scientists conduct large-scale simulations to compute derived quantities-of-interest (QoI) from primary data. Often, QoI are linked to specific features, regions, or time intervals, such that data can be adaptively reduced without compromising the integrity of QoI. For many spatiotemporal applications, these QoI are binary in nature and represent presence or absence of a physical phenomenon. We present a pipelined compression approach that first uses neural-network-based techniques to derive regions where QoI are highly likely to be present. Then, we employ a Guaranteed Autoencoder (GAE) to compress data with differential error bounds. GAE uses QoI information to apply low-error compression to only these regions. This results in overall high compression ratios while still achieving downstream goals of simulation or data collections. Experimental results are presented for climate data generated from the E3SM Simulation model for downstream quantities such as tropical cyclone and atmospheric river detection and tracking. These results show that our approach is superior to comparable methods in the literature.",
        "citation_title": "Machine Learning Techniques for Data Reduction of Climate Applications",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The Kerr nonlinearity allows for exact analytic soliton solutions in 1+1D. While nothing excludes that these solitons form in naturally-occurring real-world 3D settings as solitary walls or stripes, their observation has previously been considered unfeasible because of the strong transverse instability intrinsic to the extended nonlinear perturbation. We report the observation of solitons that are fully compatible with the 1+1D Kerr paradigm limit hosted in a 2+1D system. The waves are stripe spatial solitons in bulk copper doped potassium-lithium-tantalate-niobate (KLTN) supported by the unsaturated photorefractive screening nonlinearity. The parameters of the stripe solitons fit well, in the whole existence domain, with the 1+1D existence curve that we derive for the first time in closed form starting from the saturable model of propagation. Transverse instability, that accompanies the solitons embedded in the 3D system, is found to have a gain length much longer than the crystal. Findings establish our system as a versatile platform for investigating exact soliton solutions in bulk settings and in exploring the role of dimensionality at the transition from integrable to non-integrable regimes of propagation.",
        "citation_title": "Evidence of 1+1D photorefractive stripe solitons deep in the Kerr limit",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Interplanetary links (IPL) serve as crucial enablers for space exploration, facilitating secure and adaptable space missions. An integrated IPL with inter-satellite communication (IP-ISL) establishes a unified deep space network, expanding coverage and reducing atmospheric losses. The challenges, including irregularities in charged density, hardware impairments, and hidden celestial body brightness are analyzed with a reflectarray-based IP-ISL between Earth and Moon orbiters. It is observed that $10^{-8}$ order severe hardware impairments with intense solar plasma density drops an ideal system's spectral efficiency (SE) from $\\sim\\!38~\\textrm{(bit/s)/Hz}$ down to $0~\\textrm{(bit/s)/Hz}$. An ideal full angle of arrival fluctuation recovery with full steering range achieves $\\sim\\!20~\\textrm{(bit/s)/Hz}$ gain and a limited beamsteering with a numerical reflectarray design achieves at least $\\sim\\!1~\\textrm{(bit/s)/Hz}$ gain in severe hardware impairment cases.",
        "citation_title": "On the Role of Reflectarrays for Interplanetary Links",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Liquid argon is an excellent medium for detecting particles, given its yields and transport properties of light and charge. The technology of liquid argon time projection chambers has reached its full maturity after four decades of continuous developments and is, or will be, used in world class experiments for neutrino and dark matter searches. The collection of ionization charge in these detectors allows to perform a complete tridimensional reconstruction of the tracks of charged particles, calorimetric measurements, particle identification. This work proposes a novel approach to the problem of charge recombination in liquid argon which moves from a microscopic model and is applied to the cases of low energy electrons, alpha particles and nuclear recoils. The model is able to describe precisely several sets of experimental data available in the literature, over wide ranges of electric field strengths and kinetic energies and can be easily extended to other particles.",
        "citation_title": "Properties of Charge Recombination in Liquid Argon",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Astronomical (or Milankovi\u0107) forcing of the Earth system is key to understanding rhythmic climate change on time scales >~ 10 kyr. Paleoceanographic and paleoclimatological applications concerned with past astronomical forcing rely on astronomical calculations (solutions), which represent the backbone of cyclostratigraphy and astrochronology. Here we present state-of-the-art astronomical solutions over the past 3.5 Gyr. Our goal is to provide tuning targets and templates for interpreting deep-time cyclostratigraphic records and designing external forcing functions in climate models. Our approach yields internally consistent orbital and precession-tilt solutions, including fundamental solar system frequencies, orbital eccentricity and inclination, lunar distance, luni-solar precession rate, Earth's obliquity, and climatic precession. Contrary to expectations, we find that the long eccentricity cycle (previously assumed stable and labeled ''metronome'', recent period ~405 kyr), can become unstable on long time scales. Our results reveal episodes during which the long eccentricity cycle is very weak or absent and Earth's orbital eccentricity and climate-forcing spectrum are unrecognizable compared to the recent past. For the ratio of eccentricity-to-inclination amplitude modulation (frequently observable in paleorecords) we find a wide distribution around the recent 2:1 ratio, i.e., the system is not restricted to a 2:1 or 1:1 resonance state. Our computations show that Earth's obliquity was lower and its amplitude (variation around the mean) significantly reduced in the past. We therefore predict weaker climate forcing at obliquity frequencies in deep time and a trend toward reduced obliquity power with age in stratigraphic records. For deep-time stratigraphic and modeling applications, the orbital parameters of our 3.5-Gyr integrations are made available at 400-year resolution.",
        "citation_title": "Milankovi\u0107 Forcing in Deep Time",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This study introduces a systematic framework to compare the efficacy of Large Language Models (LLMs) for fine-tuning across various cheminformatics tasks. Employing a uniform training methodology, we assessed three well-known models-RoBERTa, BART, and LLaMA-on their ability to predict molecular properties using the Simplified Molecular Input Line Entry System (SMILES) as a universal molecular representation format. Our comparative analysis involved pre-training 18 configurations of these models, with varying parameter sizes and dataset scales, followed by fine-tuning them on six benchmarking tasks from DeepChem. We maintained consistent training environments across models to ensure reliable comparisons. This approach allowed us to assess the influence of model type, size, and training dataset size on model performance. Specifically, we found that LLaMA-based models generally offered the lowest validation loss, suggesting their superior adaptability across tasks and scales. However, we observed that absolute validation loss is not a definitive indicator of model performance - contradicts previous research - at least for fine-tuning tasks: instead, model size plays a crucial role. Through rigorous replication and validation, involving multiple training and fine-tuning cycles, our study not only delineates the strengths and limitations of each model type but also provides a robust methodology for selecting the most suitable LLM for specific cheminformatics applications. This research underscores the importance of considering model architecture and dataset characteristics in deploying AI for molecular property prediction, paving the way for more informed and effective utilization of AI in drug discovery and related fields.",
        "citation_title": "The Role of Model Architecture and Scale in Predicting Molecular Properties: Insights from Fine-Tuning RoBERTa, BART, and LLaMA",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Space Weather is the study of the conditions in the solar wind that can affect life on the surface of the Earth, particularly the increasingly technologically sophisticated devices that are part of modern life. Solar radio observations are relevant to such phenomena because they generally originate as events in the solar atmosphere, including flares, coronal mass ejections and shocks, that produce electromagnetic and particle radiations that impact the Earth. Low frequency solar radio emission arises in the solar atmosphere at the levels where these events occur: we can use frequency as a direct measure of density, and an indirect measure of height, in the atmosphere. The main radio burst types are described and illustrated using data from the Green Bank Solar Radio Burst Spectrometer, and their potential use as diagnostics of Space Weather is discussed.",
        "citation_title": "Solar Radio Bursts and Space Weather",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "There is a growing interest in reconstructing the density matrix of photoionized electrons, in particular in complex systems where decoherence can be introduced either by a partial measurement of the system or through coupling with a stochastic environment. To this, end, several methods to reconstruct the density matrix, quantum state tomography protocols, have been developed and tested on photoelectrons ejected from noble gases following absorption of XUV photons from attosecond pulses. It remains a challenge to obtain model-free, single scan protocols that can reconstruct the density matrix with high fidelities. Current methods require extensive measurements or involve complex fitting of the signal. Faithful single-scan reconstructions would be of great help to increase the number of systems that can be studied. We propose a new and more efficient protocol - rainbow-KRAKEN - that is able to reconstruct the continuous variable density matrix of a photoelectron in a single time delay scan. It is based on measuring the coherences of a photoelectron created by absorption of an XUV pulse using a broadband IR probe that is scanned in time and a narrowband IR reference that is temporally fixed to the XUV pulse. We illustrate its performance for a Fano resonance in He as well as mixed states in Ar arising from spin-orbit splitting. We show that the protocol results in excellent fidelities and near-perfect estimation of the purity.",
        "citation_title": "A multidimensional approach to quantum state tomography of photoelectron wavepackets",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Large language models (LLMs) have shown remarkable potential in various domains, but they often lack the ability to access and reason over domain-specific knowledge and tools. In this paper, we introduced CACTUS (Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that integrates cheminformatics tools to enable advanced reasoning and problem-solving in chemistry and molecular discovery. We evaluate the performance of CACTUS using a diverse set of open-source LLMs, including Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of thousands of chemistry questions. Our results demonstrate that CACTUS significantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b models achieving the highest accuracy regardless of the prompting strategy used. Moreover, we explore the impact of domain-specific prompting and hardware configurations on model performance, highlighting the importance of prompt engineering and the potential for deploying smaller models on consumer-grade hardware without significant loss in accuracy. By combining the cognitive capabilities of open-source LLMs with domain-specific tools, CACTUS can assist researchers in tasks such as molecular property prediction, similarity searching, and drug-likeness assessment. Furthermore, CACTUS represents a significant milestone in the field of cheminformatics, offering an adaptable tool for researchers engaged in chemistry and molecular discovery. By integrating the strengths of open-source LLMs with domain-specific tools, CACTUS has the potential to accelerate scientific advancement and unlock new frontiers in the exploration of novel, effective, and safe therapeutic candidates, catalysts, and materials. Moreover, CACTUS's ability to integrate with automated experimentation platforms and make data-driven decisions in real time opens up new possibilities for autonomous discovery.",
        "citation_title": "CACTUS: Chemistry Agent Connecting Tool-Usage to Science",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on $L_{1}$ regularization combined with cross-validation. As we demonstrate, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight \"shrinkage\". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.",
        "citation_title": "Network reconstruction via the minimum description length principle",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Realism about quantum theory naturally leads to realism about the quantum state of the universe. It leaves open whether it is a pure state represented by a wave function, or an impure one represented by a density matrix. I characterize and elaborate on Density Matrix Realism, the thesis that the universal quantum state is objective but can be impure. To clarify the thesis, I compare it with Wave Function Realism, explain the conditions under which they are empirically equivalent, consider two generalizations of Density Matrix Realism, and answer some frequently asked questions. I end by highlighting an implication for scientific realism.",
        "citation_title": "Density Matrix Realism",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The stability of perovskite solar cells is closely related to the defects in perovskite crystals, and there are a large number of crystal defects in the perovskite thin films prepared by the solution method, which is not conducive to the commercial production of PSCs. In this study, resveratrol(RES), a green natural antioxidant abundant in knotweed and grape leaves, was introduced into perovskite films to passivate the defect. RES achieves defect passivation by interacting with uncoordinated Pb2+ in perovskite films. The results show that the quality of the perovskite film is significantly improved, and the energy level structure of the device is optimized, and the power conversion efficiency of the device is increased from 21.62% to 23.44%. In addition, RES can hinder the degradation of perovskite structures by O2- and CO2- free radicals, and the device retained 88% of its initial PCE after over 1000 hours in pure oxygen environment. The device retains 91% of the initial PCE after more than 1000 hours at 25\u00b0C and 50+5% relative humidity. This work provides a strategy for the use of natural and environmentally friendly additives to improve the efficiency and stability of devices, and provides an idea for the development of efficient, stable and environmentally friendly PSCs.",
        "citation_title": "An eco-friendly passivation strategy of resveratrol for highly efficient and antioxidative perovskite solar cells",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The quantum dynamics of a dense and dipole-dipole coupled ensemble of two-level emitters interacting via their environmental thermostat is investigated. The static dipole-dipole interaction strengths are being considered strong enough but smaller than the transition frequency. Therefore, the established thermal equilibrium of ensemble's quantum dynamics is described with respect to the dipole-dipole coupling strengths. We have demonstrated the quantum nature of the spontaneously scattered light field in this process for weaker thermal baths as well as non-negligible dipole-dipole couplings compared to the emitter's transition frequency. Furthermore, the collectively emitted photon intensity suppresses or enhances depending on the environmental thermal baths intensities.",
        "citation_title": "Dense dipole-dipole-coupled two-level systems in a thermal bath",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Thin layers can lead to unfavorable meshes in a finite element (FE) analysis. Thin shell approximations (TSAs) avoid this issue by removing the need for a mesh of the thin layer while approximating the physics across the layer by an interface condition. Typically, a TSA requires the mesh of both sides of the TSA interface to be conforming. To alleviate this requirement, we propose to combine mortar methods and TSAs for solving the heat equation. The mortar TSA method's formulation is derived and enables an independent discretization of the subdomains on the two sides of the TSA depending on their accuracy requirements. The method is verified by comparison with a reference FE solution of a thermal model problem of a simplified superconducting accelerator magnet.",
        "citation_title": "Mortar Thin Shell Approximation for Analysis of Superconducting Accelerator Magnets",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The low efficiency of conventional liquefaction technologies based on the Joule-Thomson expansion makes liquid hydrogen currently not attractive enough for large-scale energy-related technologies that are important for the transition to a carbon-neutral society. Magnetocaloric hydrogen liquefaction has great potential to achieve higher efficiency and is therefore a crucial enabler for affordable liquid hydrogen. Cost-effective magnetocaloric materials with large magnetic entropy and adiabatic temperature changes in the temperature range of 77 $\\sim$ 20 K under commercially practicable magnetic fields are the foundation for the success of magnetocaloric hydrogen liquefaction. Heavy rare-earth-based magnetocaloric intermetallic compounds generally show excellent magnetocaloric performances, but the heavy rare-earth elements (Gd, Tb, Dy, Ho, Er, and Tm) are highly critical in resources. Yttrium and light rare-earth elements (La, Ce, Pr, and Nd) are relatively abundant, but their alloys generally show less excellent magnetocaloric properties. A dilemma appears: higher performance or lower criticality? In this review, we study how cryogenic temperature influences magnetocaloric performance by first reviewing heavy rare-earth-based intermetallic compounds. Next, we look at light rare-earth-based, \"mixed\" rare-earth-based, and Gd-based intermetallic compounds with the nature of the phase transition order taken into consideration, and summarize ways to resolve the dilemma.",
        "citation_title": "A matter of performance & criticality: a review of rare-earth-based magnetocaloric intermetallic compounds for hydrogen liquefaction",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Mixtures with many components can segregate into coexisting phases, e.g., in biological cells and synthetic materials such as metallic glass. The interactions between components dictate what phases form in equilibrium, but quantifying this relationship has proven difficult. We derive scaling relations for the number of coexisting phases in multicomponent liquids with random interactions and compositions, which we verify numerically. Our results indicate that interactions only need to increase logarithmically with the number of components for the liquid to segregate into many phases. In contrast, a stability analysis of the homogeneous state predicts a power-law scaling. This discrepancy implies an enormous parameter regime where the number of coexisting phases exceeds the number of unstable modes, generalizing the nucleation and growth regime of binary mixtures to many components.",
        "citation_title": "Scaling of phase count in multicomponent liquids",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This work explores the hypothesis that subjectively attributed meaning constitutes the phenomenal content of conscious experience. That is, phenomenal content is semantic. This form of subjective meaning manifests as an intrinsic and non-representational character of qualia. Empirically, subjective meaning is ubiquitous in conscious experiences. We point to phenomenological studies that lend evidence to support this. Furthermore, this notion of meaning closely relates to what Frege refers to as \"sense\", in metaphysics and philosophy of language. It also aligns with Peirce's \"interpretant\", in semiotics. We discuss how Frege's sense can also be extended to the raw feels of consciousness. Sense and reference both play a role in phenomenal experience. Moreover, within the context of the mind-matter relation, we provide a formalization of subjective meaning associated to one's mental representations. Identifying the precise maps between the physical and mental domains, we argue that syntactic and semantic structures transcend language, and are realized within each of these domains. Formally, meaning is a relational attribute, realized via a map that interprets syntactic structures of a formal system within an appropriate semantic space. The image of this map within the mental domain is what is relevant for experience, and thus comprises the phenomenal content of qualia. We conclude with possible implications this may have for experience-based theories of consciousness.",
        "citation_title": "Qualia and the Formal Structure of Meaning",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Understanding pedestrian dynamics is crucial for appropriately designing pedestrian spaces. The pedestrian fundamental diagram (FD), which describes the relationship between pedestrian flow and density within a given space, characterizes these dynamics. Pedestrian FDs are significantly influenced by the flow type, such as uni-directional, bi-directional, and crossing flows. However, to the authors' knowledge, generalized pedestrian FDs that are applicable to various flow types have not been proposed. This may be due to the difficulty of using statistical methods to characterize the flow types. The flow types significantly depend on the angles of pedestrian movement; however, these angles cannot be processed by standard statistics due to their periodicity. In this study, we propose a comprehensive model for pedestrian FDs that can describe the pedestrian dynamics for various flow types by applying Directional Statistics. First, we develop a novel statistic describing the pedestrian flow type solely from pedestrian trajectory data using Directional Statistics. Then, we formulate a comprehensive pedestrian FD model that can be applied to various flow types by incorporating the proposed statistics into a traditional pedestrian FD model. The proposed model was validated using actual pedestrian trajectory data. The results confirmed that the model effectively represents the essential nature of pedestrian dynamics, such as the capacity reduction due to conflict of crossing flows and the capacity improvement due to the lane formation in bi-directional flows.",
        "citation_title": "Modeling pedestrian fundamental diagram based on Directional Statistics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Tomography of single-particle-resolved detectors is of primary importance for characterizing particle correlations with applications in quantum metrology, quantum simulation and quantum computing. However, it is a non-trivial task in practice due to the unavoidable presence of noise that affects the measurement but does not originate from the detector. In this work, we address this problem for a three-dimensional single-atom-resolved detector where shot-to-shot atom number fluctuations are a central issue to perform a quantum detector tomography. We overcome this difficulty by exploiting the parallel measurement of counting statistics in sub-volumes of the detector, from which we evaluate the effect of shot-to-shot fluctuations and perform a local tomography of the detector. In addition, we illustrate the validity of our method from applying it to Gaussian quantum states with different number statistics. Finally, we show that the response of Micro-Channel Plate detectors is well-described from using a binomial distribution with the detection efficiency as a single parameter.",
        "citation_title": "Tomography of a single-atom-resolved detector in the presence of shot-to-shot number fluctuations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we investigate the localization of targets in the presence of multiple scattering. We focus on the often omitted scenario in which measurement data is affected by multiple scattering, and a simpler model is employed in the estimation. We study the impact of such model mismatch by means of the Misspecified Cram\u00e9r-Rao Bound (MCRB). In numerical simulations inspired by tomographic inspection in ultrasound nondestructive testing, the MCRB is shown to correctly describe the estimation variance of localization parameters under misspecification of the wave propagation model. We provide extensive discussion on the utility of the MCRB in the practical task of verifying whether a chosen misspecified model is suitable for localization based on the properties of the maximum likelihood estimator and the nuanced distinction between bias and parameter space differences. Finally, we highlight that careful interpretation is needed whenever employing the classical CRB in the presence of mismatch through numerical examples based on the Born approximation and other simplified propagation models stemming from it.",
        "citation_title": "Misspecification of Multiple Scattering in Scalar Wave Fields and its Impact in Ultrasound Tomography",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present new calculations of atomic data needed to model autoionizing states of Fe XVI. We compare the state energies, radiative and excitation data with a sample of results from previous literature. We find a large scatter of results, the most significant ones in the autoionization rates, which are very sensitive to the configuration interaction and state mixing. We find relatively good agreement between the autoionization rates and the collisional excitation rates calculated with the R-matrix suite of programs and autostructure. The largest model, which includes J-resolved states up to n=10, produces ab-initio wavelengths and intensities of the satellite lines which agree well with solar high-resolution spectra of active regions, with few minor wavelength adjustements. We review previous literature, finding many incorrect identifications, most notably those in the NIST database. We provide several new tentative identifications in the 15-15.7 A range, and several new ones at shorter wavelengths, where previous lines were unidentified. Compared to the previous CHIANTI model, the present one has an increased flux in the 15--15.7 A range at 2 MK of a factor of 1.9, resolving the discrepancies found in the analysis of the Marshall Grazing Incidence X-Ray Spectrometer (MaGIXS) observation. It appears that the satellite lines also resolve the long-standing discrepancy in the intensity of the important Fe XVII 3D line at 15.26 A.",
        "citation_title": "Satellite lines from autoionizing states of Fe XVI and the problems with the X-ray Fe XVII lines",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Humans spend over 90% of their time in buildings which account for 40% of anthropogenic greenhouse gas (GHG) emissions, making buildings the leading cause of climate change. To incentivize more sustainable construction, building codes are used to enforce indoor comfort standards and maximum energy use. However, they currently only reward energy efficiency measures such as equipment or envelope upgrades and disregard the actual spatial configuration and usage. Using a new hypergraph model that encodes building floorplan organization and facilitates automatic geometry creation, we demonstrate that space efficiency outperforms envelope upgrades in terms of operational carbon emissions in 72%, 61% and 33% of surveyed buildings in Zurich, New York, and Singapore. Automatically generated floorplans for a case study in Zurich further increase access to daylight by up to 24%, revealing that auto-generated floorplans have the potential to improve the quality of residential spaces in terms of environmental performance and access to daylight.",
        "citation_title": "A hypergraph model shows the carbon reduction potential of effective space use in housing",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Analyzing local structures effectively is key to unraveling the origin of many physical phenomena. Unsupervised algorithms offer an effective way of handling systems in which order parameters are unknown or computationally expensive. By combining novel unsupervised algorithm (Pairwise Controlled Manifold Approximation Projection) with atomistic potential descriptors, we distinguish between various chemical environments with minimal computational overhead. In particular, we apply this method to silicon and water systems. The algorithm effectively distinguishes between solid structures and phases of silicon, including solid and liquid phases, and accurately identifies interstitial, monovacancy, and surface atoms in diamond structures. In the case of water, it is capable of identifying an ice nucleus in the liquid phase, demonstrating its applicability in nucleation studies.",
        "citation_title": "Unsupervised identification of local atomic environment from atomistic potential descriptors",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Surface diffusion and surface electromigration may lead to a morphological instability of thin solid films and nanowires. In this paper two nonlinear analyzes of a morphological instability are developed for a single-crystal cylindrical nanowire that is subjected to the axial current. These treatments extend the conventional linear stability analyzes without surface electromigration, that manifest a Rayleigh-Plateau instability. A weakly nonlinear analysis is done slightly above the Rayleigh-Plateau (longwave) instability threshold. It results in a one-dimensional Sivashinsky amplitude equation that describes a blow-up of a surface perturbation amplitude in a finite time. This is a signature of a formation of an axisymmetric spike singularity of a cylinder radius, which leads to a wire pinch-off and separation into a disjoint segments. The scaling analysis of the amplitude spike singularity is performed, and the time-and-electric field-dependent dimensions of the spike are characterized. A weakly nonlinear multi-scale analysis is done at the arbitrary distance above a longwave or a shortwave instability threshold. The time-and-electric field-dependent Fourier amplitudes of the major instability modes are derived and characterized.",
        "citation_title": "On Nanowire Morphological Instability and Pinch-Off by Surface Electromigration",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "3-Hydroxypropenal (HOCHCHCHO) is the lower energy tautomer of malonaldehyde which displays a complex rotation-tunneling spectrum. It was detected tentatively toward the solar-type protostar IRAS 16293$-$2422 B with ALMA in the framework of the Protostellar Interferometric Line Survey (PILS). Several transitions, however, had large residuals, preventing not only their detection, but also the excitation temperature of the species from being determined unambiguously. We want to extend the existing rotational line list of 3-hydroxypropenal to shed more light on the recent observational results and to facilitate additional radio astronomical searches for this molecule. We analyzed the rotation-tunneling spectrum of 3-hydroxypropenal in the frequency regions between 150 and 330 GHz and between 400 and 660 GHz. Transitions were searched for in the PILS observations of IRAS 16293$-$2422. Local thermodynamic equilibrium (LTE) models were carried out and compared to the observations to constrain the excitation temperature. Additional transitions were searched for in other ALMA archival data of the same source to confirm the presence of 3-hydroxypropenal. More than 11500 transitions were assigned in the course of our investigation with quantum numbers $2 \\le J \\le 100$, $K_a \\le 59$, and $K_c \\le 97$, resulting in a greatly improved set of spectroscopic parameters. The comparison between the LTE models and the observations yields an excitation temperature of 125 K with a column density $N = 1.0 \\times 10^{15}$ cm$^{-2}$ for this species. We identified seven additional lines of 3-hydroxypropenal that show a good agreement with the model in the ALMA archive data. The calculated rotation-tunneling spectrum of 3-hydroxypropenal has sufficient accuracy for radio astronomical searches. The detection of 3-hydroxypropenal toward IRAS 16293$-$2422 B is now secure.",
        "citation_title": "The rotation-tunneling spectrum of 3-hydroxypropenal and confirmation of its detection toward IRAS 16293$-$2422 B",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Large-scale machines like particle accelerators are usually run by a team of experienced operators. In case of a particle accelerator, these operators possess suitable background knowledge on both accelerator physics and the technology comprising the machine. Due to the complexity of the machine, particular subsystems of the machine are taken care of by experts, who the operators can turn to. In this work the reasoning and action (ReAct) prompting paradigm is used to couple an open-weights large language model (LLM) with a high-level machine control system framework and other tools, e.g. the electronic logbook or machine design documentation. By doing so, a multi-expert retrieval augmented generation (RAG) system is implemented, which assists operators in knowledge retrieval tasks, interacts with the machine directly if needed, or writes high level control system scripts. This consolidation of expert knowledge and machine interaction can simplify and speed up machine operation tasks for both new and experienced human operators.",
        "citation_title": "GAIA: A General AI Assistant for Intelligent Accelerator Operations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "As spacecraft journey further from Earth with more complex missions, systems of greater autonomy and onboard intelligence are called for. Reducing reliance on human-based mission control becomes increasingly critical if we are to increase our rate of solar-system-wide exploration. Recent work has explored AI-based goal-oriented systems to increase the level of autonomy in mission execution. These systems make use of symbolic reasoning managers to make inferences from the state of a spacecraft and a handcrafted knowledge base, enabling autonomous generation of tasks and re-planning. Such systems have proven to be successful in controlled cases, but they are difficult to implement as they require human-crafted ontological models to allow the spacecraft to understand the world. Reinforcement learning has been applied to train robotic agents to pursue a goal. A new architecture for autonomy is called for. This work explores the application of Large Language Models (LLMs) as the high-level control system of a spacecraft. Using a systems engineering approach, this work presents the design and development of an agentic spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate the utility of such an architecture in achieving higher levels of spacecraft autonomy. A series of deep space mission scenarios simulated within the popular game engine Kerbal Space Program (KSP) are used as case studies to evaluate the implementation against the requirements. It is shown the reasoning and planning abilities of present-day LLMs do not scale well as the complexity of a mission increases, but this can be alleviated with adequate prompting frameworks and strategic selection of the agent's level of authority over the host spacecraft. This research evaluates the potential of LLMs in augmenting autonomous decision-making systems for future robotic space applications.",
        "citation_title": "LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration",
        "date_delivered": "[Submitted on 13 Apr 2024]"
    },
    {
        "abstract": "GROMACS is a widely-used molecular dynamics software package with a focus on performance, portability, and maintainability across a broad range of platforms. Thanks to its early algorithmic redesign and flexible heterogeneous parallelization, GROMACS has successfully harnessed GPU accelerators for more than a decade. With the diversification of accelerator platforms in HPC and no obvious choice for a multi-vendor programming model, the GROMACS project found itself at a crossroads. The performance and portability requirements, and a strong preference for a standards-based solution, motivated our choice to use SYCL on both new HPC GPU platforms: AMD and Intel. Since the GROMACS 2022 release, the SYCL backend has been the primary means to target AMD GPUs in preparation for exascale HPC architectures like LUMI and Frontier. SYCL is a cross-platform, royalty-free, C++17-based standard for programming hardware accelerators. It allows using the same code to target GPUs from all three major vendors with minimal specialization. While SYCL implementations build on native toolchains, performance of such an approach is not immediately evident. Biomolecular simulations have challenging performance characteristics: latency sensitivity, the need for strong scaling, and typical iteration times as short as hundreds of microseconds. Hence, obtaining good performance across the range of problem sizes and scaling regimes is particularly challenging. Here, we share the results of our work on readying GROMACS for AMD GPU platforms using SYCL, and demonstrate performance on Cray EX235a machines with MI250X accelerators. Our findings illustrate that portability is possible without major performance compromises. We provide a detailed analysis of node-level kernel and runtime performance with the aim of sharing best practices with the HPC community on using SYCL as a performance-portable GPU framework.",
        "citation_title": "GROMACS on AMD GPU-Based HPC Platforms: Using SYCL for Performance and Portability",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A formalism of classical mechanics is given for time-dependent many-body states of quantum mechanics, describing both fluid flow and point mass trajectories. The familiar equations of energy, motion, and those of Lagrangian mechanics are obtained. An energy and continuity equation is demonstrated to be equivalent to the real and imaginary parts of the time dependent Schroedinger equation, respectively, where the Schroedinger equation is in density matrix form. For certain stationary states, using Lagrangian mechanics and a Hamiltonian function for quantum mechanics, equations for point-mass trajectories are obtained. For 1-body states and fluid flows, the energy equation and equations of motion are the Bernoulli and Euler equations of fluid mechanics, respectively. Generalizations of the energy and Euler equations are derived to obtain equations that are in the same form as they are in classical mechanics. The fluid flow type is compressible, inviscid, irrotational, with the nonclassical element of local variable mass. Over all space mass is conserved. The variable mass is a necessary condition for the fluid flow to agree with the zero orbital angular momentum for s states of hydrogen. Cross flows are examined, where velocity directions are changed without changing the kinetic energy. For one-electron atoms, the velocity modification gives closed orbits for trajectories, and mass conservation, vortexes, and density stratification for fluid flows. For many body states, Under certain conditions, and by hypotheses, Euler equations of orbital-flows are obtained. One-body Schroedinger equations that are a generalization of the Hartree-Fock equations are also obtained. These equations contain a quantum Coulomb's law, involving the 2-body pair function of reduced density matrix theory that replace the charge densities.",
        "citation_title": "A Formulation of Quantum Fluid Mechanics and Trajectories",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We observe an inverse turbulent-wave cascade, from small to large lengthscales, in a homogeneous 2D Bose gas driven isotropically on a lengthscale much smaller than its size. Starting with an equilibrium condensed gas, at long drive times we observe a nonthermal steady state. At increasing lengthscales, starting from the forcing one, the steady-state momentum distribution features in turn: (i) a power-law spectrum, with an exponent close to the analytical result for a particle cascade in weak-wave turbulence, and (ii) a spectrum intriguingly reminiscent of a nonthermal fixed point associated with universal coarsening in an isolated 2D gas. In further experiments, based on anisotropic driving, we also reveal the qualitative picture of the cascade-formation dynamics.",
        "citation_title": "Observation of an inverse turbulent-wave cascade in a driven quantum gas",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A new reference state for density functional theory, termed the independent atom ansatz, is introduced in this work. This ansatz allows for the exact representation of electron density in terms of non-interacting, atom-localized orbitals. Self-consistent equations for localized states are derived. Total energy functional is found to closely resemble tight binding theory. The independent atom ansatz facilitates partial cancellation of inter-atomic electron-electron and electron-nuclear interactions, which allows for the derivation of analytical Hamiltonian matrix elements in a weak interaction limit. The formalism provides charge and energy decomposition analyses at no additional cost. It also includes mechanisms to remove self-interaction and static correlation errors. Initial numerical results for simple model systems have been previously reported [Mironenko, J. Phys. Chem. A 127, 7836 (2023)].",
        "citation_title": "Self-Consistent Equations for Nonempirical Tight Binding Theory",
        "date_delivered": "[Submitted on 9 Apr 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Both evaluating the service quality of a public transport system and understanding how passengers choose between modes or routes is imperative for public transport operators, providers of competing mobility services and policy makers. However, the literature does not offer consensus on how either of these tasks should be performed, which can lead to inconsistent or counter-intuitive results. This paper provides a formal treatment on how common manifestations of public transport (route sets, timetables and line plans) can be evaluated consistently, and how passengers distribute over routes. Our main insight is that evaluation and routing are two sides of the same coin: by solving an appropriate optimization model one obtains both the quality of the route set, timetable or line plan (the optimal objective value), and the distribution of the travelers over the routes (the optimal solution itself). We further demonstrate that the framework developed in this paper enables planners to (i) improve service by taking better decisions and (ii) assess to what degree of accuracy traveler behavior should be modeled on their network, potentially avoiding investing in complicated methods that may not be necessary.",
        "citation_title": "A Unified Approach to Evaluation and Routing in Public Transport Systems",
        "date_delivered": "[Submitted on 20 Jul 2022 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Atmospheric environments favorable for lightning and convection are commonly represented by proxies or parameterizations based on expert knowledge such as CAPE, wind shears, charge separation, or combinations thereof. Recent developments in the field of machine learning, high resolution reanalyses, and accurate lightning observations open possibilities for identifying tailored proxies without prior expert knowledge. To identify vertical profiles favorable for lightning, a deep neural network links ERA5 vertical profiles of cloud physics, mass field variables and wind to lightning location data from the Austrian Lightning Detection and Information System (ALDIS), which has been transformed to a binary target variable labelling the ERA5 cells as cells with lightning activity and cells without lightning activity. The ERA5 parameters are taken on model levels beyond the tropopause forming an input layer of approx. 670 features. The data of 2010-2018 serve as training/validation. On independent test data, 2019, the deep network outperforms a reference with features based on meteorological expertise. SHAP values highlight the atmospheric processes learned by the network which identifies cloud ice and snow content in the upper and mid-troposphere as very relevant features. As these patterns correspond to the separation of charge in thunderstorm cloud, the deep learning model can serve as physically meaningful description of lightning. Depending on the region, the neural network also exploits the vertical wind or mass profiles to correctly classify cells with lightning activity.",
        "citation_title": "Identifying Lightning Processes in ERA5 Soundings with Deep Learning",
        "date_delivered": "[Submitted on 20 Oct 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The atomic vibrations of a solid surface can play a significant role in the reactions of surface-bound molecules, as well as their adsorption and desorption. Relevant phonon modes can involve the collective motion of atoms over a wide array of length scales. In this manuscript, we demonstrate how the generalized Langevin equation can be utilized to describe these collective motions weighted by their coupling to individual sites. Our approach builds upon the generalized Langevin oscillator (GLO) model originally developed by Tully \\textit{et al.} We extend the GLO by deriving parameters from atomistic simulation data. We apply this approach to study the memory kernel of a model platinum surface and demonstrate that the memory kernel has a bimodal form due to coupling to both low-energy acoustic modes and high-energy modes near the Debye frequency. The same bimodal form was observed across a wide variety of solids of different elemental compositions, surface structures, and solvation states. By studying how these dominant modes depend on simulation size, we argue that the acoustic modes are frozen in the limit of macroscopic lattices. By simulating periodically replicated slabs of various sizes we quantify the influence of phonon confinement effects in the memory kernel and their concomitant effect on simulated sticking coefficients.",
        "citation_title": "Modeling surface vibrations and their role in molecular adsorption: a generalized Langevin approach",
        "date_delivered": "[Submitted on 12 Jan 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We present a data-driven reduced-order modeling of the space-charge dynamics for electromagnetic particle-in-cell (EMPIC) plasma simulations based on dynamic mode decomposition (DMD). The dynamics of the charged particles in kinetic plasma simulations such as EMPIC is manifested through the plasma current density defined along the edges of the spatial mesh. We showcase the efficacy of DMD in modeling the time evolution of current density through a low-dimensional feature space. Not only do such DMD-based predictive reduced-order models help accelerate EMPIC simulations, they also have the potential to facilitate investigative analysis and control applications. We demonstrate the proposed DMD-EMPIC scheme for reduced-order modeling of current density, and speed-up in EMPIC simulations involving electron beam under the influence of magnetic field, virtual cathode oscillations, and backward wave oscillator.",
        "citation_title": "Accelerating Particle-in-Cell Kinetic Plasma Simulations via Reduced-Order Modeling of Space-Charge Dynamics using Dynamic Mode Decomposition",
        "date_delivered": "[Submitted on 28 Mar 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "A fatigue-failure process is hypothesized to govern the development of tibial stress fractures, where bone damage is highly dependent on the peak strain magnitude. To date, much of the work examining tibial strains during running has ignored uphill and downhill running despite the prevalence of this terrain. This study examined the sensitivity of tibial strains to changes in running grade and speed using a combined musculoskeletal-finite element modeling routine. Seventeen participants ran on a treadmill at $\\pm$10\u00b0, $\\pm$5\u00b0, and 0\u00b0; at each grade, participants ran at 3.33 m/s and a grade-adjusted speed - 2.50 m/s and 4.17 m/s for uphill and downhill conditions, respectively. Force and motion data were recorded in each grade and speed combination. Muscle and joint contact forces were estimated using inverse-dynamics-based static optimization. These forces were applied to a participant-informed finite element model of the tibia. None of the strain variables (50th and 95th percentile strain and strained volume $\\geq$4000 $\\mu\\varepsilon$) differed as a function of running grade; however, all strain variables were sensitive to running speed (F(1)$\\geq$9.59, p$\\leq$0.03). In particular, a 1 m/s increase in running speed resulted in a 9% ($\\approx$ 260 $\\mu\\varepsilon$) and 155% ($\\approx$ 600 mm^3) increase in peak strain and strained volume, respectively. Overall, these findings suggest that faster running speed, but not changes in running grade, may be more deleterious to the tibia.",
        "citation_title": "Tibial Strains are Sensitive to Speed, but not Grade, Perturbations During Running",
        "date_delivered": "[Submitted on 6 May 2023 (v1), last revised 1 May 2024 (this version, v5)]"
    },
    {
        "abstract": "The purpose of this paper is to study the dynamics of a coherent feedback network where two two-level atoms are coupled with a semi-infinite waveguide. In this set-up, the two-level atoms can work as the photon source, and the photons can be emitted into the waveguide via the nonchiral or chiral couplings between the atom and the waveguide, according to whether the coupling strengths between the atoms and different directional propagating modes in the waveguide are identical or not. For the photon emitted by one of the two atoms, it can be reflected by the terminal mirror, or interact with the other atom, and then the photon can re-interact with the former atom. When the two atoms are both initially excited, finally there can be two-photon, one-photon or zero-photon states in the waveguide via the spontaneous emission and feedback interactions, and this is influenced by the locations of the atoms and the chirality of the coupling between the atom and the waveguide. Similarly, if only one of the two atoms is initially excited, there can be zero or one photon in the waveguide. Thus we can control the number of the photons in the waveguide and the atomic states by tuning the feedback loop length and the chiral couplings between the atom and waveguide. The photonic state in the waveguide is analyzed in the frequency domain and the spatial domain, and the transient process of photon emissions can be better understood based on the comprehensive analysis in these two domains.",
        "citation_title": "Quantum feedback control of a two-atom network closed by a semi-infinite waveguide",
        "date_delivered": "[Submitted on 10 Jun 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Widefield microscopy is widely used for non-invasive imaging of biological structures at subcellular resolution. When applied to complex specimen, its image quality is degraded by sample-induced optical aberration. Adaptive optics can correct wavefront distortion and restore diffraction-limited resolution but require wavefront sensing and corrective devices, increasing system complexity and cost. Here, we describe a self-supervised machine learning algorithm, CoCoA, that performs joint wavefront estimation and three-dimensional structural information extraction from a single input 3D image stack without the need for external training dataset. We implemented CoCoA for widefield imaging of mouse brain tissues and validated its performance with direct-wavefront-sensing-based adaptive optics. Importantly, we systematically explored and quantitatively characterized the limiting factors of CoCoA's performance. Using CoCoA, we demonstrated the first in vivo widefield mouse brain imaging using machine-learning-based adaptive optics. Incorporating coordinate-based neural representations and a forward physics model, the self-supervised scheme of CoCoA should be applicable to microscopy modalities in general.",
        "citation_title": "Coordinate-based neural representations for computational adaptive optics in widefield microscopy",
        "date_delivered": "[Submitted on 7 Jul 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The AC-coupled Strip LGAD (Strip AC-LGAD) is a novel LGAD design that diminishes the density of readout electronics through the use of strip electrodes, enabling the simultaneous measurement of time and spatial information. The Institute of High Energy Physics has designed a long Strip AC-LGAD prototype with a strip electrode length of 5.7 mm and pitches of 150 $\\mu m$, 200 $\\mu m$, and 250 $\\mu m$. Spatial and timing resolutions of the long Strip AC-LGAD are studied by pico-second laser test and beta source tests. The laser test demonstrates that spatial resolution improves as the pitch size decreases, with an optimal resolution achieved at 8.3 $\\mu$m. Furthermore, the Beta source test yields a timing resolution of 37.6 ps.",
        "citation_title": "The Performance of AC-coupled Strip LGAD developed by IHEP",
        "date_delivered": "[Submitted on 8 Jul 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "This article presents a general reduced order model (ROM) framework for addressing fluid dynamics problems involving time-dependent geometric parametrisations. The framework integrates Proper Orthogonal Decomposition (POD) and Empirical Cubature Method (ECM) hyper-reduction techniques to effectively approximate incompressible computational fluid dynamics simulations. To demonstrate the applicability of this framework, we investigate the behavior of a planar contraction-expansion channel geometry exhibiting bifurcating solutions known as the Coanda effect. By introducing time-dependent deformations to the channel geometry, we observe hysteresis phenomena in the solution.\nThe paper provides a detailed formulation of the framework, including the stabilised finite elements full order model (FOM) and ROM, with a particular focus on the considerations related to geometric parametrisation. Subsequently, we present the results obtained from the simulations, analysing the solution behavior in a phase-space for the fluid velocity at a probe point, considered as the Quantity of Interest (QoI). Through qualitative and quantitative evaluations of the ROMs and hyper-reduced order models (HROMs), we demonstrate their ability to accurately reproduce the complete solution field and the QoI.\nWhile HROMs offer significant computational speedup, enabling efficient simulations, they do exhibit some errors, particularly for testing trajectories. However, their value lies in applications where the detection of the Coanda effect holds paramount importance, even if the selected bifurcation branch is incorrect. Alternatively, for more precise results, HROMs with lower speedups can be employed.",
        "citation_title": "Geometrically Parametrised Reduced Order Models for the Study of Hysteresis of the Coanda Effect in Finite Element-based Incompressible Fluid Dynamics",
        "date_delivered": "[Submitted on 11 Jul 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We report experimental observations of the volume acoustic modes of air bubbles in water, including both the fundamental Minnaert breathing mode and a family of higher-order modes extending into the megahertz frequency range. Bubbles were placed on or near optomechanical sensors having a noise floor substantially determined by ambient medium fluctuations, and which are thus able to detect thermal motions of proximate objects. Bubble motions could be coupled to the sensor through both air (i.e., with the sensor inside the bubble) and water, verifying that sound is radiated by the high-order modes. We also present evidence for elastic-Purcell-effect modifications of the sensor's vibrational spectrum when encapsulated by a bubble, in the form of cavity-modified linewidths and line shifts.",
        "citation_title": "Coupling the thermal acoustic modes of a bubble to an optomechanical sensor",
        "date_delivered": "[Submitted on 3 Aug 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We observe strongly anisotropic third-harmonic generation mediated by resonant sum-frequency driving of Raman phonons with THz light, extending light-induced dual control of structural and optical properties in solids. Either strong enhancement or strong suppression of the third harmonic covering six orders of magnitude can be achieved, a result of interference between purely electronic and phonon-mediated contributions to the polarization field. These findings enrich capabilities for tailoring nonlinear optics via phononics and for the spectroscopy of crystalline structural dynamics.",
        "citation_title": "Phonon-Mediated Third-Harmonic Generation in Diamond",
        "date_delivered": "[Submitted on 31 Aug 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Cell rearrangements are fundamental mechanisms driving large-scale deformations of living tissues. In three-dimensional (3D) space-filling cell aggregates, cells rearrange through local topological transitions of the network of cell-cell interfaces, which is most conveniently described by the vertex model. Since these transitions are not yet mathematically properly formulated, the 3D vertex model is generally difficult to implement. The few existing implementations rely on highly customized and complex software-engineering solutions, which cannot be transparently delineated and are thus mostly non-reproducible. To solve this outstanding problem, we propose a reformulation of the vertex model. Our approach, called Graph Vertex Model (GVM), is based on storing the topology of the cell network into a knowledge graph with a particular data structure that allows performing cell-rearrangement events by simple graph transformations. Importantly, when these same transformations are applied to a two-dimensional (2D) polygonal cell aggregate, they reduce to a well-known T1 transition, thereby generalizing cell-rearrangements in 2D and 3D space-filling packings. This result suggests that the GVM's graph data structure may be the most natural representation of cell aggregates and tissues. We also develop a Python package that implements GVM, relying on a graph-database-management framework Neo4j. We use this package to characterize an order-disorder transition in 3D cell aggregates, driven by active noise and we find aggregates undergoing efficient ordering close to the transition point. In all, our work showcases knowledge graphs as particularly suitable data models for structured storage, analysis, and manipulation of tissue data.",
        "citation_title": "Graph topological transformations in space-filling cell aggregates",
        "date_delivered": "[Submitted on 9 Sep 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Gravitational Waves (GWs) have been detected in the $\\sim$100 Hz and nHz bands, but most of the gravitational spectrum remains unobserved. A variety of detector concepts have been proposed to expand the range of observable frequencies. In this work, we study the capability of GW detectors in the ``mid-band'', the $\\sim$30 mHz -- 10 Hz range between LISA and LIGO, to measure the signals from and constrain the properties of ${\\sim}$1 -- 100 $M_\\odot$ compact binaries. We focus on atom-interferometer-based detectors. We describe a Fisher matrix code, AIMforGW, which we created to evaluate their capabilities, and present numerical results for two benchmarks: terrestrial km-scale detectors, and satellite-borne detectors in medium Earth orbit. Mid-band GW detectors are particularly well-suited to pinpointing the location of GW sources on the sky. We demonstrate that a satellite-borne detector could achieve sub-degree sky localization for any detectable source with chirp mass $\\mathcal{M}_c \\lesssim 50 M_\\odot$. We also compare different detector configurations, including different locations of terrestrial detectors and various choices of the orbit of a satellite-borne detector. As we show, a network of only two terrestrial single-baseline detectors or one single-baseline satellite-borne detector would each provide close-to-uniform sky-coverage, with signal-to-noise ratios varying by less than a factor of two across the entire sky. We hope that this work contributes to the efforts of the GW community to assess the merits of different detector proposals.",
        "citation_title": "Gravitational Wave Measurement in the Mid-Band with Atom Interferometers",
        "date_delivered": "[Submitted on 14 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In this manuscript, we provide a general theory for how surface phonons couple to molecular adsorbates. Our theory maps the extended dynamics of a surface's atomic vibrational motions to a generalized Langevin equation, and by doing so captures these dynamics in a single quantity: the non-Markovian friction. The different frequency components of this friction are the phonon modes of the surface slab weighted by their coupling to the adsorbate degrees of freedom. Using this formalism, we demonstrate that physisorbed species couple primarily to acoustic phonons while chemisorbed species couple to dispersionless local vibrations. We subsequently derive equations for phonon-adjusted reaction rates using transition state theory and demonstrate that these corrections improve agreement with experimental results for CO desorption rates from Pt(111).",
        "citation_title": "A theory of phonon induced friction on molecular adsorbates",
        "date_delivered": "[Submitted on 15 Sep 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "For a circuit made of thermodynamic devices in stationary nonequilibrium, we determine the mean currents (of energy, matter, charge, etc) exchanged with external reservoirs driving the circuit out of equilibrium. Starting from the conductance matrix describing the nonlinear current--force characteristics of each device, we obtain the conductance matrix of the composite device. This generalizes the rule of resistance addition (serial association) or conductance addition (parallel association) in stationary out-of-equilibrium thermodynamics and for multiple coupled potentials and currents of different natures. Our work emphasizes the pivotal role of conservation laws when creating circuits of complex devices. Finally, two examples illustrate the determination of the conservation laws for the serial and parallel associations of thermodynamic devices.",
        "citation_title": "Thermodynamic Circuits I: Association of devices in stationary nonequilibrium",
        "date_delivered": "[Submitted on 22 Sep 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Quantum-clock interferometry has been suggested as a quantum probe to test the universality of free fall (UFF) and the universality of gravitational redshift (UGR). In typical experimental schemes it seems advantageous to employ Doppler-free E1-M1 transitions which have so far been investigated in quantum gases at rest. Here, we consider the fully quantized atomic degrees of freedom and study the interplay of the quantum center-of-mass (COM) $-$ that can become delocalized $-$ together with the internal clock transitions. In particular, we derive a model for finite-time E1-M1 transitions with atomic intern-extern coupling and arbitrary position-dependent laser intensities. We further provide generalizations to the ideal expressions for perturbed recoilless clock pulses. Finally, we show at the example of a Gaussian laser beam that the proposed quantum-clock interferometers are stable against perturbations from varying optical fields for a sufficiently small quantum delocalization of the atomic COM.",
        "citation_title": "Finite Pulse-Time Effects in Long-Baseline Quantum Clock Interferometry",
        "date_delivered": "[Submitted on 25 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The exploration of a two-dimensional wind-driven ocean model with no-slip boundaries reveals the existence of a turbulent asymptotic regime where energy dissipation becomes independent of fluid viscosity. This asymptotic flow represents an out-of-equilibrium state, characterized by a vigorous two-dimensional vortex gas superimposed onto a western-intensified gyre. The properties of the vortex gas are elucidated through scaling analysis for detached Prandtl boundary layers, providing a rationalization for the observed anomalous dissipation. The asymptotic regime demonstrates that boundary instabilities alone can be strong enough to evacuate wind-injected energy from the large-scale oceanic circulation.",
        "citation_title": "Gyre Turbulence",
        "date_delivered": "[Submitted on 3 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Inertial effects should be considered for micro- and nano-swimmers moving in a low-density medium confined by irregular structures that create entropic barriers, where viscous effects are no longer paramount. Here, we present a separation mechanism of self-propelled particles in a two-dimensional asymmetric channel, which leads to the drift of particles of different masses in opposite directions. In particular, this mechanism is based on the combined action of the spatial asymmetry of the channel structure, the temporal asymmetry inherent in particles dynamics, and an external static force. This work is relevant for potential applications that can be found in the development of lab-on-a-chip devices and artificial channels for separating particles of different masses.",
        "citation_title": "Mass-based separation of active Brownian particles in an asymmetric channel",
        "date_delivered": "[Submitted on 24 Oct 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Gaining recognition as a physics person from peers is an important contributor to undergraduate students' physics identity and, in turn, their success in physics courses. Previous research has separately demonstrated that women perceive less recognition from peers than men in their physics courses (perceived recognition) and that women receive fewer nominations from their peers as strong in their physics course than men (received recognition). The relationship between perceived and received peer recognition, however, is not well understood. Here we test three plausible models for this relationship. We conduct a large-scale, quantitative study of over 1,600 students enrolled in introductory physics courses at eight different institutions. We directly compare student gender, perceived recognition, and received recognition, controlling for other student demographics and course-level variability. Results show with high precision that, for students receiving the same amount of recognition, and having the same race or ethnicity, academic year, and major, women report significantly lower perceived recognition than men. These findings offer important implications for the design of effective instructional interventions.",
        "citation_title": "Bias in physics peer recognition does not explain gaps in perceived recognition",
        "date_delivered": "[Submitted on 30 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We demonstrate qubit state measurements assisted by a supervised convolutional neural network (CNN) in a neutral atom quantum processor. We present two CNN architectures for analyzing neutral atom qubit readout data: a compact 5-layer single-qubit CNN architecture and a 6-layer multi-qubit CNN architecture. We benchmark both architectures against a conventional Gaussian threshold analysis method. In a sparse array (9 {\\mu}m atom separation) which experiences negligible crosstalk, we observed up to 32% and 56% error reduction for the multi-qubit and single-qubit architectures respectively, as compared to the benchmark. In a tightly spaced array (5 {\\mu}m atom separation), which suffers from readout crosstalk, we observed up to 43% and 32% error reduction in the multi-qubit and single-qubit CNN architectures respectively, as compared to the benchmark. By examining the correlation between the predicted states of neighboring qubits, we found that the multi-qubit CNN architecture reduces the crosstalk correlation up to 78.5%. This work demonstrates a proof of concept for a CNN network to be implemented as a real-time readout processing method on a neutral atom quantum computer, enabling faster readout time and improved fidelity.",
        "citation_title": "Enhanced Measurement of Neutral Atom Qubits with Machine Learning",
        "date_delivered": "[Submitted on 20 Nov 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In the rapidly evolving area of integrated photonics, there is a growing need for materials that satisfy the particular requirements of increasingly complex and specialized devices and applications. Present photonic material platforms have made significant progress over the past years; however, each platform still faces specific material and performance challenges. We introduce a novel material for integrated photonics: Aluminum Gallium Nitride (AlGaN) on Aluminum Nitride (AlN) as a platform for developing reconfigurable and nonlinear on-chip optical systems. AlGaN combines compatibility with standard semiconductor fabrication technologies, high electro-optic modulation capabilities, and large nonlinear coefficients while providing a broad and low-loss spectral transmission range, making it a viable material for advanced photonic applications. In this work, we design and grow AlGaN/AlN heterostructures and integrate fundamental photonic building blocks into these chips. In particular, we fabricate edge couplers, low-loss waveguides, directional couplers, and tunable high-quality factor ring resonators to enable nonlinear light-matter interaction and quantum functionality. The comprehensive platform we present in this work paves the way for nonlinear photon-pair generation applications, on-chip nonlinear quantum frequency conversion, and fast electro-optic modulation for switching and routing classical and quantum light fields.",
        "citation_title": "AlGaN/AlN heterostructures: an emerging platform for nonlinear integrated photonics",
        "date_delivered": "[Submitted on 5 Dec 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The Chern number has been widely used to describe the topological properties of periodic structures in the momentum space. Here, we introduce a real-space spin Chern number for the optical near fields of finite-sized structures. This new spin Chern number is intrinsically quantized and equal to the structure's Euler characteristic. The relationship is robust against continuous deformation of the structure's geometry and is irrelevant to the specific material constituents or external excitation. Our work enriches topological physics by extending the concept of Chern number to the real space, opening exciting possibilities for exploring the real-space topological properties of light.",
        "citation_title": "Near-field Spin Chern Number Quantized by Real-space Topology of Optical Structures",
        "date_delivered": "[Submitted on 19 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "In Gale Crater near Mars' equator, dunes and ripples of sand stand out from the general orderless, rocky terrain. In addition, images from Curiosity, the Mars Science Laboratory rover, reveal more subtle orderly forms: widespread, meter-scale domains of evenly spaced, pebble-size rocks (termed clasts) on wind-blown sand in scattered locations. Here, we examine quantitatively several clast domains on both Mars and Earth, and compare their geometry with that of random points. The clast distributions are more orderly than expected by chance; they differ significantlty from those associated with uniform (Poisson) random processes. Moreover, they are hyperuniform, a self-organized state recently recognized in diverse active materials and biological systems but that appears novel for planetary surfaces. These patches are often surrounded by recent wind-borne ripples, suggesting an interplay between sand transport, ripple activity and clasts. Using numerical simulations, we show that clast displacements induced by gravity, combined with the evolution of the sand surface caused by aeolian sand transport and ripple migration, can produce realistic hyperuniform and random clast distributions, as well as distinct clast alignements. Our findings highlight the existence of easily overlooked disordered hyperuniform states on ground surfaces, suggesting novel self-organized states beyond distinct geometric patterns.",
        "citation_title": "Hyperuniformity on Mars: Pebbles scattered on sand",
        "date_delivered": "[Submitted on 21 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $\\Omega(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that significantly outperforms this quadratic baseline. Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011) that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. If we rely on the conjecture that the second-neighbor search finishes in log-linear time (Baron & Darling, 2020; 2022), we demonstrate theoretically that our algorithm finishes in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\\log N)$, but with a more typical log-linear complexity of $O(N\\log^2N)$. In practice, we show that our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline -- in a manner consistent with our theoretical analysis -- allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.",
        "citation_title": "Scalable network reconstruction in subquadratic time",
        "date_delivered": "[Submitted on 2 Jan 2024 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "Emitters in high refractive index materials like 4H-SiC suffer from reduced detection of photons because of losses caused by total internal reflection. Thus, integration into efficient nanophotonic structures which couple the emission of photons to a well defined waveguide mode can significantly enhance the photon detection efficiency. In addition, interfacing this waveguide to a classical fiber network is of similar importance to detect the photons and perform experiments. Here, we show a waveguide fiber interface in SiC. By careful measurements we determine efficiencies exceeding 93 % for the transfer of photons from SiC nanobeams to fibers. We use this interface to create a bright single photon source based on waveguide integrated V2 defects in 4H-SiC and achieve an overall photon count rate of 181 kilo-counts per second. We observe and quantify the strain induced shift of the ground state spin states and demonstrate coherent control of the electron spin with a coherence time of T2=42.5 $\\rm\\mu$s.",
        "citation_title": "Precise characterization of a silicon carbide waveguide fiber interface",
        "date_delivered": "[Submitted on 11 Jan 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "For synchrotron light sources, the brightness of user X-ray beams is primarily determined by the electron beam emittance and energy spread at operational intensity. A common feature of fourth-generation synchrotrons is the short length of electron bunches combined with a very small transverse beam size. Consequently, the particle density is much higher than in machines of previous generations, leading to strong collective effects that significantly increase the emittance and limit the achievable brightness at operational beam intensity. In this article, we summarize our studies of the emittance scaled with the beam energy and intensity, taking into account the effects of intrabeam scattering, beam-impedance interaction, and bunch lengthening provided by higher-harmonic RF systems, to identify optimal combinations of machine and beam parameters.",
        "citation_title": "Electron beam emittance at operational intensity in fourth-generation synchrotron light sources",
        "date_delivered": "[Submitted on 7 Feb 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We review models for unsteady porous media flow in the volume-averaging framework and we discuss the theoretical relations between the models and the definition of the model coefficients (and the uncertainty therein). The different models are compared against direct numerical simulations of oscillatory flow through a hexagonal sphere pack. The model constants are determined based on their definition in terms of the Stokes flow, the potential flow and steady nonlinear flow. Thus, the discrepancies between the model predictions and the simulation data can be attributed to shortcomings of the models' parametrisation.\nWe found that an extension of the dynamic permeability model of Pride et al. [Physical Review B 47(9), 1993] with a Forchheimer-type nonlinearity performs very well for linear flow and for nonlinear flow at low and medium frequencies, but the Forchheimer term with a coefficient obtained from the steady-state overpredicts the nonlinear drag at high frequencies. The model reduces to the unsteady Forchheimer equation with an acceleration coefficient based on the static viscous tortuosity for low frequencies.\nThe unsteady Forchheimer equation with an acceleration coefficient based on the high frequency limit of the dynamic tortuosity has large errors for linear flow at medium and high frequencies, but low errors for nonlinear flow at all frequencies. This is explained by an error cancellation between the inertial and the nonlinear drag.",
        "citation_title": "Assessment of models for nonlinear oscillatory flow through a hexagonal sphere pack",
        "date_delivered": "[Submitted on 7 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present an innovative cluster-based method employing linear combinations of diverse cluster mean-field (cMF) states, and apply it to describe the ground state of strongly-correlated spin systems. In cluster mean-field theory, the ground state wavefunction is expressed as a factorized tensor product of optimized cluster states. While our prior work concentrated on a single cMF tiling, this study removes that constraint by combining different tilings of cMF states. Selection criteria, including translational symmetry and spatial proximity, guide this process. We present benchmark calculations for the one- and two-dimensional $J_1-J_2$ and $XXZ$ Heisenberg models. Our findings highlight two key aspects. First, the method offers a semi-quantitative description of the $0.4 \\lessapprox J_2/J_1 \\lessapprox 0.6$ regime of the $J_1-J_2$ model - a particularly challenging regime for existing methods. Second, our results demonstrate the capability of our method to provide qualitative descriptions for all the models and regimes considered, establishing it as a valuable reference. However, the inclusion of additional (weak) correlations is necessary for quantitative agreement, and we explore methods to incorporate these extra correlations.",
        "citation_title": "Linear combinations of cluster mean-field states applied to spin systems",
        "date_delivered": "[Submitted on 9 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The multi-reference coupled-cluster Monte Carlo (MR-CCMC) algorithm is a determinant-based quantum Monte Carlo (QMC) algorithm that is conceptually similar to Full Configuration Interaction QMC (FCIQMC). It has been shown to offer a balanced treatment of both static and dynamic correlation while retaining polynomial scaling, although application to large systems with significant strong correlation remained impractical. In this paper, we document recent algorithmic advances that enable rapid convergence and a more black-box approach to the multi-reference problem. These include a logarithmically scaling metric-tree based excitation acceptance algorithm to search for determinants connected to the reference space at the desired excitation level and a symmetry-screening procedure for the reference space. We show that, for moderately sized reference spaces, the new search algorithm brings about an approximately 8-fold acceleration of one MR-CCMC iteration, while the symmetry screening procedure reduces the number of active reference space determinants at essentially no loss of accuracy. We also introduce a stochastic implementation of an approximate wall projector, which is the infinite imaginary time limit of the exponential projector, using a truncated expansion of the wall function in Chebyshev polynomials. Notably, this wall-Chebyshev projector can be used to accelerate any projector-based QMC algorithm. We show that it requires significantly fewer applications of the Hamiltonian to achieve the same statistical convergence. We benchmark these acceleration methods on the beryllium and carbon dimers, using initiator FCIQMC and MR-CCMC with basis sets up to cc-pVQZ quality.",
        "citation_title": "Rapidly convergent quantum Monte Carlo using a Chebyshev projector",
        "date_delivered": "[Submitted on 26 Feb 2024 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Nonlinear quantum photonics serves as a cornerstone in photonic quantum technologies, such as universal quantum computing and quantum communications. The emergence of integrated photonics platform not only offers the advantage of large-scale manufacturing but also provides a variety of engineering methods. Given the complexity of integrated photonics engineering, a comprehensive simulation framework is essential to fully harness the potential of the platform. In this context, we introduce a nonlinear quantum photonics simulation framework which can accurately model a variety of features such as adiabatic waveguide, material anisotropy, linear optics components, photon losses, and detectors. Furthermore, utilizing the framework, we have developed a device scheme, chip-scale temporal walk-off compensation, that is useful for various quantum information processing tasks. Applying the simulation framework, we show that the proposed device scheme can enhance the squeezing parameter of photon-pair sources and the conversion efficiency of quantum frequency converters without relying on higher pump power.",
        "citation_title": "Simulation framework for integrated nonlinear quantum photonics",
        "date_delivered": "[Submitted on 29 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Shear Alfven wave parametric decay instability (PDI) provides a potential path toward significant wave dissipation and plasma heating. However, fundamental questions regarding how PDI is excited in a realistic three-dimensional (3D) open system and how critically the finite perpendicular wave scale--as found in both laboratory and space plasmas--affects the excitation remain poorly understood. Here, we present the first 3D, open-boundary, hybrid kinetic-fluid simulations of kinetic Alfven wave PDI in low-beta plasmas. Key findings are that the PDI excitation is strongly limited by the wave damping present, including electron-ion collisional damping (represented by a constant resistivity) and geometrical attenuation associated with the finite-scale Alfven wave, and ion Landau damping of the child acoustic wave. The perpendicular wave scale alone, however, plays no discernible role: waves of different perpendicular scales exhibit similar instability growth as long as the magnitude of the parallel ponderomotive force remains unchanged. These findings are corroborated by theoretical analysis and estimates. The new understanding of 3D kinetic Alfv\u00e9n wave PDI physics is essential for laboratory study of the basic plasma process and may also help evaluate the relevance/role of PDI in low-beta space plasmas.",
        "citation_title": "Effects of wave damping and finite perpendicular scale on three-dimensional Alfven wave parametric decay in low-beta plasmas",
        "date_delivered": "[Submitted on 13 Mar 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "For transportation hubs, leveraging pedestrian flows for commercial activities presents an effective strategy for funding maintenance and infrastructure improvements. However, this introduces new challenges, as consumer behaviors can disrupt pedestrian flow and efficiency. To optimize both retail potential and pedestrian efficiency, careful strategic planning in store layout and facility dimensions was done by expert judgement due to the complexity in pedestrian dynamics in the retail areas of transportation hubs. This paper introduces an attention-based movement model to simulate these dynamics. By simulating retail potential of an area through the duration of visual attention it receives, and pedestrian efficiency via speed loss in pedestrian walking behaviors, the study further explores how design features can influence the retail potential and pedestrian efficiency in a bi-directional corridor inside a transportation hub. Project webpage: this https URL",
        "citation_title": "Microscopic modeling of attention-based movement behaviors",
        "date_delivered": "[Submitted on 22 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We establish the concept of topological pumping in one-dimensional systems with long-range interactions and apply it to the transport of a photon in quantum optical systems. In our theoretical investigation, we introduce an extended version of the Rice-Mele model with all-to-all exchange interactions. By analyzing its properties, we identify the general conditions for topological pumping and demonstrate the topologically protected and dispersionless transport of a photon on a one-dimensional emitter chain. As concrete examples, we investigate three different popular quantum optics platforms, namely Rydberg atom lattices, dense lattices of atoms excited to low-lying electronic states, and atoms coupled to waveguides, using experimentally relevant parameters. We observe that despite the long-ranged character of the dipole-dipole interactions, topological pumping facilitates the transport of a photon with a fidelity per cycle which can reach 99.9%. Moreover, we find that the photon pumping process remains topologically protected against local disorder in the coupling rates.",
        "citation_title": "Topological photon pumping in quantum optical systems",
        "date_delivered": "[Submitted on 8 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Numerically simulating magnetohydrodynamics (MHD) poses notable challenges, including the suppression of spurious oscillations near discontinuities (e.g., shocks) and preservation of essential physical structures (e.g., the divergence-free constraint of magnetic field and the positivity of density and pressure). This paper develops structure-preserving oscillation-eliminating discontinuous Galerkin (OEDG) schemes for ideal MHD. The schemes leverage a locally divergence-free (LDF) oscillation-eliminating (OE) procedure to suppress spurious oscillations while retaining the LDF property of magnetic field and many desirable attributes of original DG schemes, such as conservation, local compactness, and optimal convergence rates. The OE procedure is based on the solution operator of a novel damping equation, a linear system of ordinary differential equations that are exactly solvable without any discretization. The OE procedure is performed after each Runge-Kutta stage and does not impact DG spatial discretization, facilitating its easy integration into existing DG codes as an independent module. Moreover, this paper presents a rigorous positivity-preserving (PP) analysis of the LDF OEDG schemes on Cartesian meshes, utilizing the optimal convex decomposition technique and the geometric quasi-linearization (GQL) approach. Efficient PP LDF OEDG schemes are derived by incorporating appropriate discretization of Godunov-Powell source terms into only the discrete equations of cell averages, under a condition achievable through a simple PP limiter. Several one- and two-dimensional MHD tests verify the accuracy, effectiveness, and robustness of the proposed structure-preserving OEDG schemes.",
        "citation_title": "Structure-Preserving Oscillation-Eliminating Discontinuous Galerkin Schemes for Ideal MHD Equations: Locally Divergence-Free and Positivity-Preserving",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We developed a general framework for hybrid quantum-classical computing of molecular and periodic embedding approaches based on an orbital space separation of the fragment and environment degrees of freedom. We demonstrate its potential by presenting a specific implementation of periodic range-separated DFT coupled to a quantum circuit ansatz, whereby the variational quantum eigensolver and the quantum equation-of-motion algorithm are used to obtain the low-lying spectrum of the embedded fragment Hamiltonian. Application of this scheme to study localized electronic states in materials is showcased through the accurate prediction of the optical properties of the neutral oxygen vacancy in magnesium oxide (MgO). Despite some discrepancies in the position of the main absorption band, the method demonstrates competitive performance compared to state-of-the-art ab initio approaches, particularly evidenced by the excellent agreement with the experimental photoluminescence emission peak.",
        "citation_title": "A general framework for active space embedding methods: applications in quantum computing",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Chiral magnets are materials which possess unique helical arrangements of magnetic moments, which give rise to nonreciprocal transport and fascinating physics phenomena. On the one hand, their exploration is guided by the prospects of unconventional signal processing, computation schemes and magnetic memory. On the other hand, progress in applications is hindered by the challenging materials synthesis, limited scalability and typically low critical temperature. Here, we report the creation and exploration of artificial chiral magnets (ACMs) at room temperature. By employing a mass production compatible deposition technology, we synthesize ACMs, which consist of helical Ni surfaces on central cylinders. Using optical microscopy, we reveal nonreciprocal magnon transport at GHz frequencies. It is controlled by programmable toroidal moments which result from the ACM's geometrical handedness and field-dependent spin chirality. We present materials-by-design rules which optimize the helically curved ferromagnets for 3D nonreciprocal transport at room temperature and zero magnetic field.",
        "citation_title": "Room temperature realization of artificial chiral magnets with reprogrammable magnon nonreciprocity at zero field",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Background: X-ray grating-based dark-field imaging can sense the small angle scattering caused by an object's micro-structure. This technique is sensitive to lung's porous alveoli and is able to detect lung disease at an early stage. Up to now, a human-scale dark-field CT has been built for lung imaging. Purpose: This study aimed to develop a more thorough optimization method for dark-field lung CT and summarize principles for system design. Methods: We proposed a metric in the form of contrast-to-noise ratio (CNR) for system parameter optimization, and designed a phantom with concentric circle shape to fit the task of lung disease detection. Finally, we developed the calculation method of the CNR metric, and analyzed the relation between CNR and system parameters. Results: We showed that with other parameters held constant, the CNR first increases and then decreases with the system auto-correlation length (ACL). The optimal ACL is nearly not influenced by system's visibility, and is only related to phantom's property, i.e., scattering material's size and phantom's absorption. For our phantom, the optimal ACL is about 0.21 {\\mu}m. As for system geometry, larger source-detector and isocenter-detector distance can increase the system's maximal ACL, helping the system meet the optimal ACL more easily. Conclusions: This study proposed a more reasonable metric and a task-based process for optimization, and demonstrated that the system optimal ACL is only related to the phantom's property.",
        "citation_title": "Optimization of Dark-Field CT for Lung Imaging",
        "date_delivered": "[Submitted on 1 May 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Stretching an elastic material along one axis typically induces contraction along the transverse axes, a phenomenon known as the Poisson effect. From these strains, one can compute the specific volume, which generally either increases or, in the incompressible limit, remains constant as the material is stretched. However, in networks of semiflexible or stiff polymers, which are typically highly compressible yet stiffen significantly when stretched, one instead sees a significant reduction in specific volume under finite strains. This volume reduction is accompanied by increasing alignment of filaments along the strain axis and a nonlinear elastic response, with stiffening of the apparent Young's modulus. For semiflexible networks, in which entropic bending elasticity governs the linear elastic regime, the nonlinear Poisson effect is caused by the nonlinear force-extension relationship of the constituent filaments, which produces a highly asymmetric response of the constituent polymers to stretching and compression. The details of this relationship depend on the geometric and elastic properties of the underlying filaments, which can vary greatly in experimental systems. Here, we provide a comprehensive characterization of the nonlinear Poisson effect in an affine network model and explore the influence of filament properties on essential features of the macroscopic response, including strain-driven alignment and volume reduction.",
        "citation_title": "Nonlinear Poisson effect in affine semiflexible polymer networks",
        "date_delivered": "[Submitted on 1 May 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "This study investigates the impact of three cold plasma treatments on barley seed germination: direct treatment of dry seeds (DDS), direct treatment of water-soaked seeds (DWS), and indirect treatment of seeds using plasma-activated water (IPAW).",
        "citation_title": "Cold plasma treatment boosts barley germination and seedling vigor: Insights into soluble sugar, starch, and protein modifications",
        "date_delivered": "[Submitted on 18 Apr 2024]"
    },
    {
        "abstract": "Molecular dynamics (MD) is a crucial technique for simulating biological systems, enabling the exploration of their dynamic nature and fostering an understanding of their functions and properties. To address exploration inefficiency, emerging enhanced sampling approaches like coarse-graining (CG) and generative models have been employed. In this work, we propose a \\underline{Frame-to-Frame} generative model with guided \\underline{Flow}-matching (F$3$low) for enhanced sampling, which (a) extends the domain of CG modeling to the SE(3) Riemannian manifold; (b) retreating CGMD simulations as autoregressively sampling guided by the former frame via flow-matching models; (c) targets the protein backbone, offering improved insights into secondary structure formation and intricate folding pathways. Compared to previous methods, F$3$low allows for broader exploration of conformational space. The ability to rapidly generate diverse conformations via force-free generative paradigm on SE(3) paves the way toward efficient enhanced sampling methods.",
        "citation_title": "F$^3$low: Frame-to-Frame Coarse-grained Molecular Dynamics with SE(3) Guided Flow Matching",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Antimicrobial peptides (AMPs) have exhibited unprecedented potential as biomaterials in combating multidrug-resistant bacteria. Despite the increasing adoption of artificial intelligence for novel AMP design, challenges pertaining to conflicting attributes such as activity, hemolysis, and toxicity have significantly impeded the progress of researchers. This paper introduces a paradigm shift by considering multiple attributes in AMP design.\nPresented herein is a novel approach termed Hypervolume-driven Multi-objective Antimicrobial Peptide Design (HMAMP), which prioritizes the simultaneous optimization of multiple attributes of AMPs. By synergizing reinforcement learning and a gradient descent algorithm rooted in the hypervolume maximization concept, HMAMP effectively expands exploration space and mitigates the issue of pattern collapse. This method generates a wide array of prospective AMP candidates that strike a balance among diverse attributes. Furthermore, we pinpoint knee points along the Pareto front of these candidate AMPs. Empirical results across five benchmark models substantiate that HMAMP-designed AMPs exhibit competitive performance and heightened diversity. A detailed analysis of the helical structures and molecular dynamics simulations for ten potential candidate AMPs validates the superiority of HMAMP in the realm of multi-objective AMP design. The ability of HMAMP to systematically craft AMPs considering multiple attributes marks a pioneering milestone, establishing a universal computational framework for the multi-objective design of AMPs.",
        "citation_title": "HMAMP: Hypervolume-Driven Multi-Objective Antimicrobial Peptides Design",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper explores some basic concepts of Biochemical Systems Theory (BST) and Metabolic Control Analysis (MCA), two frameworks developed to understand the behavior of biochemical networks. Initially introduced by Savageau, BST focuses on system stability and employs power laws in modeling biochemical systems. On the other hand, MCA, pioneered by authors such as Kacser and Burns and Heinrich and Rapoport, emphasizes linearization of the governing equations and describes relationships (known as theorems) between different measures. Despite apparent differences, both frameworks are shown to be equivalent in many respects. Through a simple example of a linear chain, the paper demonstrates how BST and MCA yield identical results when analyzing steady-state behavior and logarithmic gains within biochemical pathways. This comparative analysis highlights the interchangeability of concepts such as kinetic orders, elasticities and other logarithmic gains.",
        "citation_title": "A Simple Comparison of Biochemical Systems Theory and Metabolic Control Analysis",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "It is now possible to conduct large scale perturbation screens with complex readout modalities, such as different molecular profiles or high content cell images. While these open the way for systematic dissection of causal cell circuits, integrated such data across screens to maximize our ability to predict circuits poses substantial computational challenges, which have not been addressed. Here, we extend two Gromov-Wasserstein Optimal Transport methods to incorporate the perturbation label for cross-modality alignment. The obtained alignment is then employed to train a predictive model that estimates cellular responses to perturbations observed with only one measurement modality. We validate our method for the tasks of cross-modality alignment and cross-modality prediction in a recent multi-modal single-cell perturbation dataset. Our approach opens the way to unified causal models of cell biology.",
        "citation_title": "Cross-modality Matching and Prediction of Perturbation Responses with Labeled Gromov-Wasserstein Optimal Transport",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Centred Kernel Alignment (CKA) has recently emerged as a popular metric to compare activations from biological and artificial neural networks (ANNs) in order to quantify the alignment between internal representations derived from stimuli sets (e.g. images, text, video) that are presented to both systems. In this paper we highlight issues that the community should take into account if using CKA as an alignment metric with neural data. Neural data are in the low-data high-dimensionality domain, which is one of the cases where (biased) CKA results in high similarity scores even for pairs of random matrices. Using fMRI and MEG data from the THINGS project, we show that if biased CKA is applied to representations of different sizes in the low-data high-dimensionality domain, they are not directly comparable due to biased CKA's sensitivity to differing feature-sample ratios and not stimuli-driven responses. This situation can arise both when comparing a pre-selected area of interest (e.g. ROI) to multiple ANN layers, as well as when determining to which ANN layer multiple regions of interest (ROIs) / sensor groups of different dimensionality are most similar. We show that biased CKA can be artificially driven to its maximum value when using independent random data of different sample-feature ratios. We further show that shuffling sample-feature pairs of real neural data does not drastically alter biased CKA similarity in comparison to unshuffled data, indicating an undesirable lack of sensitivity to stimuli-driven neural responses. Positive alignment of true stimuli-driven responses is only achieved by using debiased CKA. Lastly, we report findings that suggest biased CKA is sensitive to the inherent structure of neural data, only differing from shuffled data when debiased CKA detects stimuli-driven alignment.",
        "citation_title": "Correcting Biased Centered Kernel Alignment Measures in Biological and Artificial Neural Networks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This work explores the hypothesis that subjectively attributed meaning constitutes the phenomenal content of conscious experience. That is, phenomenal content is semantic. This form of subjective meaning manifests as an intrinsic and non-representational character of qualia. Empirically, subjective meaning is ubiquitous in conscious experiences. We point to phenomenological studies that lend evidence to support this. Furthermore, this notion of meaning closely relates to what Frege refers to as \"sense\", in metaphysics and philosophy of language. It also aligns with Peirce's \"interpretant\", in semiotics. We discuss how Frege's sense can also be extended to the raw feels of consciousness. Sense and reference both play a role in phenomenal experience. Moreover, within the context of the mind-matter relation, we provide a formalization of subjective meaning associated to one's mental representations. Identifying the precise maps between the physical and mental domains, we argue that syntactic and semantic structures transcend language, and are realized within each of these domains. Formally, meaning is a relational attribute, realized via a map that interprets syntactic structures of a formal system within an appropriate semantic space. The image of this map within the mental domain is what is relevant for experience, and thus comprises the phenomenal content of qualia. We conclude with possible implications this may have for experience-based theories of consciousness.",
        "citation_title": "Qualia and the Formal Structure of Meaning",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Anti-seizure medications (ASMs) are the primary treatment for epilepsy, yet medication tapering effects have not been investigated in a dose, region, and time-dependent manner, despite their potential impact on research and clinical practice.\nWe examined over 3000 hours of intracranial EEG recordings in 32 subjects during long-term monitoring, of which 22 underwent concurrent ASM tapering. We estimated ASM plasma levels based on known pharmaco-kinetics of all the major ASM types.\nWe found an overall decrease in the power of delta band activity around the period of maximum medication withdrawal in most (80%) subjects, independent of their epilepsy type or medication combination. The degree of withdrawal correlated positively with the magnitude of delta power decrease. This dose-dependent effect was strongly seen across all recorded cortical regions during daytime; but not in sub-cortical regions, or during night time. We found no evidence of differential effect in seizure onset, spiking, or pathological brain regions.\nThe finding of decreased delta band power during ASM tapering agrees with previous literature. Our observed dose-dependent effect indicates that monitoring ASM levels in cortical regions may be feasible for applications such as medication reminder systems, or closed-loop ASM delivery systems. ASMs are also used in other neurological and psychiatric conditions, making our findings relevant to a general neuroscience and neurology audience.",
        "citation_title": "Anti-seizure medication tapering is associated with delta band power reduction in a dose, region and time-dependent manner",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Backpropagation-optimized artificial neural networks, while precise, lack robustness, leading to unforeseen behaviors that affect their safety. Biological neural systems do solve some of these issues already. Thus, understanding the biological mechanisms of robustness is an important step towards building trustworthy and safe systems. Unlike artificial models, biological neurons adjust connectivity based on neighboring cell activity. Robustness in neural representations is hypothesized to correlate with the smoothness of the encoding manifold. Recent work suggests power law covariance spectra, which were observed studying the primary visual cortex of mice, to be indicative of a balanced trade-off between accuracy and robustness in representations. Here, we show that unsupervised local learning models with winner takes all dynamics learn such power law representations, providing upcoming studies a mechanistic model with that characteristic. Our research aims to understand the interplay between geometry, spectral properties, robustness, and expressivity in neural representations. Hence, we study the link between representation smoothness and spectrum by using weight, Jacobian and spectral regularization while assessing performance and adversarial robustness. Our work serves as a foundation for future research into the mechanisms underlying power law spectra and optimally smooth encodings in both biological and artificial systems. The insights gained may elucidate the mechanisms that realize robust neural networks in mammalian brains and inform the development of more stable and reliable artificial systems.",
        "citation_title": "Exploring mechanisms of Neural Robustness: probing the bridge between geometry and spectrum",
        "date_delivered": "[Submitted on 5 Feb 2024]"
    },
    {
        "abstract": "Effectively learning the temporal dynamics in electroencephalogram (EEG) signals is challenging yet essential for decoding brain activities using brain-computer interfaces (BCIs). Although Transformers are popular for their long-term sequential learning ability in the BCI field, most methods combining Transformers with convolutional neural networks (CNNs) fail to capture the coarse-to-fine temporal dynamics of EEG signals. To overcome this limitation, we introduce EEG-Deformer, which incorporates two main novel components into a CNN-Transformer: (1) a Hierarchical Coarse-to-Fine Transformer (HCT) block that integrates a Fine-grained Temporal Learning (FTL) branch into Transformers, effectively discerning coarse-to-fine temporal patterns; and (2) a Dense Information Purification (DIP) module, which utilizes multi-level, purified temporal information to enhance decoding accuracy. Comprehensive experiments on three representative cognitive tasks consistently verify the generalizability of our proposed EEG-Deformer, demonstrating that it either outperforms existing state-of-the-art methods or is comparable to them. Visualization results show that EEG-Deformer learns from neurophysiologically meaningful brain regions for the corresponding cognitive tasks. The source code can be found at this https URL.",
        "citation_title": "EEG-Deformer: A Dense Convolutional Transformer for Brain-computer Interfaces",
        "date_delivered": "[Submitted on 25 Apr 2024]"
    },
    {
        "abstract": "Quantification of chaos is a challenging issue in complex dynamical systems. In this paper, we discuss the chaotic properties of generalized Lotka-Volterra and May-Leonard models of biodiversity, via the Hamming distance density. We identified chaotic behavior for different scenarios via the specific features of the Hamming distance and the method of q-exponential fitting. We also investigated the spatial autocorrelation length to find the corresponding characteristic length in terms of the number of species in each system. In particular, the results concerning the characteristic length are in good accordance with the study of the chaotic behavior implemented in this work.",
        "citation_title": "Chaotic behavior in Lotka-Volterra and May-Leonard models of biodiversity",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Recent advancements in nanopore sequencing technology, particularly the R10 nanopore from Oxford Nanopore Technology, have necessitated the development of improved data processing methods to utilize their potential for more than 9-mer resolution fully. The processing of the ion currents predominantly utilizes neural network-based methods known for their high basecalling accuracy but face developmental bottlenecks at higher resolutions. In light of this, we introduce the Helicase Hidden Markov Model (HHMM), a novel framework designed to incorporate the dynamics of the helicase motor protein alongside the nucleotide sequence during nanopore sequencing. This model supports the analysis of millions of distinct states, enhancing our understanding of raw ion currents and their alignment with nucleotide sequences. Our findings demonstrate the utility of HHMM not only as a potent visualization tool but also as an effective base for developing advanced basecalling algorithms. This approach offers a promising avenue for leveraging the full capabilities of emerging high-resolution nanopore sequencing technologies.",
        "citation_title": "Modelling the nanopore sequencing process with Helicase HMMs",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Large language models appear quite creative, often performing on par with the average human on creative tasks. However, research on LLM creativity has focused solely on \\textit{products}, with little attention on the creative \\textit{process}. Process analyses of human creativity often require hand-coded categories or exploit response times, which do not apply to LLMs. We provide an automated method to characterise how humans and LLMs explore semantic spaces on the Alternate Uses Task, and contrast with behaviour in a Verbal Fluency Task. We use sentence embeddings to identify response categories and compute semantic similarities, which we use to generate jump profiles. Our results corroborate earlier work in humans reporting both persistent (deep search in few semantic spaces) and flexible (broad search across multiple semantic spaces) pathways to creativity, where both pathways lead to similar creativity scores. LLMs were found to be biased towards either persistent or flexible paths, that varied across tasks. Though LLMs as a population match human profiles, their relationship with creativity is different, where the more flexible models score higher on creativity. Our dataset and scripts are available on \\href{this https URL}{GitHub}.",
        "citation_title": "Characterising the Creative Process in Humans and Large Language Models",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This study introduces a systematic framework to compare the efficacy of Large Language Models (LLMs) for fine-tuning across various cheminformatics tasks. Employing a uniform training methodology, we assessed three well-known models-RoBERTa, BART, and LLaMA-on their ability to predict molecular properties using the Simplified Molecular Input Line Entry System (SMILES) as a universal molecular representation format. Our comparative analysis involved pre-training 18 configurations of these models, with varying parameter sizes and dataset scales, followed by fine-tuning them on six benchmarking tasks from DeepChem. We maintained consistent training environments across models to ensure reliable comparisons. This approach allowed us to assess the influence of model type, size, and training dataset size on model performance. Specifically, we found that LLaMA-based models generally offered the lowest validation loss, suggesting their superior adaptability across tasks and scales. However, we observed that absolute validation loss is not a definitive indicator of model performance - contradicts previous research - at least for fine-tuning tasks: instead, model size plays a crucial role. Through rigorous replication and validation, involving multiple training and fine-tuning cycles, our study not only delineates the strengths and limitations of each model type but also provides a robust methodology for selecting the most suitable LLM for specific cheminformatics applications. This research underscores the importance of considering model architecture and dataset characteristics in deploying AI for molecular property prediction, paving the way for more informed and effective utilization of AI in drug discovery and related fields.",
        "citation_title": "The Role of Model Architecture and Scale in Predicting Molecular Properties: Insights from Fine-Tuning RoBERTa, BART, and LLaMA",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Large language models (LLMs) have shown remarkable potential in various domains, but they often lack the ability to access and reason over domain-specific knowledge and tools. In this paper, we introduced CACTUS (Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that integrates cheminformatics tools to enable advanced reasoning and problem-solving in chemistry and molecular discovery. We evaluate the performance of CACTUS using a diverse set of open-source LLMs, including Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of thousands of chemistry questions. Our results demonstrate that CACTUS significantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b models achieving the highest accuracy regardless of the prompting strategy used. Moreover, we explore the impact of domain-specific prompting and hardware configurations on model performance, highlighting the importance of prompt engineering and the potential for deploying smaller models on consumer-grade hardware without significant loss in accuracy. By combining the cognitive capabilities of open-source LLMs with domain-specific tools, CACTUS can assist researchers in tasks such as molecular property prediction, similarity searching, and drug-likeness assessment. Furthermore, CACTUS represents a significant milestone in the field of cheminformatics, offering an adaptable tool for researchers engaged in chemistry and molecular discovery. By integrating the strengths of open-source LLMs with domain-specific tools, CACTUS has the potential to accelerate scientific advancement and unlock new frontiers in the exploration of novel, effective, and safe therapeutic candidates, catalysts, and materials. Moreover, CACTUS's ability to integrate with automated experimentation platforms and make data-driven decisions in real time opens up new possibilities for autonomous discovery.",
        "citation_title": "CACTUS: Chemistry Agent Connecting Tool-Usage to Science",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on $L_{1}$ regularization combined with cross-validation. As we demonstrate, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight \"shrinkage\". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.",
        "citation_title": "Network reconstruction via the minimum description length principle",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Multicellular tumor spheroids (MCTS) are advanced cell culture systems for assessing the impact of combinatorial radio(chemo)therapy. They exhibit therapeutically relevant in-vivo-like characteristics from 3D cell-cell and cell-matrix interactions to radial pathophysiological gradients related to proliferative activity and nutrient/oxygen supply, altering cellular radioresponse. State-of-the-art assays quantify long-term curative endpoints based on collected brightfield image time series from large treated spheroid populations per irradiation dose and treatment arm. Here, spheroid control probabilities are documented analogous to in-vivo tumor control probabilities based on Kaplan-Meier curves. This analyses require laborious spheroid segmentation of up to 100.000 images per treatment arm to extract relevant structural information from the images, e.g., diameter, area, volume and circularity. While several image analysis algorithms are available for spheroid segmentation, they all focus on compact MCTS with clearly distinguishable outer rim throughout growth. However, treated MCTS may partly be detached and destroyed and are usually obscured by dead cell debris. We successfully train two Fully Convolutional Networks, UNet and HRNet, and optimize their hyperparameters to develop an automatic segmentation for both untreated and treated MCTS. We systematically validate the automatic segmentation on larger, independent data sets of spheroids derived from two human head-and-neck cancer cell lines. We find an excellent overlap between manual and automatic segmentation for most images, quantified by Jaccard indices at around 90%. For images with smaller overlap of the segmentations, we demonstrate that this error is comparable to the variations across segmentations from different biological experts, suggesting that these images represent biologically unclear or ambiguous cases.",
        "citation_title": "Image segmentation of treated and untreated tumor spheroids by Fully Convolutional Networks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recent breakthroughs in generative modelling have led to a number of works proposing molecular generation models for drug discovery. While these models perform well at capturing drug-like motifs, they are known to often produce synthetically inaccessible molecules. This is because they are trained to compose atoms or fragments in a way that approximates the training distribution, but they are not explicitly aware of the synthesis constraints that come with making molecules in the lab. To address this issue, we introduce SynFlowNet, a GFlowNet model whose action space uses chemically validated reactions and reactants to sequentially build new molecules. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool. SynFlowNet consistently samples synthetically feasible molecules, while still being able to find diverse and high-utility candidates. Furthermore, we compare molecules designed with SynFlowNet to experimentally validated actives, and find that they show comparable properties of interest, such as molecular weight, SA score and predicted protein binding affinity.",
        "citation_title": "SynFlowNet: Towards Molecule Design with Guaranteed Synthesis Pathways",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Feedback-evolving games is a framework that models the co-evolution between payoff functions and an environmental state. It serves as a useful tool to analyze many social dilemmas such as natural resource consumption, behaviors in epidemics, and the evolution of biological populations. However, it has primarily focused on the dynamics of a single population of agents. In this paper, we consider the impact of two populations of agents that share a common environmental resource. We focus on a scenario where individuals in one population are governed by an environmentally \"responsible\" incentive policy, and individuals in the other population are environmentally \"irresponsible\". An analysis on the asymptotic stability of the coupled system is provided, and conditions for which the resource collapses are identified. We then derive consumption rates for the irresponsible population that optimally exploit the environmental resource, and analyze how incentives should be allocated to the responsible population that most effectively promote the environment via a sensitivity analysis.",
        "citation_title": "Two competing populations with a common environmental resource",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider a population constituted by two types of individuals; each of them can produce offspring in two different islands (as a particular case the islands can be interpreted as active or dormant individuals). We model the evolution of the population of each type using a two-type Feller diffusion with immigration, and we study the frequency of one of the types, in each island, when the total population size in each island is forced to be constant at a dense set of times. This leads to the solution of a SDE which we call the asymmetric two-island frequency process. We derive properties of this process and obtain a large population limit when the total size of each island tends to infinity. Additionally, we compute the fluctuations of the process around its deterministic limit. We establish conditions under which the asymmetric two-island frequency process has a moment dual. The dual is a continuous-time two-dimensional Markov chain that can be interpreted in terms of mutation, branching, pairwise branching, coalescence, and a novel mixed selection-migration term. Also, we conduct a stability analysis of the limiting deterministic dynamical system and present some numerical results to study fixation and a new form of balancing selection. When restricting to the seedbank model, we observe that some combinations of the parameters lead to balancing selection. Besides finding yet another way in which genetic reservoirs increase the genetic variability, we find that if a population that sustains a seedbank competes with one that does not, the seed producers will have a selective advantage if they reproduce faster, but will not have a selective disadvantage if they reproduce slower: their worst case scenario is balancing selection.",
        "citation_title": "Two-type branching processes with immigration, and the structured coalescents",
        "date_delivered": "[Submitted on 16 Sep 2021 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Summary: Raw nanopore signals can be analyzed while they are being generated, a process known as real-time analysis. Real-time analysis of raw signals is essential to utilize the unique features that nanopore sequencing provides, enabling the early stopping of the sequencing of a read or the entire sequencing run based on the analysis. The state-of-the-art mechanism, RawHash, offers the first hash-based efficient and accurate similarity identification between raw signals and a reference genome by quickly matching their hash values. In this work, we introduce RawHash2, which provides major improvements over RawHash, including a more sensitive quantization and chaining implementation, weighted mapping decisions, frequency filters to reduce ambiguous seed hits, minimizers for hash-based sketching, and support for the R10.4 flow cell version and various data formats such as POD5 and SLOW5. Compared to RawHash, RawHash2 provides better F1 accuracy (on average by 10.57% and up to 20.25%) and better throughput (on average by 4.0x and up to 9.9x) than RawHash. Availability and Implementation: RawHash2 is available at this https URL. We also provide the scripts to fully reproduce our results on our GitHub page.",
        "citation_title": "RawHash2: Mapping Raw Nanopore Signals Using Hash-Based Seeding and Adaptive Quantization",
        "date_delivered": "[Submitted on 11 Sep 2023 (v1), last revised 1 May 2024 (this version, v4)]"
    },
    {
        "abstract": "Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of visual cortex functionality. Moving beyond conventional approaches, we introduce a data-driven method that generates natural language descriptions for images predicted to maximally activate individual voxels of interest. Our method -- Semantic Captioning Using Brain Alignments (\"BrainSCUBA\") -- builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high predicted activations. Finally, to demonstrate how our method enables scientific discovery, we perform exploratory investigations on the distribution of \"person\" representations in the brain, and discover fine-grained semantic selectivity in body-selective areas. Unlike earlier studies that decode text, our method derives voxel-wise captions of semantic selectivity. Our results show that BrainSCUBA is a promising means for understanding functional preferences in the brain, and provides motivation for further hypothesis-driven investigation of visual cortex.",
        "citation_title": "BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity",
        "date_delivered": "[Submitted on 6 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Cryo-EM is a powerful tool for understanding macromolecular structures, yet current methods for structure reconstruction are slow and computationally demanding. To accelerate research on pose estimation, we present CESPED, a new dataset specifically designed for Supervised Pose Estimation in Cryo-EM. Alongside CESPED, we provide a PyTorch package to simplify Cryo-EM data handling and model evaluation. We evaluated the performance of a baseline model, Image2Sphere, on CESPED, which showed promising results but also highlighted the need for further improvements. Additionally, we illustrate the potential of deep learning-based pose estimators to generalise across different samples, suggesting a promising path toward more efficient processing strategies. CESPED is available at this https URL.",
        "citation_title": "CESPED: a new benchmark for supervised particle pose estimation in Cryo-EM",
        "date_delivered": "[Submitted on 10 Nov 2023 (v1), last revised 2 May 2024 (this version, v5)]"
    },
    {
        "abstract": "The influence of natural image transformations on receptive field responses is crucial for modelling visual operations in computer vision and biological vision. In this regard, covariance properties with respect to geometric image transformations in the earliest layers of the visual hierarchy are essential for expressing robust image operations, and for formulating invariant visual operations at higher levels.\nThis paper defines and proves a set of joint covariance properties under compositions of spatial scaling transformations, spatial affine transformations, Galilean transformations and temporal scaling transformations, which make it possible to characterize how different types of image transformations interact with each other and the associated spatio-temporal receptive field responses. In this regard, we also extend the notion of scale-normalized derivatives to affine-normalized derivatives, to be able to obtain true affine-covariant properties of spatial derivatives, that are computed based on spatial smoothing with affine Gaussian kernels.\nThe derived relations show how the parameters of the receptive fields need to be transformed, in order to match the output from spatio-temporal receptive fields under composed spatio-temporal image transformations. As a side effect, the presented proof for the joint covariance property over the integrated combination of the different geometric image transformations also provides specific proofs for the individual transformation properties, which have not previously been fully reported in the literature.\nThe paper also presents an in-depth theoretical analysis of geometric interpretations of the derived covariance properties, as well as outlines a number of biological interpretations of these results.",
        "citation_title": "Joint covariance properties under geometric image transformations for spatio-temporal receptive fields according to the generalized Gaussian derivative model for visual receptive fields",
        "date_delivered": "[Submitted on 17 Nov 2023 (v1), last revised 2 May 2024 (this version, v5)]"
    },
    {
        "abstract": "Epilepsy is a prevalent neurological disorder that affects approximately 1% of the global population. Around 30-40% of patients do not respond to pharmacological treatment, leading to a significant negative impact on their quality of life. Closed-loop deep brain stimulation (DBS) is a promising treatment for individuals who do not respond to medical therapy. To achieve effective seizure control, algorithms play an important role in identifying relevant electrographic biomarkers from local field potentials (LFPs) to determine the optimal stimulation timing. In this regard, the detection and classification of events from ongoing brain activity, while achieving low power through computationally unexpensive implementations, represents a major challenge in the field. To address this challenge, we here present two lightweight algorithms, the ZdensityRODE and the AMPDE, for identifying relevant events from LFPs by utilizing semantic segmentation, which involves extracting different levels of information from the LFP and relevant events from it. The algorithms performance was validated against epileptiform activity induced by 4-minopyridine in mouse hippocampus-cortex (CTX) slices and recorded via microelectrode array, as a case study. The ZdensityRODE algorithm showcased a precision and recall of 93% for ictal event detection and 42% precision for interictal event detection, while the AMPDE algorithm attained a precision of 96% and recall of 90% for ictal event detection and 54% precision for interictal event detection. While initially trained specifically for detection of ictal activity, these algorithms can be fine-tuned for improved interictal detection, aiming at seizure prediction. Our results suggest that these algorithms can effectively capture epileptiform activity; their light weight opens new possibilities for real-time seizure detection and seizure prediction and control.",
        "citation_title": "Time series segmentation for recognition of epileptiform patterns recorded via Microelectrode Arrays in vitro",
        "date_delivered": "[Submitted on 12 Feb 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The seasonal human influenza virus undergoes rapid evolution, leading to significant changes in circulating viral strains from year to year. These changes are typically driven by adaptive mutations, particularly in the antigenic epitopes, the regions of the viral surface protein haemagglutinin targeted by human antibodies. Here we describe a consistent set of methods for data-driven predictive analysis of viral evolution. Our pipeline integrates four types of data: (1) sequence data of viral isolates collected on a worldwide scale, (2) epidemiological data on incidences, (3) antigenic characterization of circulating viruses, and (4) intrinsic viral phenotypes. From the combined analysis of these data, we obtain estimates of relative fitness for circulating strains and predictions of clade frequencies for periods of up to one year. Furthermore, we obtain comparative estimates of protection against future viral populations for candidate vaccine strains, providing a basis for pre-emptive vaccine strain selection. Continuously updated predictions obtained from the prediction pipeline for influenza and SARS-CoV-2 are available on the website https://previr.app.",
        "citation_title": "Concepts and methods for predicting viral evolution",
        "date_delivered": "[Submitted on 19 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "An endhered (end-adhered) pattern is a subset of arcs in matchings, such that the corresponding starting points are consecutive and the same holds for the ending points. Such patterns are in one-to-one correspondence with the permutations. We focus on the occurrence frequency of such patterns in matchings and real-world RNA structures with pseudoknots. We present combinatorial results related to the distribution and asymptotic behavior of the pattern 21, which corresponds to two consecutive stacked bonds frequently encountered in RNA, and the pattern 12, representing the archetypal minimal pseudoknot. We show that in matchings these two patterns are equidistributed, which is quite different from what we can find in real-world RNAs. We also examine the distribution of endhered patterns of size 3, showing how the patterns change under the transformation called endhered twist. Finally, we compute the distributions of endhered patterns of size 2 and 3 in real-world secondary RNA structures with pseudoknots and discuss possible outcomes of our study.",
        "citation_title": "Endhered patterns in matchings and RNA",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present an asymptotic analysis of a stochastic two-compartmental cell proliferation system with regulatory mechanisms. We model the system as a state-dependent birth and death process. Proliferation of hematopoietic stem cells (HSCs) is regulated by population density of HSC-derived clones and differentiation of HSC is regulated by population density of HSCs. By scaling up the initial population, we show the density of dynamics converges in distribution to the solution of a system of ordinary differential equations. The system of ODE has a unique non-trivial equilibrium that is globally stable. Furthermore, we show the scaled fluctuation of the population converges in law to a linear diffusion. With initial data being Gaussian, the limit is a Gauss-Markov process and we prove the process will stabilize exponentially fast in the 2-Wasserstein metric. We apply our results to analyze and compare two regulatory mechanisms in the hematopoietic system. Simulations are conducted to verify our large-scale and long-time approximation of the dynamics. We demonstrate some regulatory mechanisms are efficient (converge to steady state rapidly) but not effective (have large fluctuation around the steady state).",
        "citation_title": "Stochastic dynamics of two-compartment models with regulatory mechanisms for hematopoiesis",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "This paper proposes a probabilistic machine learning method to price catastrophe (CAT) bonds in the primary market. The proposed method combines machine-learning-based predictive models with Conformal Prediction, an innovative algorithm that generates distribution-free probabilistic forecasts for CAT bond prices. Using primary market CAT bond transaction records between January 1999 and March 2021, the proposed method is found to be more robust and yields more accurate predictions of the bond spreads than traditional regression-based methods. Furthermore, the proposed method generates more informative prediction intervals than linear regression and identifies important nonlinear relationships between various risk factors and bond spreads, suggesting that linear regressions could misestimate the bond spreads. Overall, this paper demonstrates the potential of machine learning methods in improving the pricing of CAT bonds.",
        "citation_title": "Pricing Catastrophe Bonds -- A Probabilistic Machine Learning Approach",
        "date_delivered": "[Submitted on 10 Apr 2024]"
    },
    {
        "abstract": "A long-standing issue in mathematical finance is the speed-up of pricing options, especially multi-asset options. A recent study has proposed to use tensor train learning algorithms to speed up Fourier transform (FT)-based option pricing, utilizing the ability of tensor networks to compress high-dimensional tensors. Another usage of the tensor network is to compress functions, including their parameter dependence. In this study, we propose a pricing method, where, by a tensor learning algorithm, we build tensor trains that approximate functions appearing in FT-based option pricing with their parameter dependence and efficiently calculate the option price for the varying input parameters. As a benchmark test, we run the proposed method to price a multi-asset option for the various values of volatilities and present asset prices. We show that, in the tested cases involving up to about 10 assets, the proposed method is comparable to or outperforms Monte Carlo simulation with $10^5$ paths in terms of computational complexity, keeping the comparable accuracy.",
        "citation_title": "Learning tensor networks with parameter dependence for Fourier-based option pricing",
        "date_delivered": "[Submitted on 17 Apr 2024]"
    },
    {
        "abstract": "This paper investigates gaps in access to and the cost of housing credit by race and ethnicity using the near universe of U.S. mortgage applications. Our data contain borrower creditworthiness variables that have historically been absent from industry-wide application data and that are likely to affect application approval and loan pricing. We find large unconditional disparities in approval and pricing between racial and ethnic groups. After conditioning on key elements of observable borrower creditworthiness, these disparities are smaller but remain economically meaningful. Sensitivity analysis indicates that omitted factors as predictive of approval/pricing and race/ethnicity as credit score can explain some of the pricing disparities but cannot explain the approval disparities. Taken together, our results suggest that credit score, income, and down payment requirements significantly contribute to disparities in mortgage access and affordability but that other systemic barriers are also responsible for a large share of disparate outcomes in the mortgage market.",
        "citation_title": "Racial and Ethnic Disparities in Mortgage Lending: New Evidence from Expanded HMDA Data",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "By employing causal discovery method, the Fast Causal Inference (FCI) model to analyze data from the 2022 \"Financial Literacy Survey,\" we explore the causal relationships between financial literacy and financial activities, specifically investment participation and retirement planning. Our findings indicate that increasing financial literacy may not directly boost engagement in financial investments or retirement planning in Japan, which underscores the necessity for alternative strategies to motivate financial activities among Japanese households. This research offers valuable insights for policymakers focused on improving financial well-being by advancing the use of causal discovery algorithms in understanding financial behaviors.",
        "citation_title": "Does Financial Literacy Impact Investment Participation and Retirement Planning in Japan?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The study uses bibliometric as well as content analysis to determine the current situation regarding the application of technology adoption models (i.e., the Technology Acceptance Model, Unified Theory of Acceptance and Use of Technology, and Innovation Diffusion Theory) to the smartphone market that also includes smart wearables. Hereby the author would like to determine the connection between smartphone usage and adoption models and enrich literature by defining state-of-the-art tendencies and approaches. To achieve the goal, the author applied a two-stage approach: in the first stage, 213 articles were analyzed using Citation and Bibliographic coupling tools in VOSviewer (1.6.20). The papers were selected from the Scopus database and the search of the papers was conducted in the fields of Economics, Business, and Computer technologies. In the second stage, the author conducted a brief literature review of the most influential papers. The results illustrate the situation regarding the implementation of different models in the case of smartphone adoption. Content analyses of the most influential papers were applied to explain and enrich the results of bibliometric analyses as well as determine research gaps and future research development.",
        "citation_title": "Modelling user behavior towards smartphones and wearable technologies: A bibliometric study and brief literature review",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This article introduces the groundbreaking concept of the financial differential machine learning algorithm through a rigorous mathematical framework. Diverging from existing literature on financial machine learning, the work highlights the profound implications of theoretical assumptions within financial models on the construction of machine learning algorithms.\nThis endeavour is particularly timely as the finance landscape witnesses a surge in interest towards data-driven models for the valuation and hedging of derivative products. Notably, the predictive capabilities of neural networks have garnered substantial attention in both academic research and practical financial applications.\nThe approach offers a unified theoretical foundation that facilitates comprehensive comparisons, both at a theoretical level and in experimental outcomes. Importantly, this theoretical grounding lends substantial weight to the experimental results, affirming the differential machine learning method's optimality within the prevailing context.\nBy anchoring the insights in rigorous mathematics, the article bridges the gap between abstract financial concepts and practical algorithmic implementations.",
        "citation_title": "Mathematics of Differential Machine Learning in Derivative Pricing and Hedging",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We formulate quantum computing solutions to a large class of dynamic nonlinear asset pricing models using algorithms, in theory exponentially more efficient than classical ones, which leverage the quantum properties of superposition and entanglement. The equilibrium asset pricing solution is a quantum state. We introduce quantum decision-theoretic foundations of ambiguity and model/parameter uncertainty to deal with model selection.",
        "citation_title": "On Quantum Ambiguity and Potential Exponential Computational Speed-Ups to Solving",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the distributed systems landscape, Blockchain has catalyzed the rise of cryptocurrencies, merging enhanced security and decentralization with significant investment opportunities. Despite their potential, current research on cryptocurrency trend forecasting often falls short by simplistically merging sentiment data without fully considering the nuanced interplay between financial market dynamics and external sentiment influences. This paper presents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency trends using multimodal time-series data. Our approach, which integrates critical cryptocurrency metrics with sentiment data from news and social media analyzed through CryptoBERT, addresses the inherent volatility and prediction challenges in cryptocurrency markets. By combining elements of distributed systems, natural language processing, and financial forecasting, our method outperforms conventional models like LSTM and Transformer by up to 20\\% in prediction accuracy. This advancement deepens the understanding of distributed systems and has practical implications in financial markets, benefiting stakeholders in cryptocurrency and blockchain technologies. Moreover, our enhanced forecasting approach can significantly support decentralized science (DeSci) by facilitating strategic planning and the efficient adoption of blockchain technologies, improving operational efficiency and financial risk management in the rapidly evolving digital asset domain, thus ensuring optimal resource allocation.",
        "citation_title": "DAM: A Universal Dual Attention Mechanism for Multimodal Timeseries Cryptocurrency Trend Forecasting",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Experiments violating Bell's inequality appear to indicate deterministic models do not correspond to a realistic theory of quantum mechanics. The theory of pilot waves seemingly overcomes this hurdle via nonlocality and statistical dependence, however it necessitates the existence of \"ghost waves\". This manuscript develops a deterministic dynamical system with local interactions. The aggregate behavior of the trajectories are reminiscent of a quantum particle evolving under the Schr\u00f6dinger equation and reminiscent of Feynman's path integral interpretation in three canonical examples: motion in free space, double slit diffraction, and superluminal barrier traversal. Moreover, the system bifurcates into various dynamical regimes including a classical limit. These results illustrate a deterministic alternative to probabilistic interpretations and aims to shed light on the transition from quantum to classical mechanics.",
        "citation_title": "Towards a Deterministic Interpretation of Quantum Mechanics: Insights from Dynamical Systems",
        "date_delivered": "[Submitted on 23 Apr 2024]"
    },
    {
        "abstract": "We develop a Schwinger-Keldysh field theory (SKFT) for open quantum systems interacting with a dissipative environment and apply it to the spin-boson model as an archetypical example where the environment is composed of a bosonic bath. Prior SKFT developments of this type have been confined to the Markovian regime, as an alternative to a conventional description by the Lindblad quantum master equation (QME) which is a time-local matrix differential equation. Here we combine SKFT with a two-particle irreducible (2PI) action that resums a class of Feynman diagrams to infinite order. We obtain the time-evolution of the spin density matrix in the form of a system of integro-differential equations applicable to both Markovian and non-Markovian regimes. The latter regime--where taking into account memory effects becomes essential--poses a challenge for standard methods when trying to incorporate arbitrary properties of the system, bath, and length of time evolution. The SKFT+2PI-computed time evolution of the spin expectation values in the Markovian regime reproduces the solution of the Lindblad QME, as long as the system-bath coupling in the latter is adjusted by increasing it. In the non-Markovian regime, SKFT+2PI yields a nonperturbative solution that mimics results from both hierarchical equations of motion and tensor networks methods that we employ as benchmarks. Our SKFT+2PI approach can also access challenging cases, such as zero-temperature and sub-Ohmic bath, as well as arbitrary long evolution times. Taking into account favorable numerical cost of solving the integro-differential equations with increasing number of spins, time steps or dimensionality the SKFT+2PI approach offers a promising route for simulation of driven-dissipative systems in quantum computing or quantum magnonics and spintronics in the presence of a variety of (single or multiple) dissipative environments.",
        "citation_title": "Schwinger-Keldysh nonequilibrium quantum field theory of open quantum systems beyond the Markovian regime: Application to the spin-boson model",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In the past decade, the toolkit of quantum information has been expanded to include processes in which the basic operations do not have definite causal relations. Originally considered in the context of the unification of quantum mechanics and general relativity, these causally indefinite processes have been shown to offer advantages in a wide variety of quantum information processing tasks, ranging from quantum computation to quantum metrology. Here we overview these advantages and the experimental efforts to realise them. We survey both the different experimental techniques employed, as well as theoretical methods developed in support of the experiments, before discussing the interpretations of current experimental results and giving an outlook on the future of the field.",
        "citation_title": "Experimental Aspects of Indefinite Causal Order in Quantum Mechanics",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study quantum-classical separations between classical and quantum supervised learning models based on constant depth (i.e., shallow) circuits, in scenarios with and without noises. We construct a classification problem defined by a noiseless shallow quantum circuit and rigorously prove that any classical neural network with bounded connectivity requires logarithmic depth to output correctly with a larger-than-exponentially-small probability. This unconditional near-optimal quantum-classical separation originates from the quantum nonlocality property that distinguishes quantum circuits from their classical counterparts. We further derive the noise thresholds for demonstrating such a separation on near-term quantum devices under the depolarization noise model. We prove that this separation will persist if the noise strength is upper bounded by an inverse polynomial with respect to the system size, and vanish if the noise strength is greater than an inverse polylogarithmic function. In addition, for quantum devices with constant noise strength, we prove that no super-polynomial classical-quantum separation exists for any classification task defined by shallow Clifford circuits, independent of the structures of the circuits that specify the learning models.",
        "citation_title": "Quantum-Classical Separations in Shallow-Circuit-Based Learning with and without Noises",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Variational quantum computing offers a flexible computational paradigm with applications in diverse areas. However, a key obstacle to realizing their potential is the Barren Plateau (BP) phenomenon. When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases. Importantly, all the moving pieces of an algorithm -- choices of ansatz, initial state, observable, loss function and hardware noise -- can lead to BPs when ill-suited. Due to the significant impact of BPs on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects. As a result, the study of BPs has become a thriving area of research, influencing and cross-fertilizing other fields such as quantum optimal control, tensor networks, and learning theory. This article provides a comprehensive review of the current understanding of the BP phenomenon.",
        "citation_title": "A Review of Barren Plateaus in Variational Quantum Computing",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study thermalization slowing down of a quantum many-body system upon approach to two distinct integrability limits. Motivated by previous studies of classical systems, we identify two thermalization time scales: one quantum Lyapunov time scale is extracted by quantifying operator growth in time in an appropriately defined basis, while another ergodization time scale is related to statistics of fluctuations of the time-evolved operator around its mean value based on the eigenstate thermalization hypothesis. Using a paradigmatic Quantum Ising chain we find that both timescales diverge upon approach to integrability. The relative strength of the divergence of the scales depends on the particular integrable limit. This allows us to define two different universality classes of quantum thermalization: short- and long-range networks.",
        "citation_title": "Thermalization slowing down of weakly nonintegrable quantum spin dynamics",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In recent years, several experimental groups have claimed demonstrations of ``quantum supremacy'' or computational quantum advantage. A notable first claim by Google Quantum AI revolves around a metric called the Linear Cross Entropy Benchmarking (Linear XEB), which has been used in multiple quantum supremacy experiments since. The complexity-theoretic hardness of spoofing Linear XEB has nevertheless been doubtful due to its dependence on the Cross-Entropy Quantum Threshold (XQUATH) conjecture put forth by Aaronson and Gunn, which has been disproven for sublinear depth circuits. In efforts on demonstrating quantum supremacy by quantum Hamiltonian simulation, a similar benchmarking metric called the System Linear Cross Entropy Score (sXES) holds firm in light of the aforementioned negative result due to its fundamental distinction with Linear XEB. Moreover, the hardness of spoofing sXES complexity-theoretically rests on the System Linear Cross-Entropy Quantum Threshold Assumption (sXQUATH), the formal relationship of which to XQUATH is unclear. Despite the promises that sXES offers for future demonstration of quantum supremacy, in this work we show that it is an unsound benchmarking metric. Particularly, we prove that sXQUATH does not hold for sublinear depth circuits and present a classical algorithm that spoofs sXES for experiments corrupted with noise larger than certain threshold.",
        "citation_title": "Classically Spoofing System Linear Cross Entropy Score Benchmarking",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantum Darwinism is a paradigm to understand how classically objective reality emerges from within a fundamentally quantum universe. Despite the growing attention that this field of research as been enjoying, it is currently not known what specific properties a given Hamiltonian describing a generic quantum system must have to allow the emergence of classicality. Therefore, in the present work, we consider a broadly applicable generic model of an arbitrary finite-dimensional system interacting with an environment formed from an arbitrary collection of finite-dimensional degrees of freedom via an unspecified, potentially time-dependent Hamiltonian containing at most two-body interaction terms. We show that such models support quantum Darwinism if the set of operators acting on the system which enter the Hamiltonian satisfy a set of commutation relations with a pointer observable and with one other. We demonstrate our results by analyzing a wide range of example systems: a qutrit interacting with a qubit environment, a qubit-qubit model with interactions alternating in time, and a series of collision models including a minimal model of a quantum Maxwell demon.",
        "citation_title": "Classifying two-body Hamiltonians for Quantum Darwinism",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Bell-state measurement (BSM) on entangled states shared between quantum repeaters is the fundamental operation used to route entanglement in quantum networks. Performing BSMs on Werner states shared between repeaters leads to exponential decay in the fidelity of the end-to-end Werner state with the number of repeaters, necessitating entanglement distillation. Generally, entanglement routing protocols use \\emph{probabilistic} distillation techniques based on local operations and classical communication. In this work, we use quantum error correcting codes (QECCs) for \\emph{deterministic} entanglement distillation to route Werner states on a chain of repeaters. To maximize the end-to-end distillable entanglement, which depends on the number and fidelity of end-to-end Bell pairs, we utilize global link-state knowledge to determine the optimal policy for scheduling distillation and BSMs at the repeaters. We analyze the effect of the QECC's properties on the entanglement rate and the number of quantum memories. We observe that low-rate codes produce high-fidelity end-to-end states owing to their excellent error-correcting capability, whereas high-rate codes yield a larger number of end-to-end states but of lower fidelity. The number of quantum memories used at repeaters increases with the code rate as well as the classical computation time of the QECC's decoder.",
        "citation_title": "Entanglement Routing using Quantum Error Correction for Distillation",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantum optimization as a field has largely been restricted by the constraints of current quantum computing hardware, as limitations on size, performance, and fidelity mean most non-trivial problem instances won't fit on quantum devices. Even proposed solutions such as distributed quantum computing systems may struggle to achieve scale due to the high cost of inter-device communication. To address these concerns, we propose Deferred Constraint Quantum Divide and Conquer Algorithm (DC-QDCA), a method for constructing quantum circuits which greatly reduces inter-device communication costs for some quantum graph optimization algorithms. This is achieved by identifying a set of vertices whose removal partitions the input graph, known as a separator; by manipulating the placement of constraints associated with the vertices in the separator, we can greatly simplify the topology of the optimization circuit, reducing the number of required inter-device operations. Furthermore, we introduce an iterative algorithm which builds on these techniques to find solutions for problems with potentially thousands of variables. Our experimental results using quantum simulators have shown that we can construct tractable circuits nearly three times the size of previous QDCA methods while retaining a similar or greater level of quality.",
        "citation_title": "Scaling Up the Quantum Divide and Conquer Algorithm for Combinatorial Optimization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantum computing (QC) is poised to revolutionize problem solving across various fields, with research suggesting that systems with over 50 qubits may achieve quantum advantage surpassing supercomputers in certain optimization tasks. As the hardware size of Noisy Intermediate Scale Quantum (NISQ) computers continues to grow, Multi tenant computing (MTC) has emerged as a viable approach to enhance hardware utilization by allowing shared resource access across multiple quantum programs. However, MTC can also bring challenges and security concerns. This paper focuses on optimizing quantum hardware utilization in shared environments by implementing multi programming strategies that not only enhance hardware utilization but also effectively manage associated risks like crosstalk and fault injection. We propose a novel partitioning and allocation method called Community Based Dynamic Allocation Partitioning (COMDAP) and Secure COMDAP to refine and secure multi programming capabilities in quantum systems. COMDAP ensures equitable and efficient resource distribution, addresses the issues of suboptimal partitioning, and significantly improves hardware utilization. We report a 23 percent average improvement in hardware utilization rate compared to existing greedy heuristics, with rates averaging 92 percent. COMDAP introduces an average increase of approximately 0.05X in delta CX, alongside a 3.5 percent average reduction in PST across benchmarks.",
        "citation_title": "SHARE: Secure Hardware Allocation and Resource Efficiency in Quantum Systems",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Superconducting quantum processors are a compelling platform for analog quantum simulation due to the precision control, fast operation, and site-resolved readout inherent to the hardware. Arrays of coupled superconducting qubits natively emulate the dynamics of interacting particles according to the Bose-Hubbard model. However, many interesting condensed-matter phenomena emerge only in the presence of electromagnetic fields. Here, we emulate the dynamics of charged particles in an electromagnetic field using a superconducting quantum simulator. We realize a broadly adjustable synthetic magnetic vector potential by applying continuous modulation tones to all qubits. We verify that the synthetic vector potential obeys requisite properties of electromagnetism: a spatially-varying vector potential breaks time-reversal symmetry and generates a gauge-invariant synthetic magnetic field, and a temporally-varying vector potential produces a synthetic electric field. We demonstrate that the Hall effect--the transverse deflection of a charged particle propagating in an electromagnetic field--exists in the presence of the synthetic electromagnetic field.",
        "citation_title": "Implementing a synthetic magnetic vector potential in a 2D superconducting qubit array",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In this paper, we present a controllability analysis of the quantum Ising periodic chain of n spin 1/2 particles where the interpolating parameter between the two Hamiltonians plays the role of the control. A fundamental result in the control theory of quantum systems states that the set of achievable evolutions is (dense in) the Lie group corresponding to the Lie algebra generated by the Hamiltonians of the system. Such a dynamical Lie algebra therefore characterizes all the state transitions available for a given system. For the Ising spin periodic chain we characterize such a dynamical Lie algebra and therefore the set of all reachable states. In particular, we prove that the dynamical Lie algebra is a (3n-1)-dimensional Lie sub-algebra of su(2^n) which is a direct sum of a two dimensional center and a (3n-3)-dimensional semisimple Lie subalgebra. This in turn is the direct sum of n-1 Lie algebras isomorphic to su(2) parametrized by the eigenvalues of a fixed matrix. We display the basis for each of these Lie subalgebras. Therefore the problem of control for the Ising spin periodic chain is, modulo the two dimensional center, a problem of simultaneous control of n-1 spin 1/2 particles. In the process of proving this result, we develop some tools which are of general interest for the controllability analysis of quantum systems with symmetry.",
        "citation_title": "Controllability of the Periodic Quantum Ising Spin Chain",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Unitary designs are essential tools in several quantum information protocols. Similarly to other design concepts, unitary designs are mainly used to facilitate averaging over a relevant space, in this case, the unitary group $\\mathrm{U}(d)$. While it is known that exact unitary $t$-designs exist for any degree $t$ and dimension $d$, the most appealing type of designs, group designs (in which the elements of the design form a group), can provide at most $3$-designs. Moreover, even group $2$-designs can only exist in limited dimensions. In this paper, we present novel construction methods for creating exact generalized group designs based on the representation theory of the unitary group and its finite subgroups that overcome the $4$-design-barrier of unitary group designs. Furthermore, a construction is presented for creating generalized group $2$-designs in arbitrary dimensions.",
        "citation_title": "Generalized group designs: overcoming the 4-design-barrier and constructing novel unitary 2-designs in arbitrary dimensions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Different kinds of wave packet transforms are widely used for extracting multi-scale structures in signal processing tasks. This paper introduces the quantum circuit implementation of a broad class of wave packets, including Gabor atoms and wavelets, with compact frequency support. Our approach operates in the frequency space, involving reallocation and reshuffling of signals tailored for manipulation on quantum computers. The resulting implementation is different from the existing quantum algorithms for spatially compactly supported wavelets and can be readily extended to quantum transforms of other wave packets with compact frequency support.",
        "citation_title": "Quantum Wave Packet Transforms with compact frequency support",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A combinatorial optimization problem is to find an optimal solution under the constraints. This is one of the potential applications for quantum computers. Quantum Random Access Optimization (QRAO) is the quantum optimization algorithm that encodes multiple classical variables into a single qubit to construct a quantum Hamiltonian, thereby reducing the number of qubits required. The ground energy of the QRAO Hamiltonian provides a lower bound on the original problem's optimal value before encoding. This property allows the QRAO Hamiltonian to be used as a relaxation of the original problem, and it is thus referred to as a quantum relaxed Hamiltonian. In the Branch-and-Bound method, solving the relaxation problem plays a significant role. In this study, we developed Quantum Relaxation based Branch-and-Bound (QR-BnB), a method incorporating quantum relaxation into the Branch-and-Bound framework. We solved the MaxCut Problem and the Travelling Salesman Problem in our experiments. In all instances in this study, we obtained the optimal solution whenever we successfully computed the exact lower bound through quantum relaxation. Internal strategies, such as relaxation methods and variable selection, influence the convergence of the Branch-and-Bound. Thus, we have further developed the internal strategies for QR-BnB and examined how these strategies influence its convergence. We show that our variable selection strategy via the expectation value of the Pauli operators gives better convergence than the naive random choice. QRAO deals with only unconstrained optimization problems, but QR-BnB can handle constraints more flexibly because of the Branch-and-Bound processes on the classical computing part. We demonstrate that in our experiments with the Travelling Salesman Problem, the convergence of QR-BnB became more than three times faster by using the information in the constraints.",
        "citation_title": "Efficient Internal Strategies in Quantum Relaxation based Branch-and-Bound",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "There is a growing interest in reconstructing the density matrix of photoionized electrons, in particular in complex systems where decoherence can be introduced either by a partial measurement of the system or through coupling with a stochastic environment. To this, end, several methods to reconstruct the density matrix, quantum state tomography protocols, have been developed and tested on photoelectrons ejected from noble gases following absorption of XUV photons from attosecond pulses. It remains a challenge to obtain model-free, single scan protocols that can reconstruct the density matrix with high fidelities. Current methods require extensive measurements or involve complex fitting of the signal. Faithful single-scan reconstructions would be of great help to increase the number of systems that can be studied. We propose a new and more efficient protocol - rainbow-KRAKEN - that is able to reconstruct the continuous variable density matrix of a photoelectron in a single time delay scan. It is based on measuring the coherences of a photoelectron created by absorption of an XUV pulse using a broadband IR probe that is scanned in time and a narrowband IR reference that is temporally fixed to the XUV pulse. We illustrate its performance for a Fano resonance in He as well as mixed states in Ar arising from spin-orbit splitting. We show that the protocol results in excellent fidelities and near-perfect estimation of the purity.",
        "citation_title": "A multidimensional approach to quantum state tomography of photoelectron wavepackets",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Realism about quantum theory naturally leads to realism about the quantum state of the universe. It leaves open whether it is a pure state represented by a wave function, or an impure one represented by a density matrix. I characterize and elaborate on Density Matrix Realism, the thesis that the universal quantum state is objective but can be impure. To clarify the thesis, I compare it with Wave Function Realism, explain the conditions under which they are empirically equivalent, consider two generalizations of Density Matrix Realism, and answer some frequently asked questions. I end by highlighting an implication for scientific realism.",
        "citation_title": "Density Matrix Realism",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Subtracting accidental coincidences is a common practice quantum optics experiments. For zero mean Gaussian states, such as squeezed vacuum, we show that if one removes accidental coincidences the measurement results are quantitatively the same, both for photon coincidences at very low flux and for intensity covariances. Consequently, pure quantum effects at the photon level, like interference of photon wave functions or photon bunching, are reproduced in the correlation of fluctuations of macroscopic beams issued from spontaneous down conversion. This is true both in experiment if the detection resolution is smaller than the coherence cell (size of the mode), and in stochastic simulations based on sampling the Wigner function.\nWe discuss the limitations of this correspondence, such as Bell inequalities (for which one cannot substract accidental coincidences), highly multimode situations such as quantum imaging, and higher order correlations.",
        "citation_title": "Are we allowed to subtract accidental coincidences in experiments of photon correlations?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The quantum dynamics of a dense and dipole-dipole coupled ensemble of two-level emitters interacting via their environmental thermostat is investigated. The static dipole-dipole interaction strengths are being considered strong enough but smaller than the transition frequency. Therefore, the established thermal equilibrium of ensemble's quantum dynamics is described with respect to the dipole-dipole coupling strengths. We have demonstrated the quantum nature of the spontaneously scattered light field in this process for weaker thermal baths as well as non-negligible dipole-dipole couplings compared to the emitter's transition frequency. Furthermore, the collectively emitted photon intensity suppresses or enhances depending on the environmental thermal baths intensities.",
        "citation_title": "Dense dipole-dipole-coupled two-level systems in a thermal bath",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Objective collapse theories propose modifications to Schr\u00f6dinger's equation that solve the quantum measurement problem by interpolating between microscopic quantum dynamics and projective evolution of macroscopic objects. Objective collapse theories extending the equilibrium description of spontaneous symmetry breaking to spontaneous violations of unitarity in quantum dynamics were recently shown to possess a physical white noise limit when applied to initial two-state superpositions. Here, we show the existence of a generic physical white noise limit for models of spontaneous unitarity violation applicable to any initial state. We show that in this limit, the emergence of Born rule statistics is enforced by a fluctuation-dissipation relation, and that the ensemble averaged probability densities follow the GKSL master equation corresponding to a linear quantum semi-group, guaranteeing the absence of superluminal signalling.",
        "citation_title": "Generic signalling-free white-noise limit for models of spontaneous unitarity violation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Among various quantum machine learning (QML) algorithms, the quantum kernel method has especially attracted attention due to its compatibility with noisy intermediate-scale quantum devices and its potential to achieve quantum advantage. This method performs classification and regression by nonlinearly mapping data into quantum states in a higher dimensional Hilbert space. Thus far, the quantum kernel method has been implemented only on qubit-based systems, but continuous-variable (CV) systems can potentially offer superior computational power by utilizing its infinite-dimensional Hilbert space. Here, we demonstrate the implementation of the classification task with the CV quantum kernel method on a programmable photonic quantum processor. We experimentally prove that the CV quantum kernel method successfully classifies several datasets robustly even under the experimental imperfections, with high accuracies comparable to the classical kernel. This demonstration sheds light on the utility of CV quantum systems for QML and should stimulate further study in other CV QML algorithms.",
        "citation_title": "Continuous-variable quantum kernel method on a programmable photonic quantum processor",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present a superquantization rule which indicates the possible robust stationary states of a generic driven-dissipative quantum system. Multistability in a driven cavity mode interacting with a qudit is revealed in this way within a simple intuitive picture. The accuracy of the superquantization approach is confirmed by numerical simulations of the underlying quantum model. In the case when the qudit is composed of several two-level emitters coupled homogeneously to the cavity, we demonstrate the robustness of the superquantized steady states to single-emitter decay.",
        "citation_title": "Superquantization rule for multistability in driven-dissipative quantum systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this paper, we present a quantum algorithm for approximating multivariate traces, i.e. the traces of matrix products. Our research is motivated by the extensive utility of multivariate traces in elucidating spectral characteristics of matrices, as well as by recent advancements in leveraging quantum computing for faster numerical linear algebra. Central to our approach is a direct translation of a multivariate trace formula into a quantum circuit, achieved through a sequence of low-level circuit construction operations. To facilitate this translation, we introduce \\emph{quantum Matrix States Linear Algebra} (qMSLA), a framework tailored for the efficient generation of state preparation circuits via primitive matrix algebra operations. Our algorithm relies on sets of state preparation circuits for input matrices as its primary inputs and yields two state preparation circuits encoding the multivariate trace as output. These circuits are constructed utilizing qMSLA operations, which enact the aforementioned multivariate trace formula. We emphasize that our algorithm's inputs consist solely of state preparation circuits, eschewing harder to synthesize constructs such as Block Encodings. Furthermore, our approach operates independently of the availability of specialized hardware like QRAM, underscoring its versatility and practicality.",
        "citation_title": "Multivariate trace estimation using quantum state space linear algebra",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The potential of employing higher orders of the Trotter-Suzuki decomposition of the evolution operator for more effective simulations of quantum systems on a noisy quantum computer is explored. By examining the transverse-field Ising model and the XY model, it is demonstrated that when the gate error is decreased by approximately an order of magnitude relative to typical modern values, higher-order Trotterization becomes advantageous. This form of Trotterization yields a global minimum of the overall simulation error, comprising both the mathematical error of Trotterization and the physical error arising from gate execution.",
        "citation_title": "Optimal-order Trotter-Suzuki decomposition for quantum simulation on noisy quantum computers",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We introduce the spanning tree matching (STM) decoder for surface codes, which guarantees the error correction capability up to the code's designed distance by first employing an instance of the minimum spanning tree on a subset of ancilla qubits within the lattice. Then, a perfect matching graph is simply obtained, by selecting the edges more likely to be faulty. A comparative analysis reveals that the STM decoder, at the cost of a slight performance degradation, provides a substantial advantage in decoding time compared to the minimum weight perfect matching (MWPM) decoder. Finally, we propose an even more simplified and faster algorithm, the Rapid-Fire (RFire) decoder, designed for scenarios where decoding speed is a critical requirement.",
        "citation_title": "Spanning Tree Matching Decoder for Quantum Surface Codes",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work we study quantum algorithms for Hopcroft's problem which is a fundamental problem in computational geometry. Given $n$ points and $n$ lines in the plane, the task is to determine whether there is a point-line incidence. The classical complexity of this problem is well-studied, with the best known algorithm running in $O(n^{4/3})$ time, with matching lower bounds in some restricted settings. Our results are two different quantum algorithms with time complexity $\\widetilde O(n^{5/6})$. The first algorithm is based on partition trees and the quantum backtracking algorithm. The second algorithm uses a quantum walk together with a history-independent dynamic data structure for storing line arrangement which supports efficient point location queries. In the setting where the number of points and lines differ, the quantum walk-based algorithm is asymptotically faster. The quantum speedups for the aforementioned data structures may be useful for other geometric problems.",
        "citation_title": "Quantum algorithms for Hopcroft's problem",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Li\u00e9nard-type nonlinear oscillators with linear and nonlinear damping terms exhibit diverse dynamical behavior in both the classical and quantum regimes. In this paper, we consider examples of various one-dimensional Li\u00e9nard type-I and type-II oscillators. The associated Euler-Lagrange equations are divided into groups based on the characteristics of the damping and forcing terms. The Li\u00e9nard type-I oscillators often display localized solutions, isochronous and non-isochronous oscillations and are also precisely solvable in quantum mechanics in general, where the ordering parameters play an important role. These include Mathews-Lakshmanan and Higgs oscillators. However, the classical solutions of some of the nonlinear oscillators are expressed in terms of elliptic functions and have been found to be quasi-exactly solvable in the quantum region. The three-dimensional generalizations of these classical systems add more degrees of freedom, which show complex dynamics. Their quantum equivalents are also explored in this article. The isotonic generalizations of the non-isochronous nonlinear oscillators have also been solved both classically and quantum mechanically to advance the studies. The modified Emden equation categorized as Li\u00e9nard type-II exhibits isochronous oscillations at the classical level. This property makes it a valuable tool for studying the underlying nonlinear dynamics. The study on the quantum counterpart of the system provides a deeper understanding of the behavior in the quantum realm as a typical PT-symmetric system.",
        "citation_title": "Li\u00e9nard Type Nonlinear Oscillators and Quantum Solvability",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We investigate the fate of dissipative phase transitions in quantum many-body systems when the individual constituents are qudits ($d$-level systems) instead of qubits. As an example system, we employ a permutation-invariant $XY$ model of $N$ infinite-range interacting $d$-level spins undergoing individual and collective dissipation. In the mean-field limit, we identify a dissipative phase transition, whose critical point is independent of $d$ after a suitable rescaling of parameters. When the decay rates between all adjacent levels are identical and $d\\geq 4$, the critical point expands, in terms of the ratio between dissipation and interaction strengths, to a critical region in which two phases coexist and which increases as $d$ grows. In addition, a larger $d$ leads to a more pronounced change in spin expectation values at the critical point. Numerical investigations for finite $N$ reveal symmetry breaking signatures in the Liouvillian spectrum at the phase transition. The phase transition is furthermore marked by maximum entanglement negativity and a significant purity change of the steady state, which become more pronounced as $d$ increases. Considering qudits instead of qubits thus opens new perspectives on accessing rich phase diagrams in open many-body systems.",
        "citation_title": "Dissipative phase transition: from qubits to qudits",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We demonstrate QKD and data communication over an out-door free-space link where large-core fiber substitutes active alignment. We further prove E-band QKD as stable and robust under full daylight, despite the loss of spatial filtering.",
        "citation_title": "Solar-Blind QKD over Simplified Short-Range FSO Link",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We implement a silicon-on-insulator light emitter as optical supply for a QKD transmitter and transfer it to an electronic BiCMOS wafer. A secure key is established over short reach in co-existence with shortwave data transmission.",
        "citation_title": "First Demonstration of a Group-IV Emitter on Photonic BiCMOS Supplying a Quantum Communication Link",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the current era of Noisy Intermediate Scale Quantum (NISQ) computing, efficient digital simulation of quantum systems holds significant importance for quantum algorithm development, verification and validation. However, analysis of sparsity within these simulations remains largely unexplored. In this paper, we present a novel observation regarding the prevalent sparsity patterns inherent in quantum circuits. We introduce DiaQ, a new sparse matrix format tailored to exploit this quantum-specific sparsity, thereby enhancing simulation performance. Our contribution extends to the development of libdiaq, a numerical library implemented in C++ with OpenMP for multi-core acceleration and SIMD vectorization, featuring essential mathematical kernels for digital quantum simulations. Furthermore, we integrate DiaQ with SV-Sim, a state vector simulator, yielding substantial performance improvements across various quantum circuits (e.g., ~26.67% for GHZ-28 and ~32.72% for QFT-29 with multi-core parallelization and SIMD vectorization on Frontier). Evaluations conducted on benchmarks from SupermarQ and QASMBench demonstrate that DiaQ represents a significant step towards achieving highly efficient quantum simulations.",
        "citation_title": "DiaQ: Efficient State-Vector Quantum Simulation",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "The inverse current in coupled (ICC) quantum transport, where one induced current opposes all thermodynamic forces of a system, is a highly counter-intuitive transport phenomenon. Using an exactly solvable model of strongly-coupled quantum dots, we present thermodynamic description of ICC in energy and spin-induced particle currents, with potential applications towards unconventional and autonomous nanoscale thermoelectric generators. Our analysis reveals the connection between microscopic and macroscopic formulations of entropy production rates, elucidating the often-overlooked role of proper thermodynamic forces and conjugate fluxes in characterizing genuine ICC. In our model, the seemingly paradoxical results of ICC in the energy current arise from chemical work done by current-carrying quantum particles, while in spin-induced particle current, it stems from the relative competition between electron reservoirs controlling one particular transition.",
        "citation_title": "Thermodynamic theory of inverse current in coupled quantum transport",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The quantum imaginary time evolution (QITE) methodology was developed to overcome a critical issue as regards non-unitarity in the implementation of imaginary time evolution on a quantum computer. QITE has since been used to approximate ground states of various physical systems. In this paper, we demonstrate a practical application of QITE as a quantum numerical solver for linear partial differential equations. Our algorithm takes inspiration from QITE in that the quantum state follows the same normalised trajectory in both algorithms. However, it is our QITE methodology's ability to track the scale of the state vector over time that allows our algorithm to solve differential equations. We demonstrate our methodology with numerical simulations and use it to solve the heat equation in one and two dimensions using six and ten qubits, respectively.",
        "citation_title": "Generalising quantum imaginary time evolution to solve linear partial differential equations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "It is commonly believed that logical states of quantum error-correcting codes have to be highly entangled such that codes capable of correcting more errors require more entanglement to encode a qubit. Here we show that this belief may or may not be true depending on a particular code. To this end, we characterize a tradeoff between the code distance $d$ quantifying the number of correctable errors, and geometric entanglement of logical states quantifying their maximal overlap with product states or more general \"topologically trivial\" states. The maximum overlap is shown to be exponentially small in $d$ for three families of codes: (1) low-density parity check (LDPC) codes with commuting check operators, (2) stabilizer codes, and (3) codes with a constant encoding rate. Equivalently, the geometric entanglement of any logical state of these codes grows at least linearly with $d$. On the opposite side, we also show that this distance-entanglement tradeoff does not hold in general. For any constant $d$ and $k$ (number of logical qubits), we show there exists a family of codes such that the geometric entanglement of some logical states approaches zero in the limit of large code length.",
        "citation_title": "How much entanglement is needed for quantum error correction?",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "An algorithm for providing analytical solutions to Schr\u00f6dinger's equation with non-exactly solvable potentials is elaborated. It represents a symbiosis between the logarithmic expansion method and the techniques of the superymmetric quantum mechanics as extended toward non shape invariant potentials. The complete solution to a given Hamiltonian $H_{0}$ is obtained from the nodeless states of the Hamiltonian $H_{0}$ and of a set of supersymmetric partners $H_{1}, H_{2},..., H_{r}$. The nodeless states (dubbed \"edge\" states) are unique and in general can be ground or excited states. They are solved using the logarithmic expansion which yields an infinite systems of coupled first order hierarchical differential equations, converted later into algebraic equations with recurrence relations which can be solved order by order. We formulate the aforementioned scheme, termed to as \"Supersymmetric Expansion Algorithm'' step by step and apply it to obtain for the first time the complete analytical solutions of the three dimensional Hulth\u00e9n--, and the one-dimensional anharmonic oscillator potentials.",
        "citation_title": "Supersymmetric Expansion Algorithm and complete analytical solution for the Hulth\u00e9n and anharmonic potentials",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Benchmarking Quantum Process Units (QPU) at an application level usually requires considering the whole programming stack of the quantum computer. One critical task is the minor-embedding (resp. transpilation) step, which involves space-time overheads for annealing-based (resp. gate-based) quantum computers. This paper establishes a new protocol to generate graph instances with their associated near-optimal minor-embedding mappings to D-Wave Quantum Annealers (QA). This set of favorable mappings is used to generate a wide diversity of optimization problem instances. We use this method to benchmark QA on large instances of unconstrained and constrained optimization problems and compare the performance of the QPU with efficient classical solvers. The benchmark aims to evaluate and quantify the key characteristics of instances that could benefit from the use of a quantum computer. In this context, existing QA seem best suited for unconstrained problems on instances with densities less than $10\\%$. For constrained problems, the penalty terms used to encode the hard constraints restrict the performance of QA and suggest that these QPU will be less efficient on these problems of comparable size.",
        "citation_title": "Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Entangling photons is a critical challenge for photonic quantum information processing: entanglement is a crucial resource for quantum communication and computation but can only be performed in a probabilistic manner when using linear optics. In this work, we leverage a two-photon state matrix representation to derive necessary and sufficient conditions on two-photon entangling operations with linear optics. We give a characterization of the input photonic states that can be used to prepare arbitrary two-qudit states in d-rail encoding with post-selection. We determine how many auxiliary photons are required to prepare any two-photon state with heralding. In addition, we present a construction for generalized post-selected n-qubit control-rotation gates.",
        "citation_title": "Simple rules for two-photon state preparation with linear optics",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Quantum state tomography with rigorous guarantees with respect to the trace distance, the most operationally meaningful metric for distinguishing quantum states, has been studied extensively for finite-dimensional systems; however, it remains almost unexplored for continuous variable systems. This work fills this gap. We prove that learning energy-constrained $n$-mode states without any additional prior assumption is extremely inefficient: The minimum number of copies needed for achieving an $\\varepsilon$-approximation in trace distance scales as $\\sim \\varepsilon^{-2n}$, in stark contrast to the $n$-qudit case, where the $\\varepsilon$-scaling is $\\sim \\varepsilon^{-2}$. Specifically, we find the optimal sample complexity of tomography of energy-constrained pure states, thereby establishing the ultimate achievable performance of tomography of continuous variable systems. Given such an extreme inefficiency, we then investigate whether more structured, yet still physically interesting, classes of quantum states can be efficiently tomographed. We rigorously prove that this is indeed the case for Gaussian states, a result previously assumed but never proved in the literature. To accomplish this, we establish bounds on the trace distance between two Gaussian states in terms of the norm distance of their first and second moments, which constitute technical tools of independent interest. This allows us to answer a fundamental question for the field of Gaussian quantum information: by estimating the first and second moments of an unknown Gaussian state with precision $\\varepsilon$, what is the resulting trace distance error on the state? Lastly, we show how to efficiently learn $t$-doped Gaussian states, i.e., states prepared by Gaussian unitaries and at most $t$ local non-Gaussian evolutions, unveiling more of the structure of these slightly-perturbed Gaussian systems.",
        "citation_title": "Learning quantum states of continuous variable systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We introduce a hardware-specific, problem-dependent digital-analog quantum algorithm of a counterdiabatic quantum dynamics tailored for optimization problems. Specifically, we focus on trapped-ion architectures, taking advantage from global M\u00f8lmer-S\u00f8rensen gates as the analog interactions complemented by digital gates, both of which are available in the state-of-the-art technologies. We show an optimal configuration of analog blocks and digital steps leading to a substantial reduction in circuit depth compared to the purely digital approach. This implies that, using the proposed encoding, we can address larger optimization problem instances, requiring more qubits, while preserving the coherence time of current devices. Furthermore, we study the minimum gate fidelity required by the analog blocks to outperform the purely digital simulation, finding that it is below the best fidelity reported in the literature. To validate the performance of the digital-analog encoding, we tackle the maximum independent set problem, showing that it requires fewer resources compared to the digital case. This hybrid co-design approach paves the way towards quantum advantage for efficient solutions of quantum optimization problems.",
        "citation_title": "Digital-Analog Counterdiabatic Quantum Optimization with Trapped Ions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the circuit quantum electrodynamics architectures, to realize a long-range quantum network mediated by flying photon, it is necessary to shape the temporal profile of emitted photons to achieve high transfer efficiency between two quantum nodes. In this work, we demonstrate a new single-rail and dual-rail time-bin shaped photon generator without additional flux-tunable elements, which can act as a quantum interface of a point-to-point quantum network. In our approach, we adopt a qubit-resonator-transmission line configuration, and the effective coupling strength between the qubit and the resonator can be varied by parametrically modulating the qubit frequency. In this way, the coupling is directly proportional to the parametric modulation amplitude and covers a broad tunable range beyond 20 MHz for the sample we used. Additionally, when emitting shaped photons, we find that the spurious frequency shift (-0.4 MHz) due to parametric modulation is small and can be readily calibrated through chirping. We develop an efficient photon field measurement setup based on the data stream processing of GPU. Utilizing this system, we perform photon temporal profile measurement, quantum state tomography of photon field, and quantum process tomography of single-rail quantum state transfer based on a heterodyne measurement scheme. The single-rail encoding state transfer fidelity of shaped photon emission is 90.32%, and that for unshaped photon is 97.20%, respectively. We believe that the fidelity of shaped photon emission is mainly limited by the qubit coherence time. The results demonstrate that our method is hardware efficient, simple to implement, and scalable. It could become a viable tool in a high-quality quantum network utilizing both single-rail and dual-rail time-bin encoding.",
        "citation_title": "On-demand shaped photon emission based on a parametrically modulated qubit",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Quantum parameter estimation theory is an important component of quantum information theory and provides the statistical foundation that underpins important topics such as quantum system identification and quantum waveform estimation. When there is more than one parameter the ultimate precision in the mean square error given by the quantum Cram\u00e9r-Rao bound is not necessarily achievable. For non-full rank quantum states, it was not known when this bound can be saturated (achieved) when only a single copy of the quantum state encoding the unknown parameters is available. This single-copy scenario is important because of its experimental/practical tractability. Recently, necessary and sufficient conditions for saturability of the quantum Cram\u00e9r-Rao bound in the multiparameter single-copy scenario have been established in terms of i) the commutativity of a set of projected symmetric logarithmic derivatives and ii) the existence of a unitary solution to a system of coupled nonlinear partial differential equations. New sufficient conditions were also obtained that only depend on properties of the symmetric logarithmic derivatives. In this paper, key structural properties of optimal measurements that saturate the quantum Cram\u00e9r-Rao bound are illuminated. These properties are exploited to i) show that the sufficient conditions are in fact necessary and sufficient for an optimal measurement to be projective, ii) give an alternative proof of previously established necessary conditions, and iii) describe general POVMs, not necessarily projective, that saturate the multiparameter QCRB. Examples are given where a unitary solution to the system of nonlinear partial differential equations can be explicitly calculated when the required conditions are fulfilled.",
        "citation_title": "Saturation of the Multiparameter Quantum Cram\u00e9r-Rao Bound at the Single-Copy Level with Projective Measurements",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A formalism of classical mechanics is given for time-dependent many-body states of quantum mechanics, describing both fluid flow and point mass trajectories. The familiar equations of energy, motion, and those of Lagrangian mechanics are obtained. An energy and continuity equation is demonstrated to be equivalent to the real and imaginary parts of the time dependent Schroedinger equation, respectively, where the Schroedinger equation is in density matrix form. For certain stationary states, using Lagrangian mechanics and a Hamiltonian function for quantum mechanics, equations for point-mass trajectories are obtained. For 1-body states and fluid flows, the energy equation and equations of motion are the Bernoulli and Euler equations of fluid mechanics, respectively. Generalizations of the energy and Euler equations are derived to obtain equations that are in the same form as they are in classical mechanics. The fluid flow type is compressible, inviscid, irrotational, with the nonclassical element of local variable mass. Over all space mass is conserved. The variable mass is a necessary condition for the fluid flow to agree with the zero orbital angular momentum for s states of hydrogen. Cross flows are examined, where velocity directions are changed without changing the kinetic energy. For one-electron atoms, the velocity modification gives closed orbits for trajectories, and mass conservation, vortexes, and density stratification for fluid flows. For many body states, Under certain conditions, and by hypotheses, Euler equations of orbital-flows are obtained. One-body Schroedinger equations that are a generalization of the Hartree-Fock equations are also obtained. These equations contain a quantum Coulomb's law, involving the 2-body pair function of reduced density matrix theory that replace the charge densities.",
        "citation_title": "A Formulation of Quantum Fluid Mechanics and Trajectories",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We develop a general theory for multiphoton qubit-resonator interactions enhanced by a qubit drive. The interactions generate qubit-conditional operations in the resonator when the driving is near $n$-photon cross-resonance, namely, the qubit drive is $n$-times the resonator frequency. We pay special attention to the strong driving regime, where the interactions are conditioned on the qubit dressed states. We consider the specific case where $n=2$, which results in qubit-conditional squeezing (QCS). We propose to use the QCS protocol for amplifying resonator displacements and their superpositions. We find the QCS protocol to generate a superposition of orthogonally squeezed states following a properly chosen qubit measurement. We outline quantum information processing applications for these states, including encoding a qubit in a resonator and performing a quantum non-demolition measurement of the qubit inferred from the resonator's second statistical moment. Next, we employ a two-tone drive to engineer an effective $n$-photon Rabi Hamiltonian in any desired coupling regime. In other words, the effective coupling strengths can be tuned over a wide range, thus allowing for the realization of new regimes that have so far been inaccessible. Finally, we propose a multiphoton circuit QED implementation based on a transmon qubit coupled to a resonator via an asymmetric SQUID. We provide realistic parameter estimates for the two-photon operation regime that can host the aforementioned two-photon protocols. We use numerical simulations to show that even in the presence of spurious terms and decoherence, our analytical predictions are robust.",
        "citation_title": "Driven Multiphoton Qubit-Resonator Interactions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Given a quantum channel and a state which satisfy a fixed point equation approximately (say, up to an error $\\varepsilon$), can one find a new channel and a state, which are respectively close to the original ones, such that they satisfy an exact fixed point equation? It is interesting to ask this question for different choices of constraints on the structures of the original channel and state, and requiring that these are also satisfied by the new channel and state. We affirmatively answer the above question, under fairly general assumptions on these structures, through a compactness argument. Additionally, for channels and states satisfying certain specific structures, we find explicit upper bounds on the distances between the pairs of channels (and states) in question. When these distances decay quickly (in a particular, desirable manner) as $\\varepsilon\\to 0$, we say that the original approximate fixed point equation is rapidly fixable. We establish rapid fixability, not only for general quantum channels, but also when the original and new channels are both required to be unitary, mixed unitary or unital. In contrast, for the case of bipartite quantum systems with channels acting trivially on one subsystem, we prove that approximate fixed point equations are not rapidly fixable. In this case, the distance to the closest channel (and state) which satisfy an exact fixed point equation can depend on the dimension of the quantum system in an undesirable way. We apply our results on approximate fixed point equations to the question of robustness of quantum Markov chains (QMC) and establish the following: For any tripartite quantum state, there exists a dimension-dependent upper bound on its distance to the set of QMCs, which decays to zero as the conditional mutual information of the state vanishes.",
        "citation_title": "Robustness of Fixed Points of Quantum Channels and Application to Approximate Quantum Markov Chains",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A long-standing issue in mathematical finance is the speed-up of pricing options, especially multi-asset options. A recent study has proposed to use tensor train learning algorithms to speed up Fourier transform (FT)-based option pricing, utilizing the ability of tensor networks to compress high-dimensional tensors. Another usage of the tensor network is to compress functions, including their parameter dependence. In this study, we propose a pricing method, where, by a tensor learning algorithm, we build tensor trains that approximate functions appearing in FT-based option pricing with their parameter dependence and efficiently calculate the option price for the varying input parameters. As a benchmark test, we run the proposed method to price a multi-asset option for the various values of volatilities and present asset prices. We show that, in the tested cases involving up to about 10 assets, the proposed method is comparable to or outperforms Monte Carlo simulation with $10^5$ paths in terms of computational complexity, keeping the comparable accuracy.",
        "citation_title": "Learning tensor networks with parameter dependence for Fourier-based option pricing",
        "date_delivered": "[Submitted on 17 Apr 2024]"
    },
    {
        "abstract": "Gauge-field configurations with non-trivial topology have profound consequences for the physics of Abelian and non-Abelian gauge theories. Over time, arguments have been gathering for the existence of gauge-field configurations with fractional topological charge, called fractons. Ground-state properties of gauge theories can drastically change in presence of fractons in the path integral. However, understanding the origin of such fractons is usually restricted to semi-classical argumentation. Here, we show that fractons persist in strongly correlated many-body systems, using the multiflavor Schwinger model of quantum electrodynamics as a paradigm example. Through detailed numerical tensor-network analysis, we find strong fracton signatures even in highly discretized lattice models, at sizes that are implementable on already existing quantum-simulation devices. Our work sheds light on how the non-trivial topology of gauge theories persists in challenging non-perturbative regimes, and it shows a path forward to probing it in table-top experiments.",
        "citation_title": "Non-perturbative signatures of fractons in the twisted multi-flavor Schwinger Model",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Quantum machine learning is a new research field combining quantum information science and machine learning. Quantum computing technologies seem to be particularly well suited to solving problems in the health sector in an efficient way, because they may deal with large datasets more efficiently than classical AI.\nAlzheimer's disease is a neurodegenerative brain disorder that mostly affects elderly people, causing important cognitive impairments. It is the most common cause of dementia and it has an effect on memory, thought, learning abilities and movement control. This type of disease has no cure, consequently an early diagnosis is fundamental for reducing its impact. The analysis of handwriting can be effective for diagnosing, as many researches have conjectured. The DARWIN (Diagnosis AlzheimeR WIth haNdwriting) dataset contains handwriting samples from people affected by Alzheimer's disease and a group of healthy people. Here we apply quantum AI to this use-case. In particular, we use this dataset to test kernel methods for classification task and compare their performances with the ones obtained via quantum machine learning methods. We find that quantum and classical algorithms achieve similar performances and in some cases quantum methods perform even better.\nOur results pave the way for future new quantum machine learning applications in early-screening diagnostics in the healthcare domain.",
        "citation_title": "Quantum AI for Alzheimer's disease early screening",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "From the quasisymmetry-group perspective [Phys. Rev. Lett. 126, 120604 (2021)], we show the universal existence of collective, coherent modes of excitations with small momenta in many-body scar models in the degenerate limit, where the energy spacing in the scar tower vanishes. The number of these modes, as well as the quantum numbers carried by them, are given, not by the symmetry of the Hamiltonian, but by the quasisymmetry of the scar tower: hence the name quasi-Goldstone modes. Based on this, we draw a concrete analogy between the paradigm of spontaneous symmetry breaking and the many-body scar physics in the degenerate limit.",
        "citation_title": "Quasi-Nambu-Goldstone modes in many-body scar models",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Recently Danielson, Satishchandran, and Wald (DSW) have shown that quantum superpositions held outside of Killing horizons will decohere at a steady rate. This occurs because of the inevitable radiation of soft photons (gravitons), which imprint a electromagnetic (gravitational) ``which-path'' memory onto the horizon. Rather than appealing to this global description, an experimenter ought to also have a local description for the cause of decoherence. One might intuitively guess that this is just the bombardment of Hawking/Unruh radiation on the system, however simple calculations challenge this idea -- the same superposition held in a finite temperature inertial laboratory does not decohere at the DSW rate. In this work we provide a local description of the decoherence by mapping the DSW set-up onto a worldline-localized model resembling an Unruh-DeWitt particle detector. We present an interpretation in terms of random local forces which do not sufficiently self-average over long times. Using the Rindler horizon as a concrete example we clarify the crucial role of temperature, and show that the Unruh effect is the only quantum mechanical effect underlying these random forces. A general lesson is that for an environment which induces Ohmic friction on the central system (as one gets from the classical Abraham-Lorentz-Dirac force, in an accelerating frame) the fluctuation-dissipation theorem implies that when this environment is at finite temperature it will cause steady decoherence on the central system. Our results agree with DSW and provide the complementary local perspective.",
        "citation_title": "Decoherence by warm horizons",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantum Federated Learning (QFL) is an emerging concept that aims to unfold federated learning (FL) over quantum networks, enabling collaborative quantum model training along with local data privacy. We explore the challenges of deploying QFL on cloud platforms, emphasizing quantum intricacies and platform limitations. The proposed data-encoding-driven QFL, with a proof of concept (GitHub Open Source) using genomic data sets on quantum simulators, shows promising results.",
        "citation_title": "Quantum Federated Learning Experiments in the Cloud with Data Encoding",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Quantum cloud computing is an emerging computing paradigm that allows seamless access to quantum hardware as cloud-based services. However, effective use of quantum resources is challenging and necessitates robust simulation frameworks for effective resource management design and evaluation. To address this need, we proposed QSimPy, a novel discrete-event simulation framework designed with the main focus of facilitating learning-centric approaches for quantum resource management problems in cloud environments. Underpinned by extensibility, compatibility, and reusability principles, QSimPy provides a lightweight simulation environment based on SimPy, a well-known Python-based simulation engine for modeling dynamics of quantum cloud resources and task operations. We integrate the Gymnasium environment into our framework to support the creation of simulated environments for developing and evaluating reinforcement learning-based techniques for optimizing quantum cloud resource management. The QSimPy framework encapsulates the operational intricacies of quantum cloud environments, supporting research in dynamic task allocation and optimization through DRL approaches. We also demonstrate the use of QSimPy in developing reinforcement learning policies for quantum task placement problems, demonstrating its potential as a useful framework for future quantum cloud research.",
        "citation_title": "QSimPy: A Learning-centric Simulation Framework for Quantum Cloud Resource Management",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We implanted Fe$^+$ ions in nanodiamond (ND) powder containing negatively charged nitrogen-vacancy (NV-) centers and studied their Raman spectra and optically detected magnetic resonance (ODMR) in various applied magnetic fields with green light (532 nm) excitation. In Raman spectra, we observed a blue shift of the NV$^-$ peak associated with the conversion of the electronic sp$^3$ configuration to the disordered sp$^2$ one typical for the carbon/graphite structure. In the ODMR spectra, we observed a red shift of the resonance position caused by local heating by an absorptive environment that recovers after annealing. To reveal the red shift mechanism in ODMR, we created a controlled absorptive environment around ND by adding iron-based Fe$_2$O$_3$ and graphitic sp$^2$ powders to the ND suspension. This admixture caused a substantial increase in the observed shift proportional to the applied laser power, corresponding to an increase in the local temperature by 150-180 K. This surprisingly large shift is absent in non-irradiated NV-ND powders, is associated only with the modification of the local temperature by the absorptive environment of NV-NDs and can be studied using ODMR signals of NV$^-$.",
        "citation_title": "Optically detected magnetic resonance study of thermal effects due to absorbing environment around nitrogen-vacancy-nanodiamond powders",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Tomography of single-particle-resolved detectors is of primary importance for characterizing particle correlations with applications in quantum metrology, quantum simulation and quantum computing. However, it is a non-trivial task in practice due to the unavoidable presence of noise that affects the measurement but does not originate from the detector. In this work, we address this problem for a three-dimensional single-atom-resolved detector where shot-to-shot atom number fluctuations are a central issue to perform a quantum detector tomography. We overcome this difficulty by exploiting the parallel measurement of counting statistics in sub-volumes of the detector, from which we evaluate the effect of shot-to-shot fluctuations and perform a local tomography of the detector. In addition, we illustrate the validity of our method from applying it to Gaussian quantum states with different number statistics. Finally, we show that the response of Micro-Channel Plate detectors is well-described from using a binomial distribution with the detection efficiency as a single parameter.",
        "citation_title": "Tomography of a single-atom-resolved detector in the presence of shot-to-shot number fluctuations",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose a spectroscopic probe of the breaking and localization of Cooper pairs in an atomic Fermi superfluid interacting with a Rydberg impurity. This is achieved by monitoring the formation of diatomic and triatomic ultralong-range molecular species in the superfluid across the BCS - Bose Einstein condensation (BEC) crossover. The triatomic Rydberg molecule in the BEC regime heralds the trapping of a tightly-bound Cooper pair, reminiscent of pion capture in nuclear matter, while the breaking of a Cooper pair on the BCS side by a diatomic Rydberg molecule is evocative of binary-star tidal disruption by a black hole. Spectroscopy of the Fermi superfluid and Rydberg molecules allows for an estimation of the Cooper-pair size while the Rydberg molecule binding energies discern many-body pairing effects.",
        "citation_title": "Breaking and trapping Cooper pairs by Rydberg-molecule spectroscopy in atomic Fermi superfluids",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we systematically investigate the inflationary complexity of the two-mode squeezed state with thermal effect for the single field inflation, modified dispersion relation, and non-trivial sound speed with the method of closed system and open system, respectively, which our analysis is valid for most inflationary models. First, the numeric of Krylov complexity in the method of the closed system indicates that the evolution of Krylov complexity highly depends on the squeezed angle parameter once taking the thermal effect into account, which will decay into some very tiny values, but the Krylov complexity will always enhance without thermal effect. For comparison, the numeric of circuit complexity shows that the evolution is always increasing no matter whether there are thermal effects or not which is independent of the evolution of squeezed angle parameter. By utilizing the method of open system, we first construct the wave function. As for the Krylov complexity with the method of open system, our investigations show the evolution of Krylov complexity will enhance upon some peaks factoring in the thermal effects. For completeness, we also calculate the Krylov entropy in the method of closed system and open system, which indicates that the hotter universe, the more chaotic the universe. Furthermore, our derivation for the Krylov complexity and Krylov entropy could nicely recover into the case of closed system under weak dissipative approximation, which confirms the validity of construction for the wave function. Finally, our numeric of Lanczos coefficient shows that the non-trivial sound speed has minimal chaos compared to the other two cases.",
        "citation_title": "Inflationary complexity of thermal state",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We formulate quantum computing solutions to a large class of dynamic nonlinear asset pricing models using algorithms, in theory exponentially more efficient than classical ones, which leverage the quantum properties of superposition and entanglement. The equilibrium asset pricing solution is a quantum state. We introduce quantum decision-theoretic foundations of ambiguity and model/parameter uncertainty to deal with model selection.",
        "citation_title": "On Quantum Ambiguity and Potential Exponential Computational Speed-Ups to Solving",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Calculation of observables with three-dimensional projected entangled pair states is generally hard, as it requires a contraction of complex multi-layer tensor networks. We utilize the multi-layer structure of these tensor networks to largely simplify the contraction. The proposed approach involves the usage of the layer structure both to simplify the search for the boundary projected entangled pair states and the single-layer mapping of the final corner transfer matrix renormalization group contraction. We benchmark our results on the cubic lattice Heisenberg model, reaching the bond dimension D = 7, and find a good agreement with the previous results.",
        "citation_title": "Single-layer tensor network approach for three-dimensional quantum systems",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We propose a theoretical scheme for a non-Hermitian atomic grating within an ultra-cold rubidium-87 ($^{87}Rb$) atomic ensemble. The grating's diffraction properties depend on the polarization states of incident photons and are controlled non-locally through Rydberg interactions. Multiple types of polarization-dependent diffraction modes are generated, benefiting from no crosstalk atomic transition channels based on transition selection rules. Those polarization-dependent diffraction modes can be switched using dynamic optical pulse trains, exploiting the Rydberg blockade effect, and are tunable by non-Hermitian optical modulation. Our work will advance the application of asymmetric optical scattering by utilizing the polarization degree of freedom within continuous media and benefit the application of versatile non-Hermitian/asymmetric optical devices.",
        "citation_title": "Polarization dependent non-Hermitian atomic grating controlled by dipole blockade effect",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We observe an inverse turbulent-wave cascade, from small to large lengthscales, in a homogeneous 2D Bose gas driven isotropically on a lengthscale much smaller than its size. Starting with an equilibrium condensed gas, at long drive times we observe a nonthermal steady state. At increasing lengthscales, starting from the forcing one, the steady-state momentum distribution features in turn: (i) a power-law spectrum, with an exponent close to the analytical result for a particle cascade in weak-wave turbulence, and (ii) a spectrum intriguingly reminiscent of a nonthermal fixed point associated with universal coarsening in an isolated 2D gas. In further experiments, based on anisotropic driving, we also reveal the qualitative picture of the cascade-formation dynamics.",
        "citation_title": "Observation of an inverse turbulent-wave cascade in a driven quantum gas",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study a family of structure-preserving deterministic numerical schemes for Lindblad equations. This family of schemes has a simple form and can systemically achieve arbitrary high-order accuracy in theory. Moreover, these schemes can also overcome the non-physical issues that arise from many traditional numerical schemes. Due to their preservation of physical nature, these schemes can be straightforwardly used as backbones for further developing randomized and quantum algorithms in simulating Lindblad equations. In this work, we systematically study these methods and perform a detailed error analysis, which is validated through numerical examples.",
        "citation_title": "Structure-preserving numerical schemes for Lindblad equations",
        "date_delivered": "[Submitted on 1 Mar 2021 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We prove that the relative entropy of entanglement is additive when \\emph{at least one of the two states} belongs to some specific class. We show that these classes include bipartite pure, maximally correlated, GHZ, Bell diagonal, isotropic, and generalized Dicke states. Previously, additivity was established only if \\textit{both} states belong to the same class. Moreover, we extend these results to entanglement monotones based on the $\\alpha$-$z$ R\u00e9nyi relative entropy. Notably, this family of monotones includes also the generalized robustness of entanglement and the geometric measure of entanglement. In addition, we prove that any monotone based on a quantum relative entropy is not additive for general states. We also compute closed-form expressions of the monotones for bipartite pure, Bell diagonal, isotropic, generalized Werner, generalized Dicke, and maximally correlated Bell diagonal states. Our results rely on developing a method that allows us to recast the initial convex optimization problem into a simpler linear one. Even though we mostly focus on entanglement theory, we expect that some of our technical results could be useful in investigating more general convex optimization problems.",
        "citation_title": "New additivity properties of the relative entropy of entanglement and its generalizations",
        "date_delivered": "[Submitted on 23 Nov 2022 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "The purpose of unitary synthesis is to find a gate sequence that optimally approximates a target unitary transformation. A new synthesis approach, called probabilistic synthesis, has been introduced, and its superiority has been demonstrated over traditional deterministic approaches with respect to approximation error and gate length. However, the optimality of current probabilistic synthesis algorithms is unknown. We obtain the tight lower bound on the approximation error obtained by the optimal probabilistic synthesis, which guarantees the sub-optimality of current algorithms. We also show its tight upper bound, which improves and unifies current upper bounds depending on the class of target unitaries. These two bounds reveal the fundamental relationship of approximation error between probabilistic approximation and deterministic approximation of unitary transformations. From a computational point of view, we show that the optimal probability distribution can be computed by the semidefinite program (SDP) we construct. We also construct an efficient probabilistic synthesis algorithm for single-qubit unitaries, rigorously estimate its time complexity, and show that it reduces the approximation error quadratically compared with deterministic algorithms.",
        "citation_title": "Probabilistic unitary synthesis with optimal accuracy",
        "date_delivered": "[Submitted on 16 Jan 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We consider two non-relativistic quantum clocks interacting with a Newtonian gravitational field produced by a spherical mass. In the framework of Page and Wootters approach, we derive a time dilation for the time states of the clocks. The delay is in agreement up to first order with the gravitational time dilation obtained from the Schwarzschild metric. This result can be extended by considering the relativistic gravitational potential: in this case we obtain the agreement with the exact Schwarzschild solution.",
        "citation_title": "Time dilation of quantum clocks in a Newtonian gravitational field",
        "date_delivered": "[Submitted on 9 Apr 2023 (v1), last revised 1 May 2024 (this version, v4)]"
    },
    {
        "abstract": "The frictionless, directional propagation of particles at the boundary of topological materials is one of the most striking phenomena in transport. These chiral edge modes lie at the heart of the integer and fractional quantum Hall effects, and their extraordinary robustness against noise and disorder reflects the quantization of Hall conductivity in these systems. Despite their central importance, controllable injection of edge modes, and direct imaging of their propagation, structure, and dynamics, is challenging. Here, we demonstrate the distillation of individual chiral edge states in a rapidly-rotating bosonic superfluid confined by an optical boundary. Tuning the wall sharpness, we reveal the smooth crossover between soft wall behaviour in which the propagation speed is proportional to wall steepness, and the hard wall regime exhibiting chiral free particles. From the skipping motion of atoms along the boundary, we spectroscopically infer the energy gap between the ground and first excited edge bands, and reveal its evolution from the bulk Landau level splitting for a soft boundary, to the hard wall limit.",
        "citation_title": "Observation of chiral edge transport in a rapidly-rotating quantum gas",
        "date_delivered": "[Submitted on 20 Apr 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Fast measurements of quantum devices is important in areas such as quantum sensing, quantum computing and nanodevice quality analysis. Here, we develop a superconductor-semiconductor multi-module microwave assembly to demonstrate charge state readout at the state-of-the-art. The assembly consist of a superconducting readout resonator interfaced to a silicon-on-insulator (SOI) chiplet containing quantum dots (QDs) in a high-$\\kappa$ nanowire transistor. The superconducting chiplet contains resonant and coupling elements as well as $LC$ filters that, when interfaced with the silicon chip, result in a resonant frequency $f=2.12$ GHz, a loaded quality factor $Q=850$, and a resonator impedance $Z=470$ $\\Omega$. Combined with the large gate lever arms of SOI technology, we achieve a minimum integration time for single and double QD transitions of 2.77 ns and 13.5 ns, respectively. We utilize the assembly to measure charge noise over 9 decades of frequency up to 500 kHz and find a 1/$f$ dependence across the whole frequency spectrum as well as a charge noise level of 4 $\\mu$eV/$\\sqrt{\\text{Hz}}$ at 1 Hz. The modular microwave circuitry presented here can be directly utilized in conjunction with other quantum device to improve the readout performance as well as enable large bandwidth noise spectroscopy, all without the complexity of superconductor-semiconductor monolithic fabrication.",
        "citation_title": "Multi-module microwave assembly for fast read-out and charge noise characterization of silicon quantum dots",
        "date_delivered": "[Submitted on 26 Apr 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We propose a setting that simulates Hawking radiation from an analogue bouncing geometry, i.e., a collapsing geometry that reverts its collapse after a finite time, in a setup consisting of a coplanar waveguide terminated in superconducting quantum-interference devices at both ends. We demonstrate experimental feasibility of the proposed setup within the current technology. Our analysis illustrates the resilience of Hawking radiation under changes in the physics at energy scales much larger than the temperature, supporting the idea that regular alternatives to black holes would also emit Hawking radiation.",
        "citation_title": "Hawking radiation from an analogue bouncing geometry",
        "date_delivered": "[Submitted on 8 Jun 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The purpose of this paper is to study the dynamics of a coherent feedback network where two two-level atoms are coupled with a semi-infinite waveguide. In this set-up, the two-level atoms can work as the photon source, and the photons can be emitted into the waveguide via the nonchiral or chiral couplings between the atom and the waveguide, according to whether the coupling strengths between the atoms and different directional propagating modes in the waveguide are identical or not. For the photon emitted by one of the two atoms, it can be reflected by the terminal mirror, or interact with the other atom, and then the photon can re-interact with the former atom. When the two atoms are both initially excited, finally there can be two-photon, one-photon or zero-photon states in the waveguide via the spontaneous emission and feedback interactions, and this is influenced by the locations of the atoms and the chirality of the coupling between the atom and the waveguide. Similarly, if only one of the two atoms is initially excited, there can be zero or one photon in the waveguide. Thus we can control the number of the photons in the waveguide and the atomic states by tuning the feedback loop length and the chiral couplings between the atom and waveguide. The photonic state in the waveguide is analyzed in the frequency domain and the spatial domain, and the transient process of photon emissions can be better understood based on the comprehensive analysis in these two domains.",
        "citation_title": "Quantum feedback control of a two-atom network closed by a semi-infinite waveguide",
        "date_delivered": "[Submitted on 10 Jun 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We consider a one-dimensional system of non-interacting fermions featuring both boundary driving and continuous monitoring of the bulk particle density. Due to the measurements, the expectation values of the local density and current operators are random variables whose average behavior is described by a well studied Lindblad master equation. By means of exact numerical computations, we go beyond the averaged dynamics and study their full probability distribution functions, focusing on the late-time stationary regime. We find that, contrary to the averaged values, the spatial profiles of the median density and current are non-trivial, exhibiting qualitative differences as a function of the monitoring strength. At weak monitoring, the medians are close to the means, displaying diffusive spatial profiles. At strong monitoring, we find that the median density and current develop a domain-wall and single-peak profile, respectively, which are suggestive of a Zeno-like localization in typical quantum trajectories. While we are not able to identify a sharp phase transition as a function of the monitoring rate, our work highlights the usefulness of characterizing typical behavior beyond the averaged values in the context of monitored many-body quantum dynamics.",
        "citation_title": "Density and current statistics in boundary-driven monitored fermionic chains",
        "date_delivered": "[Submitted on 16 Jun 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Hypergraph product codes are a promising avenue to achieving fault-tolerant quantum computation with constant overhead. When embedding these and other constant-rate qLDPC codes into 2D, a significant number of nonlocal connections are required, posing difficulties for some quantum computing architectures. In this work, we introduce a fault-tolerance scheme that aims to alleviate the effects of implementing this nonlocality by measuring generators acting on spatially distant qubits less frequently than those which do not. We investigate the performance of a simplified version of this scheme, where the measured generators are randomly selected. When applied to hypergraph product codes and a modified small-set-flip decoding algorithm, we prove that for a sufficiently high percentage of generators being measured, a threshold still exists. We also find numerical evidence that the logical error rate is exponentially suppressed even when a large constant fraction of generators are not measured.",
        "citation_title": "Partial Syndrome Measurement for Hypergraph Product Codes",
        "date_delivered": "[Submitted on 29 Jun 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We investigate the gauging of a $\\mathbb{Z}_2$ symmetry in Narain conformal field theories (CFTs) constructed from qudit stabilizer codes. Considering both orbifold and fermionization, we establish a connection between $\\mathbb{Z}_2$ gauging procedures and modifications of the momentum lattice by vectors characterizing the $\\mathbb{Z}_2$ symmetry. We also provide three-dimensional interpretations of $\\mathbb{Z}_2$ gaugings through abelian Chern-Simons theories, which act as symmetry topological field theories.",
        "citation_title": "Narain CFTs from quantum codes and their $\\mathbb{Z}_2$ gauging",
        "date_delivered": "[Submitted on 3 Aug 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We develop a hardware-efficient ansatz for variational optimization, derived from existing ansatze in the literature, that parametrizes subsets of all interactions in the Cost Hamiltonian in each layer. We treat gate orderings as a variational parameter and observe that doing so can provide significant performance boosts in experiments. We carried out experimental runs of a compilation-optimized implementation of fully-connected Sherrington-Kirkpatrick Hamiltonians on a 50-qubit linear-chain subsystem of Rigetti Aspen-M-3 transmon processor. Our results indicate that, for the best circuit designs tested, the average performance at optimized angles and gate orderings increases with circuit depth (using more parameters), despite the presence of a high level of noise. We report performance significantly better than using a random guess oracle for circuits involving up to approx 5000 two-qubit and approx 5000 one-qubit native gates. We additionally discuss various takeaways of our results toward more effective utilization of current and future quantum processors for optimization.",
        "citation_title": "Design and execution of quantum circuits using tens of superconducting qubits and thousands of gates for dense Ising optimization problems",
        "date_delivered": "[Submitted on 18 Aug 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Systems with conserved dipole moment have drawn considerable interest in light of their realization in recent experiments on tilted optical lattices. An important question for such systems is delineating the conditions under which they admit a unique gapped ground state that is consistent with all symmetries. Here, we study one-dimensional translation-invariant lattices that conserve U(1) charge and $\\mathbb{Z}_L$ dipole moment, where discreteness of the dipole symmetry is enforced by periodic boundary conditions, with $L$ the system size. We show that in these systems, a symmetric, gapped, and non-degenerate ground state requires not only integer charge filling, but also a fixed value of the dipole filling, while other fractional dipole fillings enforce either a gapless or symmetry-breaking ground state. In contrast with prior results in the literature, we find that the dipole filling constraint depends both on the charge filling as well as the system size, emphasizing the subtle interplay of dipole symmetry with boundary conditions. We support our results with numerical simulations and exact results.",
        "citation_title": "Filling constraints on translation invariant dipole conserving systems",
        "date_delivered": "[Submitted on 30 Aug 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Cavity-electromechanical systems are extensively used for sensing and controlling the vibrations of mechanical resonators down to their quantum limit. The nonlinear radiation-pressure interaction in these systems could result in an unstable response of the mechanical resonator showing features such as frequency-combs, period-doubling bifurcations and chaos. However, due to weak light-matter interaction, typically these effects appear at very high driving strengths. By using polariton modes formed by a strongly coupled flux-tunable transmon and a microwave cavity, here we demonstrate an electromechanical device and achieve a single-photon coupling rate $g_0/2\\pi$ of $160~$kHz, which is nearly 4\\% of the mechanical frequency $\\omega_m$. Due to large $g_0/\\omega_m$ ratio, the device shows an unstable mechanical response resulting in frequency combs in sub-single photon limit. We systematically investigate the boundary of the unstable response and identify two important regimes governed by the optomechanical backaction and the nonlinearity of the electromagnetic mode. Such an improvement in the single-photon coupling rate and the observations of microwave frequency combs at single-photon levels may have applications in the quantum control of the motional states and critical parametric sensing. Our experiments strongly suggest the requirement of newer approaches to understand instabilities.",
        "citation_title": "Single-photon induced instabilities in a cavity electromechanical device",
        "date_delivered": "[Submitted on 13 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We present a quantum sensing scheme achieving the ultimate quantum sensitivity in the estimation of the transverse displacement between two photons interfering at a balanced beam splitter, based on transverse-momentum sampling measurements at the output. This scheme can possibly lead to enhanced high-precision nanoscopic techniques, such as super-resolved single-molecule localization microscopy with quantum dots, by circumventing the requirements in standard direct imaging of cameras resolution at the diffraction limit, and of highly magnifying objectives. Interestingly, we show that our interferometric technique achieves the ultimate spatial precision in nature irrespectively of the overlap of the two displaced photonic wavepackets, while its precision is only reduced of a constant factor for photons differing in any non-spatial degrees of freedom. This opens a new research paradigm based on the interface between spatially resolved quantum interference and quantum-enhanced spatial sensitivity.",
        "citation_title": "Estimation with ultimate quantum precision of the transverse displacement between two photons via two-photon interference sampling measurements",
        "date_delivered": "[Submitted on 13 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The role of CP-indivisibility and incompatibility as valuable resources for various information-theoretic tasks is widely acknowledged. This study delves into the intricate relationship between CP-divisibility and channel compatibility. Our investigation focuses on the behaviour of incompatibility robustness of quantum channels for a pair of generic dynamical maps. We show that the incompatibility robustness of channels is monotonically non-increasing for a pair of generic CP-divisible dynamical maps. Further, our explicit study of the behaviour of incompatibility robustness with time for some specific dynamical maps reveals non-monotonic behaviour in the CP-indivisible regime. Additionally, we propose a measure of CP-indivisibility based on the incompatibility robustness of quantum channels. Our investigation provides valuable insights into the nature of quantum dynamical maps and their relevance in information-theoretic applications.",
        "citation_title": "Relating CP divisibility of dynamical maps with compatibility of channels",
        "date_delivered": "[Submitted on 19 Sep 2023 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Quantum-clock interferometry has been suggested as a quantum probe to test the universality of free fall (UFF) and the universality of gravitational redshift (UGR). In typical experimental schemes it seems advantageous to employ Doppler-free E1-M1 transitions which have so far been investigated in quantum gases at rest. Here, we consider the fully quantized atomic degrees of freedom and study the interplay of the quantum center-of-mass (COM) $-$ that can become delocalized $-$ together with the internal clock transitions. In particular, we derive a model for finite-time E1-M1 transitions with atomic intern-extern coupling and arbitrary position-dependent laser intensities. We further provide generalizations to the ideal expressions for perturbed recoilless clock pulses. Finally, we show at the example of a Gaussian laser beam that the proposed quantum-clock interferometers are stable against perturbations from varying optical fields for a sufficiently small quantum delocalization of the atomic COM.",
        "citation_title": "Finite Pulse-Time Effects in Long-Baseline Quantum Clock Interferometry",
        "date_delivered": "[Submitted on 25 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We demonstrate qubit state measurements assisted by a supervised convolutional neural network (CNN) in a neutral atom quantum processor. We present two CNN architectures for analyzing neutral atom qubit readout data: a compact 5-layer single-qubit CNN architecture and a 6-layer multi-qubit CNN architecture. We benchmark both architectures against a conventional Gaussian threshold analysis method. In a sparse array (9 {\\mu}m atom separation) which experiences negligible crosstalk, we observed up to 32% and 56% error reduction for the multi-qubit and single-qubit architectures respectively, as compared to the benchmark. In a tightly spaced array (5 {\\mu}m atom separation), which suffers from readout crosstalk, we observed up to 43% and 32% error reduction in the multi-qubit and single-qubit CNN architectures respectively, as compared to the benchmark. By examining the correlation between the predicted states of neighboring qubits, we found that the multi-qubit CNN architecture reduces the crosstalk correlation up to 78.5%. This work demonstrates a proof of concept for a CNN network to be implemented as a real-time readout processing method on a neutral atom quantum computer, enabling faster readout time and improved fidelity.",
        "citation_title": "Enhanced Measurement of Neutral Atom Qubits with Machine Learning",
        "date_delivered": "[Submitted on 20 Nov 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The neutral atom array has gained prominence in quantum computing for its scalability and operation fidelity. Previous works focus on fixed atom arrays (FAAs) that require extensive SWAP operations for long-range interactions. This work explores a novel architecture reconfigurable atom arrays (RAAs), also known as field programmable qubit arrays (FPQAs), which allows for coherent atom movements during circuit execution under some constraints. Such atom movements, which are unique to this architecture, could reduce the cost of long-range interactions significantly if the atom movements could be scheduled strategically.\nIn this work, we introduce Atomique, a compilation framework designed for qubit mapping, atom movement, and gate scheduling for RAA. Atomique contains a qubit-array mapper to decide the coarse-grained mapping of the qubits to arrays, leveraging MAX k-Cut on a constructed gate frequency graph to minimize SWAP overhead. Subsequently, a qubit-atom mapper determines the fine-grained mapping of qubits to specific atoms in the array and considers load balance to prevent hardware constraint violations. We further propose a router that identifies parallel gates, schedules them simultaneously, and reduces depth. We evaluate Atomique across 20+ diverse benchmarks, including generic circuits (arbitrary, QASMBench, SupermarQ), quantum simulation, and QAOA circuits. Atomique consistently outperforms IBM Superconducting, FAA with long-range gates, and FAA with rectangular and triangular topologies, achieving significant reductions in depth and the number of two-qubit gates.",
        "citation_title": "Atomique: A Quantum Compiler for Reconfigurable Neutral Atom Arrays",
        "date_delivered": "[Submitted on 25 Nov 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We study the SYK model -- an important toy model for quantum gravity on IBM's superconducting qubit quantum computers. By using a graph-coloring algorithm to minimize the number of commuting clusters of terms in the qubitized Hamiltonian, we find the gate complexity of the time evolution using the first-order product formula for $N$ Majorana fermions is $\\mathcal{O}(N^5 J^{2}t^2/\\epsilon)$ where $J$ is the dimensionful coupling parameter, $t$ is the evolution time, and $\\epsilon$ is the desired precision. With this improved resource requirement, we perform the time evolution for $N=6, 8$ with maximum two-qubit circuit depth of 343. We perform different error mitigation schemes on the noisy hardware results and find good agreement with the exact diagonalization results on classical computers and noiseless simulators. In particular, we compute return probability after time $t$ and out-of-time order correlators (OTOC) which is a standard observable of quantifying the chaotic nature of quantum systems.",
        "citation_title": "Sachdev-Ye-Kitaev model on a noisy quantum computer",
        "date_delivered": "[Submitted on 29 Nov 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "We show that marginals of blocks of $t$ systems of any finitely correlated translation invariant state on a chain can be learned, in trace distance, with $O(t^2)$ copies -- with an explicit dependence on local dimension, memory dimension and spectral properties of a certain map constructed from the state -- and computational complexity polynomial in $t$. The algorithm requires only the estimation of a marginal of a controlled size, in the worst case bounded by the minimum bond dimension, from which it reconstructs a translation invariant matrix product operator. In the analysis, a central role is played by the theory of operator systems. A refined error bound can be proven for $C^*$-finitely correlated states, which have an operational interpretation in terms of sequential quantum channels applied to the memory system. We can also obtain an analogous error bound for a class of matrix product density operators reconstructible by local marginals. In this case, a linear number of marginals must be estimated, obtaining a sample complexity of $\\tilde{O}(t^3)$. The learning algorithm also works for states that are only close to a finitely correlated state, with the potential of providing competitive algorithms for other interesting families of states.",
        "citation_title": "Learning finitely correlated states: stability of the spectral reconstruction",
        "date_delivered": "[Submitted on 12 Dec 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Quantum measurements are a fundamental component of quantum computing. However, on modern-day quantum computers, measurements can be more error prone than quantum gates, and are susceptible to non-unital errors as well as non-local correlations due to measurement crosstalk. While readout errors can be mitigated in post-processing, it is inefficient in the number of qubits due to a combinatorially-large number of possible states that need to be characterized. In this work, we show that measurement errors can be tailored into a simple stochastic error model using randomized compiling, enabling the efficient mitigation of readout errors via quasi-probability distributions reconstructed from the measurement of a single preparation state in an exponentially large confusion matrix. We demonstrate the scalability and power of this approach by correcting readout errors without matrix inversion on a large number of different preparation states applied to a register of eight superconducting transmon qubits. Moreover, we show that this method can be extended to mid-circuit measurements used for active feedback via quasi-probabilistic error cancellation, and demonstrate the correction of measurement errors on an ancilla qubit used to detect and actively correct bit-flip errors on an entangled memory qubit. Our approach enables the correction of readout errors on large numbers of qubits, and offers a strategy for correcting readout errors in adaptive circuits in which the results of mid-circuit measurements are used to perform conditional operations on non-local qubits in real time.",
        "citation_title": "Quasi-Probabilistic Readout Correction of Mid-Circuit Measurements for Adaptive Feedback via Measurement Randomized Compiling",
        "date_delivered": "[Submitted on 21 Dec 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "This work proposes a protocol for Fermionic Hamiltonian learning. For the Hubbard model defined on a bounded-degree graph, the Heisenberg-limited scaling is achieved while allowing for state preparation and measurement errors. To achieve $\\epsilon$-accurate estimation for all parameters, only $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ total evolution time is needed, and the constant factor is independent of the system size. Moreover, our method only involves simple one or two-site Fermionic manipulations, which is desirable for experiment implementation.",
        "citation_title": "Quantum Hamiltonian Learning for the Fermi-Hubbard Model",
        "date_delivered": "[Submitted on 28 Dec 2023 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Emitters in high refractive index materials like 4H-SiC suffer from reduced detection of photons because of losses caused by total internal reflection. Thus, integration into efficient nanophotonic structures which couple the emission of photons to a well defined waveguide mode can significantly enhance the photon detection efficiency. In addition, interfacing this waveguide to a classical fiber network is of similar importance to detect the photons and perform experiments. Here, we show a waveguide fiber interface in SiC. By careful measurements we determine efficiencies exceeding 93 % for the transfer of photons from SiC nanobeams to fibers. We use this interface to create a bright single photon source based on waveguide integrated V2 defects in 4H-SiC and achieve an overall photon count rate of 181 kilo-counts per second. We observe and quantify the strain induced shift of the ground state spin states and demonstrate coherent control of the electron spin with a coherence time of T2=42.5 $\\rm\\mu$s.",
        "citation_title": "Precise characterization of a silicon carbide waveguide fiber interface",
        "date_delivered": "[Submitted on 11 Jan 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "This work considers the non-interactive source simulation problem (NISS). In the standard NISS scenario, a pair of distributed agents, Alice and Bob, observe a distributed binary memoryless source $(X^d,Y^d)$ generated based on joint distribution $P_{X,Y}$. The agents wish to produce a pair of discrete random variables $(U_d,V_d)$ with joint distribution $P_{U_d,V_d}$, such that $P_{U_d,V_d}$ converges in total variation distance to a target distribution $Q_{U,V}$. Two variations of the standard NISS scenario are considered. In the first variation, in addition to $(X^d,Y^d)$ the agents have access to a shared Bell state. The agents each measure their respective state, using a measurement of their choice, and use its classical output along with $(X^d,Y^d)$ to simulate the target distribution. This scenario is called the entanglement-assisted NISS (EA-NISS). In the second variation, the agents have access to a classical common random bit $Z$, in addition to $(X^d,Y^d)$. This scenario is called the classical common randomness NISS (CR-NISS). It is shown that for binary-output NISS scenarios, the set of feasible distributions for EA-NISS and CR-NISS are equal with each other. Hence, there is not quantum advantage in these EA-NISS scenarios. For non-binary output NISS scenarios, it is shown through an example that there are distributions that are feasible in EA-NISS but not in CR-NISS. This shows that there is a quantum advantage in non-binary output EA-NISS.",
        "citation_title": "Quantum Advantage in Non-Interactive Source Simulation",
        "date_delivered": "[Submitted on 31 Jan 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The nonincreasing feature of temporal quantum steering under a completely positive trace-preserving (CPTP) map, as proposed by Chen, et al. in Phys. Rev. Lett. 116, 020503 (2016), has been considered as a practical measure of non-Markovianity. In this paper, we utilize an all-optical scheme to simulate a non-Markovian collision model and to examine how Gaussian steering can be used as a tool for quantifying the non-Markovianity of a structured continuous variable (CV) Gaussian channel. By modifying the reflectivity of the beam splitters (BSs), we are able to tune the degree of non-Markovianity of the channel. After analyzing the non-Markovian degree of the dissipative channel within two steering scenarios, we discovered that the Gaussian steering-based non-Markovian measure depends the specific scenario because of the asymmetry of Gaussian steering. We also compared the Gaussian steering based non-Markovianity to the one based on the violation of the divisibility of CPTP map.",
        "citation_title": "Witnessing non-Markovianity with Gaussian quantum steering in collision model",
        "date_delivered": "[Submitted on 1 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We investigate classes of interacting quantum spin systems in a single-mode cavity with a Dicke coupling, as a paradigmatic example of strongly correlated light-matter systems. Coming from the limit of weak light-matter couplings and large number of matter entities, we map the relevant low-energy sector of a broad class of models in the non-superradiant phases onto the exactly solvable Dicke model. We apply the outcomes to the Dicke-Ising model as a paradigmatic example, in agreement with results obtained by mean-field theory. We further accompany and verify our findings with finite-size calculations, using exact diagonalization and the series expansion method pcst++.",
        "citation_title": "(Almost) Everything is a Dicke model -- Mapping non-superradiant correlated light-matter systems to the exactly solvable Dicke model",
        "date_delivered": "[Submitted on 23 Feb 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Nonlinear quantum photonics serves as a cornerstone in photonic quantum technologies, such as universal quantum computing and quantum communications. The emergence of integrated photonics platform not only offers the advantage of large-scale manufacturing but also provides a variety of engineering methods. Given the complexity of integrated photonics engineering, a comprehensive simulation framework is essential to fully harness the potential of the platform. In this context, we introduce a nonlinear quantum photonics simulation framework which can accurately model a variety of features such as adiabatic waveguide, material anisotropy, linear optics components, photon losses, and detectors. Furthermore, utilizing the framework, we have developed a device scheme, chip-scale temporal walk-off compensation, that is useful for various quantum information processing tasks. Applying the simulation framework, we show that the proposed device scheme can enhance the squeezing parameter of photon-pair sources and the conversion efficiency of quantum frequency converters without relying on higher pump power.",
        "citation_title": "Simulation framework for integrated nonlinear quantum photonics",
        "date_delivered": "[Submitted on 29 Feb 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The observation that free electrons can interact coherently with quantized electromagnetic fields and matter systems has led to a plethora of proposals leveraging the unique quantum properties of free electrons. At the heart of these proposals lies the assumption of a strong quantum interaction between a flying free electron and a photonic mode. However, existing schemes are intrinsically limited by electron diffraction, which puts an upper bound on the interaction length and therefore the quantum coupling strength. Here, we propose the use of \"free-electron fibers'': effectively one-dimensional photonic systems where free electrons co-propagate with two guided modes. The first mode applies a ponderomotive trap to the free electron, effectively lifting the limitations due to electron diffraction. The second mode strongly couples to the guided free electron, with an enhanced coupling that is orders of magnitude larger than previous designs. Moreover, the extended interaction lengths enabled by our scheme allows for strong single-photon nonlinearities mediated by free electrons. We predict a few interesting observable quantum effects in our system, such as deterministic single-photon emission and complex, nonlinear multimode dynamics. Our proposal paves the way towards the realization of many anticipated effects in free-electron quantum optics, such as non-Gaussian light generation, deterministic single photon emission, and quantum gates controlled by free-electron--photon interactions.",
        "citation_title": "Strong coupling and single-photon nonlinearity in free-electron quantum optics",
        "date_delivered": "[Submitted on 19 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We establish the concept of topological pumping in one-dimensional systems with long-range interactions and apply it to the transport of a photon in quantum optical systems. In our theoretical investigation, we introduce an extended version of the Rice-Mele model with all-to-all exchange interactions. By analyzing its properties, we identify the general conditions for topological pumping and demonstrate the topologically protected and dispersionless transport of a photon on a one-dimensional emitter chain. As concrete examples, we investigate three different popular quantum optics platforms, namely Rydberg atom lattices, dense lattices of atoms excited to low-lying electronic states, and atoms coupled to waveguides, using experimentally relevant parameters. We observe that despite the long-ranged character of the dipole-dipole interactions, topological pumping facilitates the transport of a photon with a fidelity per cycle which can reach 99.9%. Moreover, we find that the photon pumping process remains topologically protected against local disorder in the coupling rates.",
        "citation_title": "Topological photon pumping in quantum optical systems",
        "date_delivered": "[Submitted on 8 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We introduce two quantitative measures of the strength of causal relations in general physical theories. These two measures called the maximum and minimum causal effect, capture the maximum and minimum changes in a physical system induced by changes in another system. In quantum theory, we show that both measures possess important properties, such as continuity and faithfulness, and can be evaluated through optimization over orthogonal pairs of input states. For the maximum causal effect, we provide numerical lower bounds based on a variational algorithm, which can be used to estimate the strength of causal relations without performing a full quantum process tomography. To illustrate our algorithm, we analyze two paradigmatic examples, the first involving a coherent quantum superposition of direct cause and common cause, and the second involving communication through a coherent quantum superposition of two completely depolarizing channels.",
        "citation_title": "Maximum and minimum causal effects of physical processes",
        "date_delivered": "[Submitted on 11 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We study the deep multi-scale entanglement renormalization ansatz (DMERA) on quantum hardware and the causal cone of a subset of the qubits which make up the ansatz. This causal cone spans $O(M+\\log{N})$ physical qubits on a quantum device, where $M$ and $N$ are the subset size and the total number qubits in the ansatz respectively. This allows for the determination of the von Neumann entanglement entropy of the $N$ qubit wave-function using $O(M+\\log{N})$ qubits by diagonalization of the reduced density matrix (RDM). We show this by randomly initializing a 16-qubit DMERA and diagonalizing the resulting RDM of the $M$-qubit subsystem using density matrix simulation. As an example of practical interest, we also encode the variational ground state of the quantum critical long-range transverse field Ising model (LRTIM) on 8 spins using DMERA. We perform density matrix simulation with and without noise to obtain entanglement entropies in separate experiments using only 4 qubits. Finally we repeat the experiment on the IBM Kyoto backend reproducing simulation results.",
        "citation_title": "Qubit frugal entanglement determination with the deep multi-scale entanglement renormalization ansatz",
        "date_delivered": "[Submitted on 12 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The quantum phase estimation algorithm stands as the primary method for determining the ground state energy of a molecular electronic Hamiltonian on a quantum computer. In this context, the ability to initialize a classically tractable state that has a strong overlap with the desired ground state is critical as it directly affects the runtime of the algorithm. However, several numerical studies have shown that this overlap decays exponentially with system size. In this work, we demonstrate that this decay can be alleviated by optimizing the molecular orbital basis, for an initial state constructed from a single Slater determinant. We propose a practical method to achieve this optimization without knowledge of the true molecular ground state and test this method numerically. By comparing the resulting optimized orbitals to the natural orbitals, we find improved overlap. Specifically, for four iron-sulfur molecules, which are known to suffer from the mentioned decay, we show that our method yields one to two orders of magnitude improvement compared to localized molecular orbitals.",
        "citation_title": "Enhancing initial state overlap through orbital optimization for faster molecular electronic ground-state energy estimation",
        "date_delivered": "[Submitted on 12 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "A (2+1)D topological ordered phase with U(1) symmetry may or may not have a symmetric gapped edge state, even if both thermal and electric Hall conductivity are vanishing. It is recently discovered that there are \"higher\" versions of Hall conductivity valid for fermionic fractional quantum Hall (FQH) states, which obstructs symmetry-preserving gapped edge state beyond thermal and electric Hall conductivity. In this paper, we show that one can extract higher Hall conductivity from a single wave function of an FQH state, by evaluating the expectation value of the \"partial rotation\" unitary which is a combination of partial spatial rotation and a U(1) phase rotation. This result is verified numerically with the fermionic Laughlin state with $\\nu=1/3$, $1/5$, as well as the non-Abelian Moore-Read state. Together with topological entanglement entropy, we prove that the expectation values of the partial rotation completely determines if a bosonic/fermionic Abelian topological order with U(1) symmetry has a symmetry-preserving gappable edge state or not. We also show that thermal and electric Hall conductivity of Abelian topological order can be extracted by partial rotations. Even in non-Abelian FQH states, partial rotation provides the Lieb-Schultz-Mattis type theorem constraining the low-energy spectrum of the bulk-boundary system. The generalization of higher Hall conductivity to the case with Lie group symmetry is also presented.",
        "citation_title": "Higher Hall conductivity from a single wave function: Obstructions to symmetry-preserving gapped edge of (2+1)D topological order",
        "date_delivered": "[Submitted on 16 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Diagonalization of matrix pencils provide a uniform technique to transcribe operator-valued violations of Boole's `conditions of possible experience' involving multipartite correlations into contextuality. They also provide structural analysis of the contexts involved, and thereby suggest compact forms of deviations of quantized systems from classical predictions.",
        "citation_title": "Converting nonlocality into contextuality",
        "date_delivered": "[Submitted on 24 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The Kerr-cat qubit is a bosonic qubit in which multi-photon Schrodinger cat states are stabilized by applying a two-photon drive to an oscillator with a Kerr nonlinearity. The suppressed bit-flip rate with increasing cat size makes this qubit a promising candidate to implement quantum error correction codes tailored for noise-biased qubits. However, achieving strong light-matter interactions necessary for stabilizing and controlling this qubit has traditionally required strong microwave drives that heat the qubit and degrade its performance. In contrast, increasing the coupling to the drive port removes the need for strong drives at the expense of large Purcell decay. By integrating an effective band-block filter on-chip, we overcome this trade-off and realize a Kerr-cat qubit in a scalable 2D superconducting circuit with high coherence. This filter provides 30 dB of isolation at the qubit frequency with negligible attenuation at the frequencies required for stabilization and readout. We experimentally demonstrate quantum non-demolition readout fidelity of 99.6% for a cat with 8 photons. Also, to have high-fidelity universal control over this qubit, we combine fast Rabi oscillations with a new demonstration of the X(90) gate through phase modulation of the stabilization drive. Finally, the lifetime in this architecture is examined as a function of the cat size of up to 10 photons in the oscillator achieving a bit-flip time higher than 1 ms and only a linear decrease in the phase-flip time, in good agreement with the theoretical analysis of the circuit. Our qubit shows promise as a building block for fault-tolerant quantum processors with a small footprint.",
        "citation_title": "High-Coherence Kerr-cat qubit in 2D architecture",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Two-photon interference is an interesting quantum phenomenon that is usually captured in two distinct types of experiments, namely the Hanbury-Brown- Twiss (HBT) experiment and the Hong-Ou-Mandel (HOM) experiment. While the HBT experiment was carried out much earlier in 1956, with classical light, the demonstration of the HOM effect came much later in 1987. Unlike the former, the latter has been argued to be a purely quantum effect. A generalized formulation of two-particle interference is presented here. The HOM and the HBT effects emerge as special cases in the general analysis. A realizable two-particle interference experiment, which is intermediate between the two effects, is proposed and analyzed. Thus two-particle interference is shown to be a single phenomenon with various possible implementations, including the HBT and HOM setups.",
        "citation_title": "Generalized Two-Particle Interference",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We developed a general framework for hybrid quantum-classical computing of molecular and periodic embedding approaches based on an orbital space separation of the fragment and environment degrees of freedom. We demonstrate its potential by presenting a specific implementation of periodic range-separated DFT coupled to a quantum circuit ansatz, whereby the variational quantum eigensolver and the quantum equation-of-motion algorithm are used to obtain the low-lying spectrum of the embedded fragment Hamiltonian. Application of this scheme to study localized electronic states in materials is showcased through the accurate prediction of the optical properties of the neutral oxygen vacancy in magnesium oxide (MgO). Despite some discrepancies in the position of the main absorption band, the method demonstrates competitive performance compared to state-of-the-art ab initio approaches, particularly evidenced by the excellent agreement with the experimental photoluminescence emission peak.",
        "citation_title": "A general framework for active space embedding methods: applications in quantum computing",
        "date_delivered": "[Submitted on 29 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them. In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures. In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.",
        "citation_title": "Overcoming model uncertainty -- how equivalence tests can benefit from model averaging",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Modelling epidemics is crucial for understanding the emergence, transmission, impact and control of diseases. Spatial individual-level models (ILMs) that account for population heterogeneity are a useful tool, accounting for factors such as location, vaccination status and genetic information. Parametric forms for spatial risk functions, or kernels, are often used, but rely on strong assumptions about underlying transmission mechanisms. Here, we propose a class of non-parametric spatial disease transmission model, fitted within a Bayesian Markov chain Monte Carlo (MCMC) framework, allowing for more flexible assumptions when estimating the effect on spatial distance and infection risk. We focus upon two specific forms of non-parametric spatial infection kernel: piecewise constant and piecewise linear. Although these are relatively simple forms, we find them effective. The performance of these models is examined using simulated data, including under circumstances of model misspecification, and then applied to data from the UK 2001 foot-and-mouth disease.",
        "citation_title": "Individual-level models of disease transmission incorporating piecewise spatial risk functions",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "In the problem of quickest change detection (QCD), a change occurs at some unknown time in the distribution of a sequence of independent observations. This work studies a QCD problem where the change is either a bad change, which we aim to detect, or a confusing change, which is not of our interest. Our objective is to detect a bad change as quickly as possible while avoiding raising a false alarm for pre-change or a confusing change. We identify a specific set of pre-change, bad change, and confusing change distributions that pose challenges beyond the capabilities of standard Cumulative Sum (CuSum) procedures. Proposing novel CuSum-based detection procedures, S-CuSum and J-CuSum, leveraging two CuSum statistics, we offer solutions applicable across all kinds of pre-change, bad change, and confusing change distributions. For both S-CuSum and J-CuSum, we provide analytical performance guarantees and validate them by numerical results. Furthermore, both procedures are computationally efficient as they only require simple recursive updates.",
        "citation_title": "Quickest Change Detection with Confusing Change",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper proposes a Workflow for Assessing Treatment effeCt Heterogeneity (WATCH) in clinical drug development targeted at clinical trial sponsors. The workflow is designed to address the challenges of investigating treatment effect heterogeneity (TEH) in randomized clinical trials, where sample size and multiplicity limit the reliability of findings. The proposed workflow includes four steps: Analysis Planning, Initial Data Analysis and Analysis Dataset Creation, TEH Exploration, and Multidisciplinary Assessment. The workflow aims to provide a systematic approach to explore treatment effect heterogeneity in the exploratory setting, taking into account external evidence and best scientific understanding.",
        "citation_title": "WATCH: A Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "The Monty Hall problem is notorious for its deceptive simplicity. Although today it is widely used as a provocative thought experiment to introduce Bayesian thinking to students of probability, in the not so distant past it was rejected by established mathematicians. This essay provides some historical background to the problem and explains why it is considered so counter-intuitive to many. It is argued that the main barrier to understanding the problem is the back-grounding of the concept of dependence in probability theory as it is commonly taught. To demonstrate this, a Bayesian solution is provided and augmented with a probabilistic graphical model (PGM) inspired by the work of Pearl (1988, 1998). Although the Bayesian approach produces the correct answer, without a representation of the dependency structure of events implied by the problem, the salient fact that motivates the problem's solution remains hidden.",
        "citation_title": "What's So Hard about the Monty Hall Problem?",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We present a novel approach for modeling bounded count time series data, by deriving accurate upper and lower bounds for the variance of a bounded count random variable while maintaining a fixed mean. Leveraging these bounds, we propose semiparametric mean and variance joint (MVJ) models utilizing a clipped-Laplace link function. These models offer a flexible and feasible structure for both mean and variance, accommodating various scenarios of under-dispersion, equi-dispersion, or over-dispersion in bounded time series. The proposed MVJ models feature a linear mean structure with positive regression coefficients summing to one and allow for negative regression cefficients and autocorrelations. We demonstrate that the autocorrelation structure of MVJ models mirrors that of an autoregressive moving-average (ARMA) process, provided the proposed clipped-Laplace link functions with nonnegative regression coefficients summing to one are utilized. We establish conditions ensuring the stationarity and ergodicity properties of the MVJ process, along with demonstrating the consistency and asymptotic normality of the conditional least squares estimators. To aid model selection and diagnostics, we introduce two model selection criteria and apply two model diagnostics statistics. Finally, we conduct simulations and real data analyses to investigate the finite-sample properties of the proposed MVJ models, providing insights into their efficacy and applicability in practical scenarios.",
        "citation_title": "Semiparametric mean and variance joint models with clipped-Laplace link functions for bounded integer-valued time series",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Dedicated to the memory of Professor Tze Leung Lai, this paper introduces three multi-hypothesis sequential tests. These tests are derived from one-sided versions of the sequential probability ratio test and its modifications. They are proven to be first-order asymptotically optimal for testing simple and parametric composite hypotheses when error probabilities are small. These tests exhibit near optimality properties not only in classical i.i.d. observation models but also in general non-i.i.d. models, provided that the log-likelihood ratios between hypotheses converge r-completely to positive and finite numbers. These findings extend the seminal work of Lai (1981) on two hypotheses.",
        "citation_title": "Nearly Optimum Properties of Certain Multi-Decision Sequential Rules for General Non-i.i.d. Stochastic Models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The links between the mean families of Lehmer and H\u00f6lder and the weighted maximum likelihood estimator have recently been established in the case of a regular univariate exponential family. In this article, we will extend the outcomes obtained to the multivariate case. This extension provides a probabilistic interpretation of these families of means and could therefore broaden their uses in various applications.",
        "citation_title": "Deriving Lehmer and H\u00f6lder means as maximum weighted likelihood estimates for the multivariate exponential family",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recently, deep neural networks have been found to nearly interpolate training data but still generalize well in various applications. To help understand such a phenomenon, it has been of interest to analyze the ridge estimator and its interpolation limit in high-dimensional regression models. For this motivation, we study the ridge estimator in a rotationally sparse setting of high-dimensional linear regression, where the signal of a response is aligned with a small number, $d$, of covariates with large or spiked variances, compared with the remaining covariates with small or tail variances, \\textit{after} an orthogonal transformation of the covariate vector. We establish high-probability upper and lower bounds on the out-sample and in-sample prediction errors in two distinct regimes depending on the ratio of the effective rank of tail variances over the sample size $n$. The separation of the two regimes enables us to exploit relevant concentration inequalities and derive concrete error bounds without making any oracle assumption or independent components assumption on covariate vectors. Moreover, we derive sufficient and necessary conditions which indicate that the prediction errors of ridge estimation can be of the order $O(\\frac{d}{n})$ if and only if the gap between the spiked and tail variances are sufficiently large. We also compare the orders of optimal out-sample and in-sample prediction errors and find that, remarkably, the optimal out-sample prediction error may be significantly smaller than the optimal in-sample one. Finally, we present numerical experiments which empirically confirm our theoretical findings.",
        "citation_title": "On Ridge Estimation in High-dimensional Rotationally Sparse Linear Regression",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on $L_{1}$ regularization combined with cross-validation. As we demonstrate, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight \"shrinkage\". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.",
        "citation_title": "Network reconstruction via the minimum description length principle",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Generalized Linear Mixed Models (GLMMs) are widely used for analysing clustered data. One well-established method of overcoming the integral in the marginal likelihood function for GLMMs is penalized quasi-likelihood (PQL) estimation, although to date there are few asymptotic distribution results relating to PQL estimation for GLMMs in the literature. In this paper, we establish large sample results for PQL estimators of the parameters and random effects in independent-cluster GLMMs, when both the number of clusters and the cluster sizes go to infinity. This is done under two distinct regimes: conditional on the random effects (essentially treating them as fixed effects) and unconditionally (treating the random effects as random). Under the conditional regime, we show the PQL estimators are asymptotically normal around the true fixed and random effects. Unconditionally, we prove that while the estimator of the fixed effects is asymptotically normally distributed, the correct asymptotic distribution of the so-called prediction gap of the random effects may in fact be a normal scale-mixture distribution under certain relative rates of growth. A simulation study is used to verify the finite sample performance of our theoretical results.",
        "citation_title": "Asymptotic Results for Penalized Quasi-Likelihood Estimation in Generalized Linear Mixed Models",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In this work, we discuss a general class of the estimators for the cumulative distribution function (CDF) based on judgment post stratification (JPS) sampling scheme which includes both empirical and kernel distribution functions. Specifically, we obtain the expectation of the estimators in this class and show that they are asymptotically more efficient than their competitors in simple random sampling (SRS), as long as the rankings are better than random guessing. We find a mild condition that is necessary and sufficient for them to be asymptotically unbiased. We also prove that given the same condition, the estimators in this class are strongly uniformly consistent estimators of the true CDF, and converge in distribution to a normal distribution when the sample size goes to infinity.\nWe then focus on the kernel distribution function (KDF) in the JPS design and obtain the optimal bandwidth. We next carry out a comprehensive Monte Carlo simulation to compare the performance of the KDF in the JPS design for different choices of sample size, set size, ranking quality, parent distribution, kernel function as well as both perfect and imperfect rankings set-ups with its counterpart in SRS design. It is found that the JPS estimator dramatically improves the efficiency of the KDF as compared to its SRS competitor for a wide range of the settings. Finally, we apply the described procedure on a real dataset from medical context to show their usefulness and applicability in practice.",
        "citation_title": "Statistical Inference on the Cumulative Distribution Function using Judgment Post Stratification",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Many clinical questions involve estimating the effects of multiple treatments using observational data. When using longitudinal data, the interest is often in the effect of treatment strategies that involve sustaining treatment over time. This requires causal inference methods appropriate for handling multiple treatments and time-dependent confounding. Robins Generalised methods (g-methods) are a family of methods which can deal with time-dependent confounding and some of these have been extended to situations with multiple treatments, although there are currently no studies comparing different methods in this setting. We show how five g-methods (inverse-probability-of-treatment weighted estimation of marginal structural models, g-formula, g-estimation, censoring and weighting, and a sequential trials approach) can be extended to situations with multiple treatments, compare their performances in a simulation study, and demonstrate their application with an example using data from the UK CF Registry.",
        "citation_title": "Investigating the causal effects of multiple treatments using longitudinal data: a simulation study",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Self-supervised learning for image denoising problems in the presence of denaturation for noisy data is a crucial approach in machine learning. However, theoretical understanding of the performance of the approach that uses denatured data is lacking. To provide better understanding of the approach, in this paper, we analyze a self-supervised denoising algorithm that uses denatured data in depth through theoretical analysis and numerical experiments. Through the theoretical analysis, we discuss that the algorithm finds desired solutions to the optimization problem with the population risk, while the guarantee for the empirical risk depends on the hardness of the denoising task in terms of denaturation levels. We also conduct several experiments to investigate the performance of an extended algorithm in practice. The results indicate that the algorithm training with denatured images works, and the empirical performance aligns with the theoretical results. These results suggest several insights for further improvement of self-supervised image denoising that uses denatured data in future directions.",
        "citation_title": "Investigating Self-Supervised Image Denoising with Denaturation",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Distributed acoustic sensing through fiber-optical cables can contribute to traffic monitoring systems. Using data from a day of field testing on a 50 km long fiber-optic cable along a railroad track in Norway, we detect and track cars and trains along a segment of the fiber-optic cable where the road runs parallel to the railroad tracks. We develop a method for automatic detection of events and then use these in a Kalman filter variant known as joint probabilistic data association for object tracking and classification. Model parameters are specified using in-situ log data along with the fiber-optic signals. Running the algorithm over an entire day, we highlight results of counting cars and trains over time and their estimated velocities.",
        "citation_title": "Tracking and classifying objects with DAS data along railway",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Shot charts in basketball analytics provide an indispensable tool for evaluating players' shooting performance by visually representing the distribution of field goal attempts across different court locations. However, conventional methods often overlook the bounded nature of the basketball court, leading to inaccurate representations, particularly along the boundaries and corners. In this paper, we propose a novel model-based approach to shot chart estimation and visualization that explicitly considers the physical boundaries of the basketball court. By employing Gaussian mixtures for bounded data, our methodology allows to obtain more accurate estimation of shot density distributions for both made and missed shots. Bayes' rule is then applied to derive estimates for the probability of successful shooting from any given locations, and to identify the regions with the highest expected scores. To illustrate the efficacy of our proposal, we apply it to data from the 2022-23 NBA regular season, showing its usefulness through detailed analyses of shot patterns for two prominent players.",
        "citation_title": "A Model-Based Approach to Shot Charts Estimation in Basketball",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recent works have shown an interest in investigating the frequentist asymptotic properties of Bayesian procedures for high-dimensional linear models under sparsity constraints. However, there exists a gap in the literature regarding analogous theoretical findings for non-linear models within the high-dimensional setting. The current study provides a novel contribution, focusing specifically on a non-linear mixed-effects model. In this model, the residual variance is assumed to be known, while the covariance matrix of the random effects and the regression vector are unknown and must be estimated. The prior distribution for the sparse regression coefficients consists of a mixture of a point mass at zero and a Laplace distribution, while an Inverse-Wishart prior is employed for the covariance parameter of the random effects. First, the effective dimension of this model is bounded with high posterior probabilities. Subsequently, we derive posterior contraction rates for both the covariance parameter and the prediction term of the response vector. Finally, under additional assumptions, the posterior distribution is shown to contract for recovery of the unknown sparse regression vector at the same rate as observed in the linear case.",
        "citation_title": "Posterior contraction rates in a sparse non-linear mixed-effects model",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We develop a set of variable selection methods for the Cox model under interval censoring, in the ultra-high dimensional setting where the dimensionality can grow exponentially with the sample size. The methods select covariates via a penalized nonparametric maximum likelihood estimation with some popular penalty functions, including lasso, adaptive lasso, SCAD, and MCP. We prove that our penalized variable selection methods with folded concave penalties or adaptive lasso penalty enjoy the oracle property. Extensive numerical experiments show that the proposed methods have satisfactory empirical performance under various scenarios. The utility of the methods is illustrated through an application to a genome-wide association study of age to early childhood caries.",
        "citation_title": "Variable Selection in Ultra-high Dimensional Feature Space for the Cox Model with Interval-Censored Data",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Adaptive experiments such as multi-arm bandits adapt the treatment-allocation policy and/or the decision to stop the experiment to the data observed so far. This has the potential to improve outcomes for study participants within the experiment, to improve the chance of identifying best treatments after the experiment, and to avoid wasting data. Seen as an experiment (rather than just a continually optimizing system) it is still desirable to draw statistical inferences with frequentist guarantees. The concentration inequalities and union bounds that generally underlie adaptive experimentation algorithms can yield overly conservative inferences, but at the same time the asymptotic normality we would usually appeal to in non-adaptive settings can be imperiled by adaptivity. In this article we aim to explain why, how, and when adaptivity is in fact an issue for inference and, when it is, understand the various ways to fix it: reweighting to stabilize variances and recover asymptotic normality, always-valid inference based on joint normality of an asymptotic limiting sequence, and characterizing and inverting the non-normal distributions induced by adaptivity.",
        "citation_title": "Demistifying Inference after Adaptive Experiments",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Recently, there has been a significant focus on exploring the theoretical aspects of deep learning, especially regarding its performance in classification tasks. Bayesian deep learning has emerged as a unified probabilistic framework, seeking to integrate deep learning with Bayesian methodologies seamlessly. However, there exists a gap in the theoretical understanding of Bayesian approaches in deep learning for classification. This study presents an attempt to bridge that gap. By leveraging PAC-Bayes bounds techniques, we present theoretical results on the prediction or misclassification error of a probabilistic approach utilizing Spike-and-Slab priors for sparse deep learning in classification. We establish non-asymptotic results for the prediction error. Additionally, we demonstrate that, by considering different architectures, our results can achieve minimax optimal rates in both low and high-dimensional settings, up to a logarithmic factor. Moreover, our additional logarithmic term yields slight improvements over previous works. Additionally, we propose and analyze an automated model selection approach aimed at optimally choosing a network architecture with guaranteed optimality.",
        "citation_title": "Misclassification bounds for PAC-Bayesian sparse deep learning",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Knowing whether vaccine protection wanes over time is important for health policy and drug development. However, quantifying waning effects is difficult. A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time. Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect. We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination. We identify sharp bounds on the challenge effect under non-parametric assumptions that are broadly applicable in vaccine trials using routinely collected data. We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time. Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial. Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose.",
        "citation_title": "Quantification of vaccine waning as a challenge effect",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Economic policy sciences are constantly investigating the quality of well-being of broad sections of the population in order to describe the current interdependence between unequal living conditions, low levels of education and a lack of integration into society. Such studies are often carried out in the form of surveys, e.g. as part of the EU-SILC program. If the survey is designed at national or international level, the results of the study are often used as a reference by a broad range of public institutions. However, the sampling strategy per se may not capture enough information to provide an accurate representation of all population strata. Problems might arise from rare, or hard-to-sample, populations and the conclusion of the study may be compromised or unrealistic. We propose here a two-phase methodology to identify rare, poorly sampled populations and then resample the hard-to-sample strata. We focused our attention on the 2019 EU-SILC section concerning the Italian region of Liguria. Methods based on dispersion indices or deep learning were used to detect rare populations. A multi-frame survey was proposed as the sampling design. The results showed that factors such as citizenship, material deprivation and large families are still fundamental characteristics that are difficult to capture.",
        "citation_title": "Strategies for Rare Population Detection and Sampling: A Methodological Approach in Liguria",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We obtain several inequalities on the generalized means of dependent p-values. In particular, the weighted harmonic mean of p-values is strictly sub-uniform under several dependence assumptions of p-values, including independence, weak negative association, the class of extremal mixture copulas, and some Clayton copulas. Sub-uniformity of the harmonic mean of p-values has an important implication in multiple hypothesis testing: It is statistically invalid to merge p-values using the harmonic mean unless a proper threshold or multiplier adjustment is used, and this invalidity applies across all significance levels. The required multiplier adjustment on the harmonic mean explodes as the number of p-values increases, and hence there does not exist a constant multiplier that works for any number of p-values, even under independence.",
        "citation_title": "Sub-uniformity of harmonic mean p-values",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider the problem of making nonparametric inference in multi-dimensional diffusion models from low-frequency data. Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques. In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation. Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood. Using these developments, for the nonlinear inverse problem of recovering the diffusivity (in divergence form models), we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods. This enables the efficient implementation of a large class of statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates. We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors. Interestingly, the optimisation schemes provided satisfactory numerical recovery while exhibiting rapid convergence towards stationary points despite the problem nonlinearity; thus our approach may lead to significant computational speed-ups. The reproducible code is available online at this https URL.",
        "citation_title": "Statistical algorithms for low-frequency diffusion data: A PDE approach",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The Pareto front of a set of vectors is the subset which is comprised solely of all of the best trade-off points. By interpolating this subset, we obtain the optimal trade-off surface. In this work, we prove a very useful result which states that all Pareto front surfaces can be explicitly parametrised using polar coordinates. In particular, our polar parametrisation result tells us that we can fully characterise any Pareto front surface using the length function, which is a scalar-valued function that returns the projected length along any positive radial direction. Consequently, by exploiting this representation, we show how it is possible to generalise many useful concepts from linear algebra, probability and statistics, and decision theory to function over the space of Pareto front surfaces. Notably, we focus our attention on the stochastic setting where the Pareto front surface itself is a stochastic process. Among other things, we showcase how it is possible to define and estimate many statistical quantities of interest such as the expectation, covariance and quantile of any Pareto front surface distribution. As a motivating example, we investigate how these statistics can be used within a design of experiments setting, where the goal is to both infer and use the Pareto front surface distribution in order to make effective decisions. Besides this, we also illustrate how these Pareto front ideas can be used within the context of extreme value theory. Finally, as a numerical example, we applied some of our new methodology on a real-world air pollution data set.",
        "citation_title": "Random Pareto front surfaces",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Supervised machine learning models and public surveillance data has been employed for infectious disease forecasting in many settings. These models leverage various data sources capturing drivers of disease spread, such as climate conditions or human behavior. However, few models have incorporated the organizational structure of different geographic locations for forecasting. Traveling waves of seasonal outbreaks have been reported for dengue, influenza, and other infectious diseases, and many of the drivers of infectious disease dynamics may be shared across different cities, either due to their geographic or socioeconomic proximity. In this study, we developed a machine learning model to predict case counts of four infectious diseases across Brazilian cities one week ahead by incorporating information from related cities. We compared selecting related cities using both geographic distance and GDP per capita. Incorporating information from geographically proximate cities improved predictive performance for two of the four diseases, specifically COVID-19 and Zika. We also discuss the impact on forecasts in the presence of anomalous contagion patterns and the limitations of the proposed methodology.",
        "citation_title": "Integrating socioeconomic and geographic data to enhance infectious disease prediction in Brazilian cities",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "The cosinor model is frequently used to represent gene expression given the 24 hour day-night cycle time at which a corresponding tissue sample is collected. However, the timing of many biological processes are based on individual-specific internal timing systems that are offset relative to day-night cycle time. When these offsets are unknown, they pose a challenge in performing statistical analyses with a cosinor model. To clarify, when sample collection times are mis-recorded, cosinor regression can yield attenuated parameter estimates, which would also attenuate test statistics. This attenuation bias would inflate type II error rates in identifying genes with oscillatory behavior. This paper proposes a heuristic method to account for unknown offsets when tissue samples are collected in a longitudinal design. Specifically, this method involves first estimating individual-specific cosinor models for each gene. The times of sample collection for that individual are then translated based on the estimated phase-shifts across every gene. Simulation studies confirm that this method mitigates bias in estimation and inference. Illustrations with real data from three circadian biology studies highlight that this method produces parameter estimates and inferences akin to those obtained when each individual's offset is known.",
        "citation_title": "A mixed effects cosinor modelling framework for circadian gene expression",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present a flexible, deterministic numerical method for computing left-tail rare events of sums of non-negative, independent random variables. The method is based on iterative numerical integration of linear convolutions by means of Newtons-Cotes rules. The periodicity properties of convoluted densities combined with the Trapezoidal rule are exploited to produce a robust and efficient method, and the method is flexible in the sense that it can be applied to all kinds of non-negative continuous RVs. We present an error analysis and study the benefits of utilizing Newton-Cotes rules versus the fast Fourier transform (FFT) for numerical integration, showing that although there can be efficiency-benefits to using FFT, Newton-Cotes rules tend to preserve the relative error better, and indeed do so at an acceptable computational cost. Numerical studies on problems with both known and unknown rare-event probabilities showcase the method's performance and support our theoretical findings.",
        "citation_title": "A Fast and Accurate Numerical Method for the Left Tail of Sums of Independent Random Variables",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper proposes a probabilistic machine learning method to price catastrophe (CAT) bonds in the primary market. The proposed method combines machine-learning-based predictive models with Conformal Prediction, an innovative algorithm that generates distribution-free probabilistic forecasts for CAT bond prices. Using primary market CAT bond transaction records between January 1999 and March 2021, the proposed method is found to be more robust and yields more accurate predictions of the bond spreads than traditional regression-based methods. Furthermore, the proposed method generates more informative prediction intervals than linear regression and identifies important nonlinear relationships between various risk factors and bond spreads, suggesting that linear regressions could misestimate the bond spreads. Overall, this paper demonstrates the potential of machine learning methods in improving the pricing of CAT bonds.",
        "citation_title": "Pricing Catastrophe Bonds -- A Probabilistic Machine Learning Approach",
        "date_delivered": "[Submitted on 10 Apr 2024]"
    },
    {
        "abstract": "In vibration-based condition monitoring, optimal filter design improves fault detection by enhancing weak fault signatures within vibration signals. This process involves optimising a derived objective function from a defined objective. The objectives are often based on proxy health indicators to determine the filter's parameters. However, these indicators can be compromised by irrelevant extraneous signal components and fluctuating operational conditions, affecting the filter's efficacy. Fault detection primarily uses the fault component's prominence in the squared envelope spectrum, quantified by a squared envelope spectrum-based signal-to-noise ratio. New optimal filter objective functions are derived from the proposed generalised envelope spectrum-based signal-to-noise objective for machines operating under variable speed conditions. Instead of optimising proxy health indicators, the optimal filter coefficients of the formulation directly maximise the squared envelope spectrum-based signal-to-noise ratio over targeted frequency bands using standard gradient-based optimisers. Four derived objective functions from the proposed objective effectively outperform five prominent methods in tests on three experimental datasets.",
        "citation_title": "Generalised envelope spectrum-based signal-to-noise objectives: Formulation, optimisation and application for gear fault detection under time-varying speed conditions",
        "date_delivered": "[Submitted on 26 Apr 2024]"
    },
    {
        "abstract": "Mitigating cybersecurity risk in electric vehicle (EV) charging demand forecasting plays a crucial role in the safe operation of collective EV chargings, the stability of the power grid, and the cost-effective infrastructure expansion. However, existing methods either suffer from the data privacy issue and the susceptibility to cyberattacks or fail to consider the spatial correlation among different stations. To address these challenges, a federated graph learning approach involving multiple charging stations is proposed to collaboratively train a more generalized deep learning model for demand forecasting while capturing spatial correlations among various stations and enhancing robustness against potential attacks. Firstly, for better model performance, a Graph Neural Network (GNN) model is leveraged to characterize the geographic correlation among different charging stations in a federated manner. Secondly, to ensure robustness and deal with the data heterogeneity in a federated setting, a message passing that utilizes a global attention mechanism to aggregate personalized models for each client is proposed. Thirdly, by concerning cyberattacks, a special credit-based function is designed to mitigate potential threats from malicious clients or unwanted attacks. Extensive experiments on a public EV charging dataset are conducted using various deep learning techniques and federated learning methods to demonstrate the prediction accuracy and robustness of the proposed approach.",
        "citation_title": "Federated Graph Learning for EV Charging Demand Forecasting with Personalization Against Cyberattacks",
        "date_delivered": "[Submitted on 30 Apr 2024]"
    },
    {
        "abstract": "Variational quantum computing offers a flexible computational paradigm with applications in diverse areas. However, a key obstacle to realizing their potential is the Barren Plateau (BP) phenomenon. When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases. Importantly, all the moving pieces of an algorithm -- choices of ansatz, initial state, observable, loss function and hardware noise -- can lead to BPs when ill-suited. Due to the significant impact of BPs on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects. As a result, the study of BPs has become a thriving area of research, influencing and cross-fertilizing other fields such as quantum optimal control, tensor networks, and learning theory. This article provides a comprehensive review of the current understanding of the BP phenomenon.",
        "citation_title": "A Review of Barren Plateaus in Variational Quantum Computing",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Statistical learning theory and the Probably Approximately Correct (PAC) criterion are the common approach to mathematical learning theory. PAC is widely used to analyze learning problems and algorithms, and have been studied thoroughly. Uniform worst case bounds on the convergence rate have been well established using, e.g., VC theory or Radamacher complexity. However, in a typical scenario the performance could be much better. In this paper, we consider PAC learning using a somewhat different tradeoff, the error exponent - a well established analysis method in Information Theory - which describes the exponential behavior of the probability that the risk will exceed a certain threshold as function of the sample size. We focus on binary classification and find, under some stability assumptions, an improved distribution dependent error exponent for a wide range of problems, establishing the exponential behavior of the PAC error probability in agnostic learning. Interestingly, under these assumptions, agnostic learning may have the same error exponent as realizable learning. The error exponent criterion can be applied to analyze knowledge distillation, a problem that so far lacks a theoretical analysis.",
        "citation_title": "Error Exponent in Agnostic PAC Learning",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Linear representation learning is widely studied due to its conceptual simplicity and empirical utility in tasks such as compression, classification, and feature extraction. Given a set of points $[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n] = \\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and a vector $\\mathbf{y} \\in \\mathbb{R}^d$, the goal is to find coefficients $\\mathbf{w} \\in \\mathbb{R}^n$ so that $\\mathbf{X} \\mathbf{w} \\approx \\mathbf{y}$, subject to some desired structure on $\\mathbf{w}$. In this work we seek $\\mathbf{w}$ that forms a local reconstruction of $\\mathbf{y}$ by solving a regularized least squares regression problem. We obtain local solutions through a locality function that promotes the use of columns of $\\mathbf{X}$ that are close to $\\mathbf{y}$ when used as a regularization term. We prove that, for all levels of regularization and under a mild condition that the columns of $\\mathbf{X}$ have a unique Delaunay triangulation, the optimal coefficients' number of non-zero entries is upper bounded by $d+1$, thereby providing local sparse solutions when $d \\ll n$. Under the same condition we also show that for any $\\mathbf{y}$ contained in the convex hull of $\\mathbf{X}$ there exists a regime of regularization parameter such that the optimal coefficients are supported on the vertices of the Delaunay simplex containing $\\mathbf{y}$. This provides an interpretation of the sparsity as having structure obtained implicitly from the Delaunay triangulation of $\\mathbf{X}$. We demonstrate that our locality regularized problem can be solved in comparable time to other methods that identify the containing Delaunay simplex.",
        "citation_title": "Locality Regularized Reconstruction: Structured Sparsity and Delaunay Triangulations",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "We study the problem of learning a binary classifier on the vertices of a graph. In particular, we consider classifiers given by monophonic halfspaces, partitions of the vertices that are convex in a certain abstract sense. Monophonic halfspaces, and related notions such as geodesic halfspaces,have recently attracted interest, and several connections have been drawn between their properties(e.g., their VC dimension) and the structure of the underlying graph $G$. We prove several novel results for learning monophonic halfspaces in the supervised, online, and active settings. Our main result is that a monophonic halfspace can be learned with near-optimal passive sample complexity in time polynomial in $n = |V(G)|$. This requires us to devise a polynomial-time algorithm for consistent hypothesis checking, based on several structural insights on monophonic halfspaces and on a reduction to $2$-satisfiability. We prove similar results for the online and active settings. We also show that the concept class can be enumerated with delay $\\operatorname{poly}(n)$, and that empirical risk minimization can be performed in time $2^{\\omega(G)}\\operatorname{poly}(n)$ where $\\omega(G)$ is the clique number of $G$. These results answer open questions from the literature (Gonz\u00e1lez et al., 2020), and show a contrast with geodesic halfspaces, for which some of the said problems are NP-hard (Seiffarth et al., 2023).",
        "citation_title": "Efficient Algorithms for Learning Monophonic Halfspaces in Graphs",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "This paper presents a new algorithm member for accelerating first-order methods for bilevel optimization, namely the \\emph{(Perturbed) Restarted Accelerated Fully First-order methods for Bilevel Approximation}, abbreviated as \\texttt{(P)RAF${}^2$BA}. The algorithm leverages \\emph{fully} first-order oracles and seeks approximate stationary points in nonconvex-strongly-convex bilevel optimization, enhancing oracle complexity for efficient optimization. Theoretical guarantees for finding approximate first-order stationary points and second-order stationary points at the state-of-the-art query complexities are established, showcasing their effectiveness in solving complex optimization tasks. Empirical studies for real-world problems are provided to further validate the outperformance of our proposed algorithms. The significance of \\texttt{(P)RAF${}^2$BA} in optimizing nonconvex-strongly-convex bilevel optimization problems is underscored by its state-of-the-art convergence rates and computational efficiency.",
        "citation_title": "Accelerated Fully First-Order Methods for Bilevel and Minimax Optimization",
        "date_delivered": "[Submitted on 1 May 2024]"
    },
    {
        "abstract": "Linkage methods are among the most popular algorithms for hierarchical clustering. Despite their relevance the current knowledge regarding the quality of the clustering produced by these methods is limited. Here, we improve the currently available bounds on the maximum diameter of the clustering obtained by complete-link for metric spaces.\nOne of our new bounds, in contrast to the existing ones, allows us to separate complete-link from single-link in terms of approximation for the diameter, which corroborates the common perception that the former is more suitable than the latter when the goal is producing compact clusters.\nWe also show that our techniques can be employed to derive upper bounds on the cohesion of a class of linkage methods that includes the quite popular average-link.",
        "citation_title": "New bounds on the cohesion of complete-link and other linkage methods for agglomeration clustering",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Restless multi-armed bandits (RMAB) play a central role in modeling sequential decision making problems under an instantaneous activation constraint that at most B arms can be activated at any decision epoch. Each restless arm is endowed with a state that evolves independently according to a Markov decision process regardless of being activated or not. In this paper, we consider the task of learning in episodic RMAB with unknown transition functions and adversarial rewards, which can change arbitrarily across episodes. Further, we consider a challenging but natural bandit feedback setting that only adversarial rewards of activated arms are revealed to the decision maker (DM). The goal of the DM is to maximize its total adversarial rewards during the learning process while the instantaneous activation constraint must be satisfied in each decision epoch. We develop a novel reinforcement learning algorithm with two key contributors: a novel biased adversarial reward estimator to deal with bandit feedback and unknown transitions, and a low-complexity index policy to satisfy the instantaneous activation constraint. We show $\\tilde{\\mathcal{O}}(H\\sqrt{T})$ regret bound for our algorithm, where $T$ is the number of episodes and $H$ is the episode length. To our best knowledge, this is the first algorithm to ensure $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret for adversarial RMAB in our considered challenging settings.",
        "citation_title": "Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Neural collapse (NC) is a simple and symmetric phenomenon for deep neural networks (DNNs) at the terminal phase of training, where the last-layer features collapse to their class means and form a simplex equiangular tight frame aligning with the classifier vectors. However, the relationship of the last-layer features to the data and intermediate layers during training remains unexplored. To this end, we characterize the geometry of intermediate layers of ResNet and propose a novel conjecture, progressive feedforward collapse (PFC), claiming the degree of collapse increases during the forward propagation of DNNs. We derive a transparent model for the well-trained ResNet according to that ResNet with weight decay approximates the geodesic curve in Wasserstein space at the terminal phase. The metrics of PFC indeed monotonically decrease across depth on various datasets. We propose a new surrogate model, multilayer unconstrained feature model (MUFM), connecting intermediate layers by an optimal transport regularizer. The optimal solution of MUFM is inconsistent with NC but is more concentrated relative to the input data. Overall, this study extends NC to PFC to model the collapse phenomenon of intermediate layers and its dependence on the input data, shedding light on the theoretical understanding of ResNet in classification problems.",
        "citation_title": "Progressive Feedforward Collapse of ResNet Training",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We study Thompson Sampling-based algorithms for stochastic bandits with bounded rewards. As the existing problem-dependent regret bound for Thompson Sampling with Gaussian priors [Agrawal and Goyal, 2017] is vacuous when $T \\le 288 e^{64}$, we derive a more practical bound that tightens the coefficient of the leading term %from $288 e^{64}$ to $1270$. Additionally, motivated by large-scale real-world applications that require scalability, adaptive computational resource allocation, and a balance in utility and computation, we propose two parameterized Thompson Sampling-based algorithms: Thompson Sampling with Model Aggregation (TS-MA-$\\alpha$) and Thompson Sampling with Timestamp Duelling (TS-TD-$\\alpha$), where $\\alpha \\in [0,1]$ controls the trade-off between utility and computation. Both algorithms achieve $O \\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $K$ is the number of arms, $T$ is the finite learning horizon, and $\\Delta$ denotes the single round performance loss when pulling a sub-optimal arm.",
        "citation_title": "Efficient and Adaptive Posterior Sampling Algorithms for Bandits",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Decentralized learning is appealing as it enables the scalable usage of large amounts of distributed data and resources (without resorting to any central entity), while promoting privacy since every user minimizes the direct exposure of their data. Yet, without additional precautions, curious users can still leverage models obtained from their peers to violate privacy. In this paper, we propose Decor, a variant of decentralized SGD with differential privacy (DP) guarantees. Essentially, in Decor, users securely exchange randomness seeds in one communication round to generate pairwise-canceling correlated Gaussian noises, which are injected to protect local models at every communication round. We theoretically and empirically show that, for arbitrary connected graphs, Decor matches the central DP optimal privacy-utility trade-off. We do so under SecLDP, our new relaxation of local DP, which protects all user communications against an external eavesdropper and curious users, assuming that every pair of connected users shares a secret, i.e., an information hidden to all others. The main theoretical challenge is to control the accumulation of non-canceling correlated noise due to network sparsity. We also propose a companion SecLDP privacy accountant for public use.",
        "citation_title": "The Privacy Power of Correlated Noise in Decentralized Learning",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "In the realm of multi-arm bandit problems, the Gittins index policy is known to be optimal in maximizing the expected total discounted reward obtained from pulling the Markovian arms. In most realistic scenarios however, the Markovian state transition probabilities are unknown and therefore the Gittins indices cannot be computed. One can then resort to reinforcement learning (RL) algorithms that explore the state space to learn these indices while exploiting to maximize the reward collected. In this work, we propose tabular (QGI) and Deep RL (DGN) algorithms for learning the Gittins index that are based on the retirement formulation for the multi-arm bandit problem. When compared with existing RL algorithms that learn the Gittins index, our algorithms have a lower run time, require less storage space (small Q-table size in QGI and smaller replay buffer in DGN), and illustrate better empirical convergence to the Gittins index. This makes our algorithm well suited for problems with large state spaces and is a viable alternative to existing methods. As a key application, we demonstrate the use of our algorithms in minimizing the mean flowtime in a job scheduling problem when jobs are available in batches and have an unknown service time distribution. \\",
        "citation_title": "Tabular and Deep Reinforcement Learning for Gittins Index",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "This paper presents a novel mathematical framework based on stochastic geometry to investigate the electromagnetic field exposure of idle and active users in cellular networks implementing dynamic beamforming. Accurate modeling of antenna gain becomes crucial in this context, encompassing both the main and the side lobes. The marginal distribution of EMF exposure for each type of users is initially derived. Subsequently, network performance is scrutinized by introducing a new metric aimed at ensuring minimal downlink coverage while simultaneously maintaining EMF exposure below distinct thresholds for both idle and active users. The metrics exhibit a high dependency on various parameters, such as the distance between active and idle users and the number of antenna elements.",
        "citation_title": "Stochastic Geometry Analysis of EMF Exposure of Idle Users and Network Performance with Dynamic Beamforming",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Deep Neural Networks (DNN) have shown great promise in many classification applications, yet are widely known to have poorly calibrated predictions when they are over-parametrized. Improving DNN calibration without comprising on model accuracy is of extreme importance and interest in safety critical applications such as in the health-care sector. In this work, we show that decoupling the training of feature extraction layers and classification layers in over-parametrized DNN architectures such as Wide Residual Networks (WRN) and Visual Transformers (ViT) significantly improves model calibration whilst retaining accuracy, and at a low training cost. In addition, we show that placing a Gaussian prior on the last hidden layer outputs of a DNN, and training the model variationally in the classification training stage, even further improves calibration. We illustrate these methods improve calibration across ViT and WRN architectures for several image classification benchmark datasets.",
        "citation_title": "Decoupling Feature Extraction and Classification Layers for Calibrated Neural Networks",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Differentiable particle filters combine the flexibility of neural networks with the probabilistic nature of sequential Monte Carlo methods. However, traditional approaches rely on the availability of labelled data, i.e., the ground truth latent state information, which is often difficult to obtain in real-world applications. This paper compares the effectiveness of two semi-supervised training objectives for differentiable particle filters. We present results in two simulated environments where labelled data are scarce.",
        "citation_title": "Revisiting semi-supervised training objectives for differentiable particle filters",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Practical Bayesian learning often requires (1) online inference, (2) dynamic models, and (3) ensembling over multiple different models. Recent advances have shown how to use random feature approximations to achieve scalable, online ensembling of Gaussian processes with desirable theoretical properties and fruitful applications. One key to these methods' success is the inclusion of a random walk on the model parameters, which makes models dynamic. We show that these methods can be generalized easily to any basis expansion model and that using alternative basis expansions, such as Hilbert space Gaussian processes, often results in better performance. To simplify the process of choosing a specific basis expansion, our method's generality also allows the ensembling of several entirely different models, for example, a Gaussian process and polynomial regression. Finally, we propose a novel method to ensemble static and dynamic models together.",
        "citation_title": "Dynamic Online Ensembles of Basis Expansions",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We present a new random walk for uniformly sampling high-dimensional convex bodies. It achieves state-of-the-art runtime complexity with stronger guarantees on the output than previously known, namely in R\u00e9nyi divergence (which implies TV, $\\mathcal{W}_2$, KL, $\\chi^2$). The proof departs from known approaches for polytime algorithms for the problem -- we utilize a stochastic diffusion perspective to show contraction to the target distribution with the rate of convergence determined by functional isoperimetric constants of the stationary density.",
        "citation_title": "In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may choose to (not) comply with a treatment recommendation, whenever it is made available to them, based on unobserved confounding factors. We provide non-parametric identification, estimation, and inference for Dynamic Local Average Treatment Effects, which are expected values of multi-period treatment contrasts among appropriately defined complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify local average effects of contrasts that correspond to offering treatment at any single time step. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify local average treatment effects of treating in multiple time periods.",
        "citation_title": "Dynamic Local Average Treatment Effects",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Quantum parameter estimation theory is an important component of quantum information theory and provides the statistical foundation that underpins important topics such as quantum system identification and quantum waveform estimation. When there is more than one parameter the ultimate precision in the mean square error given by the quantum Cram\u00e9r-Rao bound is not necessarily achievable. For non-full rank quantum states, it was not known when this bound can be saturated (achieved) when only a single copy of the quantum state encoding the unknown parameters is available. This single-copy scenario is important because of its experimental/practical tractability. Recently, necessary and sufficient conditions for saturability of the quantum Cram\u00e9r-Rao bound in the multiparameter single-copy scenario have been established in terms of i) the commutativity of a set of projected symmetric logarithmic derivatives and ii) the existence of a unitary solution to a system of coupled nonlinear partial differential equations. New sufficient conditions were also obtained that only depend on properties of the symmetric logarithmic derivatives. In this paper, key structural properties of optimal measurements that saturate the quantum Cram\u00e9r-Rao bound are illuminated. These properties are exploited to i) show that the sufficient conditions are in fact necessary and sufficient for an optimal measurement to be projective, ii) give an alternative proof of previously established necessary conditions, and iii) describe general POVMs, not necessarily projective, that saturate the multiparameter QCRB. Examples are given where a unitary solution to the system of nonlinear partial differential equations can be explicitly calculated when the required conditions are fulfilled.",
        "citation_title": "Saturation of the Multiparameter Quantum Cram\u00e9r-Rao Bound at the Single-Copy Level with Projective Measurements",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Algorithms frequently assist, rather than replace, human decision-makers. However, the design and analysis of algorithms often focus on predicting outcomes and do not explicitly model their effect on human decisions. This discrepancy between the design and role of algorithmic assistants becomes of particular concern in light of empirical evidence that suggests that algorithmic assistants again and again fail to improve human decisions. In this article, we formalize the design of recommendation algorithms that assist human decision-makers without making restrictive ex-ante assumptions about how recommendations affect decisions. We formulate an algorithmic-design problem that leverages the potential-outcomes framework from causal inference to model the effect of recommendations on a human decision-maker's binary treatment choice. Within this model, we introduce a monotonicity assumption that leads to an intuitive classification of human responses to the algorithm. Under this monotonicity assumption, we can express the human's response to algorithmic recommendations in terms of their compliance with the algorithm and the decision they would take if the algorithm sends no recommendation. We showcase the utility of our framework using an online experiment that simulates a hiring task. We argue that our approach explains the relative performance of different recommendation algorithms in the experiment, and can help design solutions that realize human-AI complementarity.",
        "citation_title": "Designing Algorithmic Recommendations to Achieve Human-AI Complementarity",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "A patient's digital twin is a computational model that describes the evolution of their health over time. Digital twins have the potential to revolutionize medicine by enabling individual-level computer simulations of human health, which can be used to conduct more efficient clinical trials or to recommend personalized treatment options. Due to the overwhelming complexity of human biology, machine learning approaches that leverage large datasets of historical patients' longitudinal health records to generate patients' digital twins are more tractable than potential mechanistic models. In this manuscript, we describe a neural network architecture that can learn conditional generative models of clinical trajectories, which we call Digital Twin Generators (DTGs), that can create digital twins of individual patients. We show that the same neural network architecture can be trained to generate accurate digital twins for patients across 13 different indications simply by changing the training set and tuning hyperparameters. By introducing a general purpose architecture, we aim to unlock the ability to scale machine learning approaches to larger datasets and across more indications so that a digital twin could be created for any patient in the world.",
        "citation_title": "Digital Twin Generators for Disease Modeling",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "Bayesian few-shot classification has been a focal point in the field of few-shot learning. This paper seamlessly integrates mirror descent-based variational inference into Gaussian process-based few-shot classification, addressing the challenge of non-conjugate inference. By leveraging non-Euclidean geometry, mirror descent achieves accelerated convergence by providing the steepest descent direction along the corresponding manifold. It also exhibits the parameterization invariance property concerning the variational distribution. Experimental results demonstrate competitive classification accuracy, improved uncertainty quantification, and faster convergence compared to baseline models. Additionally, we investigate the impact of hyperparameters and components. Code is publicly available at this https URL.",
        "citation_title": "Accelerating Convergence in Bayesian Few-Shot Classification",
        "date_delivered": "[Submitted on 2 May 2024]"
    },
    {
        "abstract": "We generalize McDiarmid's inequality for functions with bounded differences on a high probability set, using an extension argument. Those functions concentrate around their conditional expectations. We further extend the results to concentration in general metric spaces.",
        "citation_title": "An extension of McDiarmid's inequality",
        "date_delivered": "[Submitted on 17 Nov 2015 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "While neural networks can be approximated by linear models as their width increases, certain properties of wide neural networks cannot be captured by linear models. In this work we show that recently proposed Neural Quadratic Models can exhibit the \"catapult phase\" [Lewkowycz et al. 2020] that arises when training such models with large learning rates. We then empirically show that the behaviour of neural quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. Our analysis further demonstrates that quadratic models can be an effective tool for analysis of neural networks.",
        "citation_title": "Quadratic models for understanding catapult dynamics of neural networks",
        "date_delivered": "[Submitted on 24 May 2022 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Over the last decade, an approach that has gained a lot of popularity to tackle nonparametric testing problems on general (i.e., non-Euclidean) domains is based on the notion of reproducing kernel Hilbert space (RKHS) embedding of probability distributions. The main goal of our work is to understand the optimality of two-sample tests constructed based on this approach. First, we show the popular MMD (maximum mean discrepancy) two-sample test to be not optimal in terms of the separation boundary measured in Hellinger distance. Second, we propose a modification to the MMD test based on spectral regularization by taking into account the covariance information (which is not captured by the MMD test) and prove the proposed test to be minimax optimal with a smaller separation boundary than that achieved by the MMD test. Third, we propose an adaptive version of the above test which involves a data-driven strategy to choose the regularization parameter and show the adaptive test to be almost minimax optimal up to a logarithmic factor. Moreover, our results hold for the permutation variant of the test where the test threshold is chosen elegantly through the permutation of the samples. Through numerical experiments on synthetic and real data, we demonstrate the superior performance of the proposed test in comparison to the MMD test and other popular tests in the literature.",
        "citation_title": "Spectral Regularized Kernel Two-Sample Tests",
        "date_delivered": "[Submitted on 19 Dec 2022 (v1), last revised 1 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold with noises. Our approach first constructs a graph representation, referred to as the skeleton, to capture the underlying geometric structure. We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function. We also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph. The proposed regression framework suggests a novel way to deal with data with underlying geometric structures and provides additional advantages in handling the union of multiple manifolds, additive noises, and noisy observations. We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples.",
        "citation_title": "Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure",
        "date_delivered": "[Submitted on 19 Mar 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We consider the problem of finite-time identification of linear dynamical systems from $T$ samples of a single trajectory. Recent results have predominantly focused on the setup where no structural assumption is made on the system matrix $A^* \\in \\mathbb{R}^{n \\times n}$, and have consequently analyzed the ordinary least squares (OLS) estimator in detail. We assume prior structural information on $A^*$ is available, which can be captured in the form of a convex set $\\mathcal{K}$ containing $A^*$. For the solution of the ensuing constrained least squares estimator, we derive non-asymptotic error bounds in the Frobenius norm that depend on the local size of $\\mathcal{K}$ at $A^*$. To illustrate the usefulness of these results, we instantiate them for four examples, namely when (i) $A^*$ is sparse and $\\mathcal{K}$ is a suitably scaled $\\ell_1$ ball; (ii) $\\mathcal{K}$ is a subspace; (iii) $\\mathcal{K}$ consists of matrices each of which is formed by sampling a bivariate convex function on a uniform $n \\times n$ grid (convex regression); (iv) $\\mathcal{K}$ consists of matrices each row of which is formed by uniform sampling (with step size $1/T$) of a univariate Lipschitz function. In all these situations, we show that $A^*$ can be reliably estimated for values of $T$ much smaller than what is needed for the unconstrained setting.",
        "citation_title": "Learning linear dynamical systems under convex constraints",
        "date_delivered": "[Submitted on 27 Mar 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Distinguishing causal connections from correlations is important in many scenarios. However, the presence of unobserved variables, such as the latent confounder, can introduce bias in conditional independence testing commonly employed in constraint-based causal discovery for identifying causal relations. To address this issue, existing methods introduced proxy variables to adjust for the bias caused by unobserveness. However, these methods were either limited to categorical variables or relied on strong parametric assumptions for identification. In this paper, we propose a novel hypothesis-testing procedure that can effectively examine the existence of the causal relationship over continuous variables, without any parametric constraint. Our procedure is based on discretization, which under completeness conditions, is able to asymptotically establish a linear equation whose coefficient vector is identifiable under the causal null hypothesis. Based on this, we introduce our test statistic and demonstrate its asymptotic level and power. We validate the effectiveness of our procedure using both synthetic and real-world data.",
        "citation_title": "Causal Discovery via Conditional Independence Testing with Proxy Variables",
        "date_delivered": "[Submitted on 9 May 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "We introduce an approach which allows detecting causal relationships between variables for which the time evolution is available. Causality is assessed by a variational scheme based on the Information Imbalance of distance ranks, a statistical test capable of inferring the relative information content of different distance measures. We test whether the predictability of a putative driven system Y can be improved by incorporating information from a potential driver system X, without explicitly modeling the underlying dynamics and without the need to compute probability densities of the dynamic variables. This framework makes causality detection possible even between high-dimensional systems where only few of the variables are known or measured. Benchmark tests on coupled chaotic dynamical systems demonstrate that our approach outperforms other model-free causality detection methods, successfully handling both unidirectional and bidirectional couplings. We also show that the method can be used to robustly detect causality in human electroencephalography data.",
        "citation_title": "Robust inference of causality in high-dimensional dynamical processes from the Information Imbalance of distance ranks",
        "date_delivered": "[Submitted on 18 May 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "This paper is concerned with the problem of policy evaluation with linear function approximation in discounted infinite horizon Markov decision processes. We investigate the sample complexities required to guarantee a predefined estimation error of the best linear coefficients for two widely-used policy evaluation algorithms: the temporal difference (TD) learning algorithm and the two-timescale linear TD with gradient correction (TDC) algorithm. In both the on-policy setting, where observations are generated from the target policy, and the off-policy setting, where samples are drawn from a behavior policy potentially different from the target policy, we establish the first sample complexity bound with high-probability convergence guarantee that attains the optimal dependence on the tolerance level. We also exhihit an explicit dependence on problem-related quantities, and show in the on-policy setting that our upper bound matches the minimax lower bound on crucial problem parameters, including the choice of the feature maps and the problem dimension.",
        "citation_title": "High-probability sample complexities for policy evaluation with linear function approximation",
        "date_delivered": "[Submitted on 30 May 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Modular approaches that use a different composition of modules for each problem are a promising direction in continual learning (CL). However, searching through the large, discrete space of module compositions is challenging, especially because evaluating a composition's performance requires a round of neural network training. We address this challenge through a modular CL framework, PICLE, that uses a probabilistic model to cheaply compute the fitness of each composition, allowing PICLE to achieve both perceptual, few-shot and latent transfer. The model combines prior knowledge about good module compositions with dataset-specific information. We evaluate PICLE using two benchmark suites designed to assess different desiderata of CL techniques. Comparing to a wide range of approaches, we show that PICLE is the first modular CL algorithm to achieve perceptual, few-shot and latent transfer while scaling well to large search spaces, outperforming previous state-of-the-art modular CL approaches on long problem sequences.",
        "citation_title": "A Probabilistic Framework for Modular Continual Learning",
        "date_delivered": "[Submitted on 11 Jun 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Motivated by the emergence of decentralized machine learning (ML) ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental information asymmetries that arise in decentralized ML: uncertainty in the assessment of model quality and uncertainty regarding the optimal performance of any model. We show that a principal can cope with such asymmetry via simple linear contracts that achieve 1-1/e fraction of the optimal utility. To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract. We also study linear contracts and derive the optimal utility in the more complex setting of multiple interactions.",
        "citation_title": "Delegating Data Collection in Decentralized Machine Learning",
        "date_delivered": "[Submitted on 4 Sep 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Solving a linear system $Ax=b$ is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used. We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter $\\omega$ has a strong impact on its runtime. For this method, we prove that a bandit online learning algorithm--using only the number of iterations as feedback--can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed $\\omega$ as the sequence length increases. Furthermore, when given additional structural information, we show that a contextual bandit method asymptotically achieves the performance of the instance-optimal policy, which selects the best $\\omega$ for each instance. Our work provides the first learning-theoretic treatment of high-precision linear system solvers and the first end-to-end guarantees for data-driven scientific computing, demonstrating theoretically the potential to speed up numerical methods using well-understood learning algorithms.",
        "citation_title": "Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances",
        "date_delivered": "[Submitted on 3 Oct 2023 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturing.",
        "citation_title": "Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions",
        "date_delivered": "[Submitted on 9 Oct 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Accurate predictions of the populations and spatial distributions of wild animal species is critical from a species management and conservation perspective. Culling is a measure taken for various reasons, including when overpopulation of a species is observed or suspected. Thus accurate estimates of population numbers are essential for specifying, monitoring, and evaluating the impact of such programmes. Population data for wild animals is generally collated from various sources and at differing spatial resolutions. Citizen science projects typically provide point referenced data, whereas site surveys, hunter reports, and official government data may be aggregated and released at a small area or regional level. Jointly modelling these data resources involves overcoming challenges of spatial misalignment.\nIn this article, we develop an N mixture modelling methodology for joint modelling of species populations in the presence of spatially misaligned data, motivated by the three main species of wild deer in the Republic of Ireland; fallow, red and sika. Previous studies of deer populations investigated the distribution and abundance on a species by species basis, failing to account for possible correlation between individual species and the impact of ecological covariates on their distributions.",
        "citation_title": "Spatial Joint Species N-Mixture Models for Multi-Source Observational Data with Application to Wild Deer Population Abundance",
        "date_delivered": "[Submitted on 30 Oct 2023 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. Our proposed method improves time series forecasting and classification performance compared to state-of-the-art Transformer-based models, while it is more efficient in terms of the number of parameters and training/inference time. Code is available at this repository: this https URL.",
        "citation_title": "Learning to Embed Time Series Patches Independently",
        "date_delivered": "[Submitted on 27 Dec 2023 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $\\Omega(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that significantly outperforms this quadratic baseline. Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011) that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. If we rely on the conjecture that the second-neighbor search finishes in log-linear time (Baron & Darling, 2020; 2022), we demonstrate theoretically that our algorithm finishes in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\\log N)$, but with a more typical log-linear complexity of $O(N\\log^2N)$. In practice, we show that our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline -- in a manner consistent with our theoretical analysis -- allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.",
        "citation_title": "Scalable network reconstruction in subquadratic time",
        "date_delivered": "[Submitted on 2 Jan 2024 (v1), last revised 2 May 2024 (this version, v4)]"
    },
    {
        "abstract": "Differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. In reporting scientific discoveries, Bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of P-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. We present a novel differentially private Bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. Furthermore, by focusing on differentially private Bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. We also provide a set of sufficient conditions to establish results on Bayes factor consistency under the proposed framework. The utility of the devised technology is showcased via several numerical experiments.",
        "citation_title": "Differentially private Bayesian tests",
        "date_delivered": "[Submitted on 27 Jan 2024 (v1), last revised 1 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.",
        "citation_title": "Diffusive Gibbs Sampling",
        "date_delivered": "[Submitted on 5 Feb 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "This paper presents a new approach for batch Bayesian Optimization (BO) called Thompson Sampling-Regret to Sigma Ratio directed sampling (TS-RSR), where we sample a new batch of actions by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our sampling objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. Theoretically, we provide rigorous convergence guarantees on our algorithm's regret, and numerically, we demonstrate that our method attains state-of-the-art performance on a range of challenging synthetic and realistic test functions, where it outperforms several competitive benchmark batch BO algorithms.",
        "citation_title": "TS-RSR: A provably efficient approach for batch bayesian optimization",
        "date_delivered": "[Submitted on 7 Mar 2024 (v1), last revised 2 May 2024 (this version, v3)]"
    },
    {
        "abstract": "Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any black-box prediction model and converts point predictions into set predictions that have a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: for a given problem, which of the plethora of prediction methods (random forests, neural nets, regularized linear models, etc.) should we conformalize? This paper proposes a new approach towards conformal model aggregation in online settings that is based on combining the prediction sets from several algorithms by voting, where weights on the models are adapted over time based on past performance.",
        "citation_title": "Conformal online model aggregation",
        "date_delivered": "[Submitted on 22 Mar 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Understanding what makes high-dimensional data learnable is a fundamental question in machine learning. On the one hand, it is believed that the success of deep learning lies in its ability to build a hierarchy of representations that become increasingly more abstract with depth, going from simple features like edges to more complex concepts. On the other hand, learning to be insensitive to invariances of the task, such as smooth transformations for image datasets, has been argued to be important for deep networks and it strongly correlates with their performance. In this work, we aim to explain this correlation and unify these two viewpoints. We show that by introducing sparsity to generative hierarchical models of data, the task acquires insensitivity to spatial transformations that are discrete versions of smooth transformations. In particular, we introduce the Sparse Random Hierarchy Model (SRHM), where we observe and rationalize that a hierarchical representation mirroring the hierarchical model is learnt precisely when such insensitivity is learnt, thereby explaining the strong correlation between the latter and performance. Moreover, we quantify how the sample complexity of CNNs learning the SRHM depends on both the sparsity and hierarchical structure of the task.",
        "citation_title": "How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model",
        "date_delivered": "[Submitted on 16 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Most existing temporal point process models are characterized by conditional intensity function. These models often require numerical approximation methods for likelihood evaluation, which potentially hurts their performance. By directly modelling the integral of the intensity function, i.e., the cumulative hazard function (CHF), the likelihood can be evaluated accurately, making it a promising approach. However, existing CHF-based methods are not well-defined, i.e., the mathematical constraints of CHF are not completely satisfied, leading to untrustworthy results. For multivariate temporal point process, most existing methods model intensity (or density, etc.) functions for each variate, limiting the scalability. In this paper, we explore using neural networks to model a flexible but well-defined CHF and learning the multivariate temporal point process with low parameter complexity. Experimental results on six datasets show that the proposed model achieves the state-of-the-art performance on data fitting and event prediction tasks while having significantly fewer parameters and memory usage than the strong competitors. The source code and data can be obtained from this https URL.",
        "citation_title": "Cumulative Hazard Function Based Efficient Multivariate Temporal Point Process Learning",
        "date_delivered": "[Submitted on 21 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task. We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically. Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data. We also test the algorithm's performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality. Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases. We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process.",
        "citation_title": "Automating the Discovery of Partial Differential Equations in Dynamical Systems",
        "date_delivered": "[Submitted on 25 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.",
        "citation_title": "KAN: Kolmogorov-Arnold Networks",
        "date_delivered": "[Submitted on 30 Apr 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    },
    {
        "abstract": "The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates. Estimating the air change rate in indoor environments, however, remains challenging. It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors. In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties. The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber. Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach. In addition, uncertainties in real-life contexts were quantified with an incremental variance {\\sigma} for the Wiener process. This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal. The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified. A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi. Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm.",
        "citation_title": "Implementing Bayesian inference on a stochastic CO2-based grey-box model for assessing indoor air quality in Canadian primary schools",
        "date_delivered": "[Submitted on 1 May 2024 (v1), last revised 2 May 2024 (this version, v2)]"
    }
]